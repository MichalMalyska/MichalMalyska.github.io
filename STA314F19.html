<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Michal Malyska" />


<title>STA314F19</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Michal Malyska</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-university"></span>
     
    Teaching
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="STA130F19.html">
        <span class="fa fa-book"></span>
         
        STA130 - Intro to Data Science Fall 2019
      </a>
    </li>
    <li>
      <a href="STA314F19.html">
        <span class="fa fa-book"></span>
         
        STA314 - Machine Learning I Fall 2019
      </a>
    </li>
    <li>
      <a href="STA220.html">
        <span class="fa fa-book"></span>
         
        STA220 - The Practice of Statistics I Summer 2019
      </a>
    </li>
    <li>
      <a href="STA314.html">
        <span class="fa fa-book"></span>
         
        STA314 - Machine Learning I Fall 2018
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-book"></span>
     
    Course Work
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Coursework_Summary.html">
        <span class="fa fa-book"></span>
         
        Coursework Summary
      </a>
    </li>
    <li>
      <a href="STA410_A1.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 1
      </a>
    </li>
    <li>
      <a href="STA410_A2.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 2
      </a>
    </li>
    <li>
      <a href="STA410_A3.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 3
      </a>
    </li>
    <li>
      <a href="STA410_A4.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 4
      </a>
    </li>
    <li>
      <a href="STA447_A2.html">
        <span class="fa fa-code"></span>
         
        STA447 - Stochastic Processes - Assignment 2
      </a>
    </li>
    <li>
      <a href="STA_490_ReactionTimes_Project.html">
        <span class="fa fa-code"></span>
         
        STA490 - Stats Consulting and Collaboration
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-pencil"></span>
     
    Personal Projects
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="ASNA2019CC_datacreation.html">
        <span class="fa fa-code"></span>
         
        ASNA2019 Case Competition
      </a>
    </li>
    <li>
      <a href="Kaggle.html">
        <span class="fa fa-code"></span>
         
        Kaggle Competitions
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-code"></span>
     
    Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="RResources.html">
        <span class="fa fa-book"></span>
         
        RStudio / R resources
      </a>
    </li>
    <li>
      <a href="StatsResources.html">
        <span class="fa fa-book"></span>
         
        General Statistics resources
      </a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    About me
  </a>
</li>
<li>
  <a href="resume.pdf">
    <span class="fa fa-file-pdf-o"></span>
     
    Resume
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">STA314F19</h1>
<h4 class="author">Michal Malyska</h4>

</div>


<div id="preliminaries-and-r-setup" class="section level1">
<h1>Preliminaries and R setup</h1>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ──────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 3.2.0     ✔ purrr   0.3.2
## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
## ✔ tidyr   0.8.3     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.4.0</code></pre>
<pre><code>## ── Conflicts ─────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(MASS)</code></pre>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     select</code></pre>
<pre class="r"><code>library(ISLR)</code></pre>
<div id="r-setup" class="section level2">
<h2>R Setup</h2>
<p>If you need help setting up R and Rstudio, I host a guide on my <a href="https://michalmalyska.github.io/RResources.html">resources page</a></p>
</div>
<div id="loading-data" class="section level2">
<h2>Loading Data</h2>
<p>You can find a fully commented guide on how to load in tabular data using the faster, tidyverse function read_csv() <a href="https://michalmalyska.github.io/RResources.html">here</a></p>
</div>
</div>
<div id="tutorial-1" class="section level1">
<h1>Tutorial 1</h1>
</div>
<div id="tutorial-2" class="section level1">
<h1>Tutorial 2</h1>
</div>
<div id="tutorial-3" class="section level1">
<h1>Tutorial 3</h1>
<div id="lab" class="section level2">
<h2>Lab</h2>
<pre class="r"><code>data_orig &lt;- MASS::Boston

df &lt;- as_tibble(data_orig)


df_train &lt;- df %&gt;% sample_n(size = length(df$chas) * 0.7)

df_test &lt;- setdiff(df, df_train)</code></pre>
<p>Let’s start by showing off what bad practice is and fit a linear model without doing any kind of previous work:</p>
<pre class="r"><code>model1 &lt;- lm(medv ~ lstat, data = df_train)
summary(model1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat, data = df_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.478  -4.264  -1.492   2.266  24.180 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  34.8172     0.6978   49.90   &lt;2e-16 ***
## lstat        -0.9441     0.0477  -19.79   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.55 on 352 degrees of freedom
## Multiple R-squared:  0.5268, Adjusted R-squared:  0.5254 
## F-statistic: 391.8 on 1 and 352 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>plot(model1)</code></pre>
<p><img src="STA314F19_files/figure-html/Model%20Assumptions-1.png" width="576" /><img src="STA314F19_files/figure-html/Model%20Assumptions-2.png" width="576" /><img src="STA314F19_files/figure-html/Model%20Assumptions-3.png" width="576" /><img src="STA314F19_files/figure-html/Model%20Assumptions-4.png" width="576" /></p>
<p>Assumptions are not satisfied!</p>
<pre class="r"><code>ggplot(data = df, aes(x = medv, y = ..density..)) +
  geom_density() +
  geom_histogram(bins = 75, alpha = 0.3, fill = &quot;red&quot;) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Looking%20at%20distribution%20of%20response-1.png" width="576" /></p>
<pre class="r"><code>ggplot(data = df, aes(sample = medv)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Looking%20at%20distribution%20of%20response-2.png" width="576" /></p>
<pre class="r"><code>ggplot(data = df, aes(y = medv, x = lstat)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) +
  geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;), color = &quot;green&quot;) +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), color = &quot;blue&quot;) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Looking%20at%20distribution%20of%20response-3.png" width="576" /></p>
<p>How to make predictions:</p>
<pre class="r"><code>df_test$predictions &lt;- predict(model1, newdata = df_test)</code></pre>
<p>How to add multiple variables as predictors:</p>
<pre class="r"><code>model2 &lt;- lm(data = df_train, formula = medv ~ lstat + age)
summary(model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat + age, data = df_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.270  -4.282  -1.484   2.660  22.865 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 33.48517    0.91463  36.611   &lt;2e-16 ***
## lstat       -1.02161    0.05875 -17.388   &lt;2e-16 ***
## age          0.03386    0.01515   2.235    0.026 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.513 on 351 degrees of freedom
## Multiple R-squared:  0.5334, Adjusted R-squared:  0.5307 
## F-statistic: 200.6 on 2 and 351 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>plot(model2)</code></pre>
<p><img src="STA314F19_files/figure-html/MLR-1.png" width="576" /><img src="STA314F19_files/figure-html/MLR-2.png" width="576" /><img src="STA314F19_files/figure-html/MLR-3.png" width="576" /><img src="STA314F19_files/figure-html/MLR-4.png" width="576" /></p>
<p>How to add everything (and subtract some) variables:</p>
<pre class="r"><code>model3 &lt;- lm(data = df_train, formula = medv ~ . -age -indus)
summary(model3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ . - age - indus, data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.3489  -3.0458  -0.6117   2.0301  25.4191 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  33.894182   6.330942   5.354 1.58e-07 ***
## crim         -0.141533   0.036919  -3.834 0.000150 ***
## zn            0.049012   0.016217   3.022 0.002699 ** 
## chas          3.838400   1.064806   3.605 0.000359 ***
## nox         -19.492956   4.337231  -4.494 9.56e-06 ***
## rm            4.093314   0.518198   7.899 3.87e-14 ***
## dis          -1.617703   0.223072  -7.252 2.77e-12 ***
## rad           0.305310   0.074606   4.092 5.33e-05 ***
## tax          -0.009415   0.003915  -2.405 0.016697 *  
## ptratio      -0.854389   0.161702  -5.284 2.26e-07 ***
## black         0.009051   0.003373   2.684 0.007636 ** 
## lstat        -0.523129   0.056640  -9.236  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.007 on 342 degrees of freedom
## Multiple R-squared:  0.7314, Adjusted R-squared:  0.7227 
## F-statistic: 84.64 on 11 and 342 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>How to add interactions:</p>
<pre class="r"><code>model4 &lt;- lm(data = df_train, formula = medv ~ . -age -indus + lstat:ptratio )
summary(model4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ . - age - indus + lstat:ptratio, data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.3495  -3.0458  -0.6146   2.0275  25.4242 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    3.397e+01  7.673e+00   4.428 1.28e-05 ***
## crim          -1.416e-01  3.703e-02  -3.823 0.000156 ***
## zn             4.895e-02  1.658e-02   2.952 0.003378 ** 
## chas           3.839e+00  1.067e+00   3.599 0.000367 ***
## nox           -1.946e+01  4.671e+00  -4.166 3.93e-05 ***
## rm             4.091e+00  5.336e-01   7.667 1.86e-13 ***
## dis           -1.616e+00  2.330e-01  -6.937 2.03e-11 ***
## rad            3.053e-01  7.472e-02   4.086 5.47e-05 ***
## tax           -9.428e-03  3.981e-03  -2.368 0.018436 *  
## ptratio       -8.588e-01  2.883e-01  -2.979 0.003099 ** 
## black          9.054e-03  3.381e-03   2.678 0.007760 ** 
## lstat         -5.306e-01  4.039e-01  -1.313 0.189907    
## ptratio:lstat  3.883e-04  2.087e-02   0.019 0.985170    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.014 on 341 degrees of freedom
## Multiple R-squared:  0.7314, Adjusted R-squared:  0.7219 
## F-statistic: 77.36 on 12 and 341 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>How to write high level interactions:</p>
<pre class="r"><code>model5 &lt;- lm(data = df_train, formula = medv ~ lstat*ptratio*black)
summary(model5)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat * ptratio * black, data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.1307  -3.9053  -0.9233   1.7941  27.1501 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)         -2.814e+01  5.108e+01  -0.551   0.5820  
## lstat                2.176e+00  2.950e+00   0.738   0.4613  
## ptratio              2.287e+00  2.584e+00   0.885   0.3766  
## black                2.515e-01  1.341e-01   1.875   0.0616 .
## lstat:ptratio       -1.191e-01  1.483e-01  -0.803   0.4223  
## lstat:black         -1.130e-02  7.822e-03  -1.444   0.1495  
## ptratio:black       -1.090e-02  6.798e-03  -1.603   0.1098  
## lstat:ptratio:black  4.956e-04  3.940e-04   1.258   0.2093  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.019 on 346 degrees of freedom
## Multiple R-squared:  0.6071, Adjusted R-squared:  0.5992 
## F-statistic: 76.39 on 7 and 346 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This is what our data looks like:</p>
<pre class="r"><code>ggplot(data = df_train, aes(x = lstat, y = medv)) +
  geom_point() +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Nonlinear%20factors-1.png" width="576" /></p>
<p>How to add a funciton of a variable as a predictor and how to make simple visualizations without adding predictions:</p>
<pre class="r"><code>model6 &lt;- lm(data = df_train, formula = medv ~ lstat + I(lstat^2))
summary(model6)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat + I(lstat^2), data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.6090  -4.2046  -0.3744   2.6421  25.2400 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 44.298144   1.056174   41.94   &lt;2e-16 ***
## lstat       -2.514702   0.149400  -16.83   &lt;2e-16 ***
## I(lstat^2)   0.048744   0.004456   10.94   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.665 on 351 degrees of freedom
## Multiple R-squared:  0.6471, Adjusted R-squared:  0.6451 
## F-statistic: 321.8 on 2 and 351 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>ggplot(data = df_train, aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2)) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/unnamed-chunk-2-1.png" width="576" /></p>
<p>How to fit higher degree polynomials and visualize them:</p>
<pre class="r"><code>model7 &lt;- lm(data = df_train, formula = medv ~ poly(lstat, degree = 5))
summary(model7)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ poly(lstat, degree = 5), data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.4738  -3.1575  -0.5226   2.3261  27.0463 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                22.8469     0.2824  80.912  &lt; 2e-16 ***
## poly(lstat, degree = 5)1 -129.6530     5.3127 -24.404  &lt; 2e-16 ***
## poly(lstat, degree = 5)2   61.9586     5.3127  11.662  &lt; 2e-16 ***
## poly(lstat, degree = 5)3  -25.1276     5.3127  -4.730 3.27e-06 ***
## poly(lstat, degree = 5)4   23.5874     5.3127   4.440 1.21e-05 ***
## poly(lstat, degree = 5)5  -15.8957     5.3127  -2.992  0.00297 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.313 on 348 degrees of freedom
## Multiple R-squared:  0.6922, Adjusted R-squared:  0.6878 
## F-statistic: 156.5 on 5 and 348 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>ggplot(data = df_train, aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, degree = 5)) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/unnamed-chunk-3-1.png" width="576" /></p>
<p>How to change plotting settings in base R (For tidyverse you should use gridExtra)</p>
<pre class="r"><code>par(mfrow = c(2,2))
plot(model6)</code></pre>
<p><img src="STA314F19_files/figure-html/parmfrow-1.png" width="576" /></p>
<p>Fitting character variables:</p>
<pre class="r"><code>df2 &lt;- ISLR::Carseats

df2_train &lt;- sample_n(df2, size = length(df2$Sales) * 0.7)
df2_test &lt;- setdiff(df2, df2_train)

glimpse(df2_train)</code></pre>
<pre><code>## Observations: 280
## Variables: 11
## $ Sales       &lt;dbl&gt; 5.40, 10.59, 6.88, 4.10, 5.90, 10.08, 5.55, 2.05, 4.…
## $ CompPrice   &lt;dbl&gt; 149, 131, 96, 121, 138, 116, 104, 131, 137, 119, 122…
## $ Income      &lt;dbl&gt; 73, 120, 39, 78, 92, 72, 100, 82, 112, 98, 74, 43, 4…
## $ Advertising &lt;dbl&gt; 13, 15, 0, 4, 0, 10, 8, 0, 15, 0, 0, 0, 11, 0, 0, 10…
## $ Population  &lt;dbl&gt; 381, 262, 161, 413, 13, 456, 398, 132, 434, 18, 424,…
## $ Price       &lt;dbl&gt; 163, 124, 112, 130, 120, 130, 97, 157, 149, 126, 149…
## $ ShelveLoc   &lt;fct&gt; Bad, Medium, Good, Bad, Bad, Good, Medium, Bad, Bad,…
## $ Age         &lt;dbl&gt; 26, 30, 27, 46, 61, 41, 61, 25, 66, 73, 51, 57, 80, …
## $ Education   &lt;dbl&gt; 11, 10, 14, 10, 12, 14, 11, 14, 13, 17, 13, 10, 15, …
## $ Urban       &lt;fct&gt; No, Yes, No, No, Yes, No, Yes, Yes, Yes, No, Yes, No…
## $ US          &lt;fct&gt; Yes, Yes, No, Yes, No, Yes, Yes, No, Yes, No, No, Ye…</code></pre>
<pre class="r"><code># Fit a model with a catergorical variable
model7 &lt;- lm(data = df2_train, formula = Sales ~ ShelveLoc)
summary(model7)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Sales ~ ShelveLoc, data = df2_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.1669 -1.5894  0.0581  1.5884  6.1931 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       5.0876     0.2889   17.61  &lt; 2e-16 ***
## ShelveLocGood     5.2889     0.4243   12.46  &lt; 2e-16 ***
## ShelveLocMedium   2.0794     0.3443    6.04 4.95e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.347 on 277 degrees of freedom
## Multiple R-squared:  0.3619, Adjusted R-squared:  0.3572 
## F-statistic: 78.54 on 2 and 277 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># What are ShelveLocGood and ShelveLocMedium 
contrasts(df2$ShelveLoc)</code></pre>
<pre><code>##        Good Medium
## Bad       0      0
## Good      1      0
## Medium    0      1</code></pre>
<pre class="r"><code>Load_packages &lt;- function(){
  library(tidyverse)
  library(MASS)
  library(ISLR)
}</code></pre>
<pre class="r"><code>Load_packages()</code></pre>
<p>If you would like to see some more advanced uses of the techniques; I <a href="https://michalmalyska.github.io/STA314.html#r_for_data_science_chapter_25:_many_models">covered</a> the same lab and a chapter of the R for data science textbook that deals with fitting multiple models to the same dataset using tidyverse functions.</p>
</div>
</div>
<div id="tutorial-4" class="section level1">
<h1>Tutorial 4</h1>
<div id="homework-solutions-overview" class="section level2">
<h2>Homework Solutions Overview:</h2>
<div id="question-1" class="section level3">
<h3>Question 1</h3>
<ol style="list-style-type: lower-alpha">
<li><p>We expect that with a very large number of measurements n, a flexible learning method would be able to learn the signal without as much fear of overfitting.</p></li>
<li><p>If the number of predictors p is very large and n is small then there is a greater possibility that a flexible learning method would overfit. Then we expect the inflexible method to be better in this case.</p></li>
<li><p>A highly non-linear relationship would most likely need a flexible statistical learning method to perform optimally.</p></li>
<li><p>With a very large error term variance <span class="math inline">\(\sigma^2\)</span> there is more worry about overfitting with flexible methods and thus an inflexible method would perform better.</p></li>
</ol>
</div>
<div id="question-2" class="section level3">
<h3>Question 2</h3>
<p>Bias Variance decomposition:</p>
<p><span class="math display">\[ 
\mathbb{E}  \left[(y_0 - \hat{f}(x_0))^2 \right] =
\mathbb{V}ar(\hat{f}(x_0)) + \left( \mathbb{E} \hat{f}(x_0) - \mathbb{E}y_0 \right)^2 + \mathbb{V}ar(\epsilon)
\]</span> Or equivalently:</p>
<p><span class="math display">\[ 
\mathbb{E}  \left[(y_0 - \hat{y}_0)^2 \right] =
\mathbb{V}ar(\hat{y}_0) + \left( \mathbb{E}\hat{y}_0 - \mathbb{E}y_0 \right)^2 + \mathbb{V}ar(\epsilon)
\]</span></p>
<p>The left-hand-side of the above is the expected mean square error (MSE) or a measure of how well on average our approximation function at <span class="math inline">\(x_0\)</span> is estimating the true value <span class="math inline">\(y_0\)</span>. The first term on the right-hand-side of the above expression is the error in the MSE due to errors in “fitting” the true <span class="math inline">\(f\)</span> with our approximate <span class="math inline">\(\hat{f}\)</span>. This error come from the sensitivity of the learning procedure to the finite training data set. Typically more flexible fitting methods will be more sensitive to the errors and noise in the given training set. The second term on the right-hand-side is the error in <span class="math inline">\(f \neq \hat{f}\)</span> due to using a learning algorithm that might not be able to represent the complexities in <span class="math inline">\(f\)</span>. For example, taking to be linear when the true underlying function <span class="math inline">\(f\)</span> is non-linear. The third term on the right-hand-side represents un-learnable error due to either not having all the predictive variables in our model (predictors that if we could get values for would improve our ability to learn the function <span class="math inline">\(\hat{f}\)</span> or error that is just intrinsic to the process which we are trying to model. In either case, given the data we have there is no way to reduce this component of the MSE error. A typical plots of the things suggested look like pieces from figures on pages 3 and 7 from the Week 2 Tuesday lecture notes. In figure on page 3 (right-hand-side) we have plots of the training error, testing error and the irreducible error curves. The training error shows a steady decrease (improvement) as the flexibility of the learning method increases. The test error is the red curve that initially decreases as the flexibility increase but then begins to increase again for continued increase in flexibility. The irreducible error is the constant dotted line. Notice that the point where the testing error is as close to the irreducible error would be the optimal operating point for this system. The distance between the lowest point on the testing error curve and the irreducible error gives an indication of how much bias there is in the given learning procedure, i.e. how far the best function <span class="math inline">\(\hat{f}\)</span> will be from <span class="math inline">\(f\)</span>. In figure on page 7 (left plot) we have curves representing the individual components of the bias-variance decomposition. The blue curve is the squared bias which we see gets smaller as the complexity of the model increase (we are able to model more and more complicated patters in <span class="math inline">\(\hat{f}\)</span>. The orange curve shows the variance of the learned model i.e. as we add complexity (more flexibility), the dependence on the dataset increases. So what function we get out of our learning procedure gets more sensitive to errors/ noise in the training dataset (and the error increase). The horizontal line is the irreducible error again.</p>
</div>
<div id="question-3" class="section level3">
<h3>Question 3</h3>
<p>A very flexible fitting procedure will fit non-linear functions better (if that is indeed the model generation process that is generating your data) but will be more susceptible to errors/ noise in the training dataset. A less flexible approach exchanges where it makes errors. That is a less flexible fitting procedure is unable to model the exact non-linear f but its predictions are also likely to be more stable to errors / noise in the training dataset.</p>
</div>
<div id="question-4" class="section level3">
<h3>Question 4</h3>
<p>A parametric learning procedure means the functional form of the mapping <span class="math inline">\(f\)</span> is specified, except for the parameter values, which the learning procedure must estimate. A non-parametric learning procedure is much more flexible in the forms of <span class="math inline">\(f\)</span> it can model and the learning procedure must “learn more” from the data (the functional form that should take) and then the parameters needed to estimate it. A parametric approach is generally a less flexible fitting method while a non-parametric approach is a more flexible method with the trade-offs that that characterization contains.</p>
</div>
</div>
<div id="islr-chapter-3-exercise-5" class="section level2">
<h2>ISLR Chapter 3 Exercise 5</h2>
<p><span class="math display">\[
\hat{y}_{i} =
x_{i} \frac{\sum_{i&#39;=1}^{n}\left( x_{i&#39;} y_{i&#39;} \right)}{\sum_{j=1}^{n} x_{j}^{2}}
=\sum_{i&#39;=1}^n \frac{\left(x_i x_{i&#39;}  \right)}{\sum_{j=1}^n x_j^2}y_{i&#39;} = 
\sum_{i&#39;=1}^n a_{i&#39;} y_{i&#39;}
\]</span></p>
</div>
<div id="islr-chapter-3-exercise-7" class="section level2">
<h2>ISLR Chapter 3 Exercise 7</h2>
<p>Given that <span class="math inline">\(\bar{x} = \bar{y} = 0\)</span></p>
<p><span class="math display">\[
R^2 = \frac{TSS - RSS}{TSS} = 1-  \frac{RSS}{TSS}
\]</span></p>
<p><span class="math display">\[
TSS = \sum_i \left( y_i - \bar{y} \right)^ 2 = \sum_i y_i^2
\]</span></p>
<p><span class="math display">\[ 
RSS = \sum_i \left(y_i - \hat{y_i} \right)^2 =
\sum_i \left(y_i - (\hat{\beta_0} + \hat{\beta_1} x_i) \right)^2
\]</span></p>
<p>now noting that:</p>
<p><span class="math display">\[
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x} = 0
\]</span></p>
<p>We simplify to get</p>
<p><span class="math display">\[
\sum_i \left(y_i - (\hat{\beta_0} + \hat{\beta_1} x_i) \right)^2 =
\sum_i \left(y_i - (\hat{\beta_1} x_i) \right)^2
\]</span></p>
<p>Then looking at formula for <span class="math inline">\(\hat{\beta_1}\)</span>:</p>
<p><span class="math display">\[
\hat{\beta_1} = \frac{\sum_i (x_i - \bar{x}) (y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} = \\
\frac{\sum_i x_iy_i}{\sum_i x_i^2}
\]</span></p>
<p>Plugging in and expanding</p>
<p><span class="math display">\[
\sum_i \left(y_i - (\hat{\beta_1} x_i) \right)^2 = 
\sum_i \left(y_i^2 - 2(\hat{\beta_1} x_i)y_i + (\hat{\beta_1} x_i)^2 \right) = \\
= \sum_i \left(y_i^2 - 2(\frac{\sum_i x_iy_i}{\sum_i x_i^2} x_i)y_i + (\frac{\sum_i x_iy_i}{\sum_i x_i^2} x_i)^2 \right) = \\
= \sum_i y_i^2 - 2 \frac{\sum_i x_iy_i}{\sum_i x_i^2} \sum_i x_i y_i +
\left( \frac{\sum_i x_iy_i}{\sum_i x_i^2} \right)^2 \sum_ix_i^2 = \\
= \sum_i y_i^2 - \frac{(\sum_i x_iy_i)^2}{\sum_i x_i^2}
\]</span></p>
<p>Plugging into the <span class="math inline">\(R^2\)</span> formula:</p>
<p><span class="math display">\[ 
R^2 =  \frac{TSS - RSS}{TSS} = \frac{\sum_i y_i^2 - \sum_i y_i^2 +
\frac{(\sum_i x_iy_i)^2}{\sum_i x_i^2}}{\sum_i y_i^2} = 
\frac{(\sum_i x_iy_i)^2}{\sum_i x_i^2 \sum_i y_i^2}
\]</span></p>
<p>Correlation is (since <span class="math inline">\(\bar{x} = \bar{y} = 0\)</span>):</p>
<p><span class="math display">\[ 
Corr(X,Y) = \frac{(\sum_i x_iy_i)}{\sqrt{\sum_i x_i^2}\sqrt{\sum_i y_i^2}}
\]</span></p>
</div>
<div id="additional-materials" class="section level2">
<h2>Additional Materials</h2>
<p>Great <a href="https://www.alexpghayes.com/blog/understanding-multinomial-regression-with-partial-dependence-plots/">blogpost</a> on understanding Mutinomial regression with Partial Dependence plots.</p>
<p>If you want to type up your homework 2 in RMarkdown and save on time I would suggest taking a look at the equatiomatic package. (This is also super useful for STA303)</p>
</div>
<div id="tutorial-questions" class="section level2">
<h2>Tutorial Questions</h2>
<ul>
<li>Why does logistic regression become unstable for linearly separable data?</li>
</ul>
<p>Answer:</p>
<p>If the data is perfectly linearly separable (meaning that you can draw a hyperplane such that all datapoints of either class are on different sides of it). Then the MLE solution to logistic regression does not exist. What this would mean when you are estimating coefficients is that as you train your model the estimates are going to diverge to infinity as your logistic regression is trying to approximate a step function. A good description of this can be found <a href="https://stats.stackexchange.com/a/254266">here</a></p>
<ul>
<li>Can you talk more about LDA?</li>
</ul>
<p>Answer:</p>
<p>I think the most helpful right now would be to share the LDA slides from STA414</p>
<p><img src="figures/STA314F19/FLDA1.png" /></p>
<p><img src="figures/STA314F19/FLDA2.png" /></p>
<p><img src="figures/STA314F19/FLDA3.png" /></p>
<p><img src="figures/STA314F19/FLDA4.png" /></p>
<p><img src="figures/STA314F19/FLDA5.png" /></p>
<p><img src="figures/STA314F19/QDA1.png" /> <img src="figures/STA314F19/QDA2.png" /> <img src="figures/STA314F19/QDA3.png" /> <img src="figures/STA314F19/QDA4.png" /> <img src="figures/STA314F19/QDA5.png" /></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
