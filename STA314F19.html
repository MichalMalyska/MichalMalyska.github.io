<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Michal Malyska" />


<title>STA314F19</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Michal Malyska</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-university"></span>
     
    Teaching
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="STA130F19.html">
        <span class="fa fa-book"></span>
         
        STA130 - Intro to Data Science Fall 2019
      </a>
    </li>
    <li>
      <a href="STA314F19.html">
        <span class="fa fa-book"></span>
         
        STA314 - Machine Learning I Fall 2019
      </a>
    </li>
    <li>
      <a href="STA220.html">
        <span class="fa fa-book"></span>
         
        STA220 - The Practice of Statistics I Summer 2019
      </a>
    </li>
    <li>
      <a href="STA314.html">
        <span class="fa fa-book"></span>
         
        STA314 - Machine Learning I Fall 2018
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-book"></span>
     
    Course Work
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Coursework_Summary.html">
        <span class="fa fa-book"></span>
         
        Coursework Summary
      </a>
    </li>
    <li>
      <a href="STA410_A1.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 1
      </a>
    </li>
    <li>
      <a href="STA410_A2.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 2
      </a>
    </li>
    <li>
      <a href="STA410_A3.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 3
      </a>
    </li>
    <li>
      <a href="STA410_A4.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 4
      </a>
    </li>
    <li>
      <a href="STA447_A2.html">
        <span class="fa fa-code"></span>
         
        STA447 - Stochastic Processes - Assignment 2
      </a>
    </li>
    <li>
      <a href="STA_490_ReactionTimes_Project.html">
        <span class="fa fa-code"></span>
         
        STA490 - Stats Consulting and Collaboration
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-pencil"></span>
     
    Personal Projects
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="ASNA2019CC_datacreation.html">
        <span class="fa fa-code"></span>
         
        ASNA2019 Case Competition
      </a>
    </li>
    <li>
      <a href="Kaggle.html">
        <span class="fa fa-code"></span>
         
        Kaggle Competitions
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-code"></span>
     
    Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="RResources.html">
        <span class="fa fa-book"></span>
         
        RStudio / R resources
      </a>
    </li>
    <li>
      <a href="StatsResources.html">
        <span class="fa fa-book"></span>
         
        General Statistics resources
      </a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    About me
  </a>
</li>
<li>
  <a href="resume.pdf">
    <span class="fa fa-file-pdf-o"></span>
     
    Resume
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">STA314F19</h1>
<h4 class="author">Michal Malyska</h4>

</div>


<div id="preliminaries-and-r-setup" class="section level1">
<h1>Preliminaries and R setup</h1>
<pre class="r"><code>library(gridExtra)
library(car)</code></pre>
<pre><code>## Loading required package: carData</code></pre>
<pre class="r"><code>library(gapminder)
library(ggdendro)
library(dendextend)</code></pre>
<pre><code>## 
## ---------------------
## Welcome to dendextend version 1.12.0
## Type citation(&#39;dendextend&#39;) for how to cite the package.
## 
## Type browseVignettes(package = &#39;dendextend&#39;) for the package vignette.
## The github page is: https://github.com/talgalili/dendextend/
## 
## Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues
## Or contact: &lt;tal.galili@gmail.com&gt;
## 
##  To suppress this message use:  suppressPackageStartupMessages(library(dendextend))
## ---------------------</code></pre>
<pre><code>## 
## Attaching package: &#39;dendextend&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggdendro&#39;:
## 
##     theme_dendro</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     cutree</code></pre>
<pre class="r"><code>library(e1071)
library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ───────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✔ ggplot2 3.2.1     ✔ purrr   0.3.3
## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.4.0</code></pre>
<pre><code>## ── Conflicts ──────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::combine() masks gridExtra::combine()
## ✖ dplyr::filter()  masks stats::filter()
## ✖ dplyr::lag()     masks stats::lag()
## ✖ dplyr::recode()  masks car::recode()
## ✖ purrr::some()    masks car::some()</code></pre>
<pre class="r"><code>library(MASS)</code></pre>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     select</code></pre>
<pre class="r"><code>library(ISLR)
library(corrplot)</code></pre>
<pre><code>## corrplot 0.84 loaded</code></pre>
<pre class="r"><code>library(pROC)</code></pre>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<pre class="r"><code>library(class)
library(tidymodels)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;xts&#39;:
##   method     from
##   as.zoo.xts zoo</code></pre>
<pre><code>## Registered S3 methods overwritten by &#39;lme4&#39;:
##   method                          from
##   cooks.distance.influence.merMod car 
##   influence.merMod                car 
##   dfbeta.influence.merMod         car 
##   dfbetas.influence.merMod        car</code></pre>
<pre><code>## ── Attaching packages ──────────────────────────────────────────────────────────────────────── tidymodels 0.0.3 ──</code></pre>
<pre><code>## ✔ broom     0.5.2          ✔ recipes   0.1.7     
## ✔ dials     0.0.3          ✔ rsample   0.0.5     
## ✔ infer     0.5.0          ✔ yardstick 0.0.4.9000
## ✔ parsnip   0.0.3.1</code></pre>
<pre><code>## ── Conflicts ─────────────────────────────────────────────────────────────────────────── tidymodels_conflicts() ──
## ✖ dplyr::combine()  masks gridExtra::combine()
## ✖ scales::discard() masks purrr::discard()
## ✖ dplyr::filter()   masks stats::filter()
## ✖ recipes::fixed()  masks stringr::fixed()
## ✖ dplyr::lag()      masks stats::lag()
## ✖ dials::margin()   masks ggplot2::margin()
## ✖ dials::offset()   masks stats::offset()
## ✖ dials::prune()    masks dendextend::prune()
## ✖ dplyr::recode()   masks car::recode()
## ✖ MASS::select()    masks dplyr::select()
## ✖ purrr::some()     masks car::some()
## ✖ yardstick::spec() masks readr::spec()
## ✖ recipes::step()   masks stats::step()</code></pre>
<pre class="r"><code>library(yardstick)
library(parsnip)
library(leaps)
library(glmnet)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Attaching package: &#39;Matrix&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:tidyr&#39;:
## 
##     expand, pack, unpack</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## 
## Attaching package: &#39;foreach&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     accumulate, when</code></pre>
<pre><code>## Loaded glmnet 2.0-18</code></pre>
<pre><code>## 
## Attaching package: &#39;glmnet&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:pROC&#39;:
## 
##     auc</code></pre>
<pre class="r"><code>library(splines)</code></pre>
<div id="r-setup" class="section level2">
<h2>R Setup</h2>
<p>If you need help setting up R and Rstudio, I host a guide on my <a href="https://michalmalyska.github.io/RResources.html">resources page</a></p>
</div>
<div id="loading-data" class="section level2">
<h2>Loading Data</h2>
<p>You can find a fully commented guide on how to load in tabular data using the faster, tidyverse function read_csv() <a href="https://michalmalyska.github.io/RResources.html">here</a></p>
</div>
</div>
<div id="tutorial-3" class="section level1">
<h1>Tutorial 3</h1>
<div id="lab" class="section level2">
<h2>Lab</h2>
<pre class="r"><code>data_orig &lt;- MASS::Boston

df &lt;- as_tibble(data_orig)


df_train &lt;- df %&gt;% sample_n(size = length(df$chas) * 0.7)

df_test &lt;- setdiff(df, df_train)</code></pre>
<p>Let’s start by showing off what bad practice is and fit a linear model without doing any kind of previous work:</p>
<pre class="r"><code>model1 &lt;- lm(medv ~ lstat, data = df_train)
summary(model1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat, data = df_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.536  -4.246  -1.309   2.445  24.167 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 35.09018    0.67952   51.64   &lt;2e-16 ***
## lstat       -0.97138    0.04662  -20.84   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.427 on 352 degrees of freedom
## Multiple R-squared:  0.5523, Adjusted R-squared:  0.551 
## F-statistic: 434.2 on 1 and 352 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>plot(model1)</code></pre>
<p><img src="STA314F19_files/figure-html/Model%20Assumptions-1.png" width="576" /><img src="STA314F19_files/figure-html/Model%20Assumptions-2.png" width="576" /><img src="STA314F19_files/figure-html/Model%20Assumptions-3.png" width="576" /><img src="STA314F19_files/figure-html/Model%20Assumptions-4.png" width="576" /></p>
<p>Assumptions are not satisfied!</p>
<pre class="r"><code>ggplot(data = df, aes(x = medv, y = ..density..)) +
  geom_density() +
  geom_histogram(bins = 75, alpha = 0.3, fill = &quot;red&quot;) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Looking%20at%20distribution%20of%20response-1.png" width="576" /></p>
<pre class="r"><code>ggplot(data = df, aes(sample = medv)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Looking%20at%20distribution%20of%20response-2.png" width="576" /></p>
<pre class="r"><code>ggplot(data = df, aes(y = medv, x = lstat)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) +
  geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;), color = &quot;green&quot;) +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), color = &quot;blue&quot;) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Looking%20at%20distribution%20of%20response-3.png" width="576" /></p>
<p>How to make predictions:</p>
<pre class="r"><code>df_test$predictions &lt;- predict(model1, newdata = df_test)</code></pre>
<p>How to add multiple variables as predictors:</p>
<pre class="r"><code>model2 &lt;- lm(data = df_train, formula = medv ~ lstat + age)
summary(model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat + age, data = df_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.644  -4.114  -1.347   2.302  22.292 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 33.07995    0.89692   36.88  &lt; 2e-16 ***
## lstat       -1.08339    0.05671  -19.11  &lt; 2e-16 ***
## age          0.04952    0.01470    3.37 0.000837 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.334 on 351 degrees of freedom
## Multiple R-squared:  0.5663, Adjusted R-squared:  0.5639 
## F-statistic: 229.2 on 2 and 351 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>plot(model2)</code></pre>
<p><img src="STA314F19_files/figure-html/MLR-1.png" width="576" /><img src="STA314F19_files/figure-html/MLR-2.png" width="576" /><img src="STA314F19_files/figure-html/MLR-3.png" width="576" /><img src="STA314F19_files/figure-html/MLR-4.png" width="576" /></p>
<p>How to add everything (and subtract some) variables:</p>
<pre class="r"><code>model3 &lt;- lm(data = df_train, formula = medv ~ . -age -indus)
summary(model3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ . - age - indus, data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.7177  -2.8975  -0.6761   2.0959  24.4027 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  40.032269   6.305107   6.349 6.89e-10 ***
## crim         -0.115508   0.036030  -3.206  0.00147 ** 
## zn            0.049570   0.017606   2.816  0.00515 ** 
## chas          2.382488   1.074831   2.217  0.02731 *  
## nox         -17.693815   4.449224  -3.977 8.53e-05 ***
## rm            3.475670   0.493264   7.046 1.02e-11 ***
## dis          -1.540561   0.230279  -6.690 9.13e-11 ***
## rad           0.317900   0.084841   3.747  0.00021 ***
## tax          -0.010915   0.004546  -2.401  0.01689 *  
## ptratio      -0.945354   0.163499  -5.782 1.66e-08 ***
## black         0.007271   0.003299   2.204  0.02817 *  
## lstat        -0.588923   0.058238 -10.112  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.023 on 342 degrees of freedom
## Multiple R-squared:  0.7343, Adjusted R-squared:  0.7257 
## F-statistic: 85.92 on 11 and 342 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>How to add interactions:</p>
<pre class="r"><code>model4 &lt;- lm(data = df_train, formula = medv ~ . -age -indus + lstat:ptratio )
summary(model4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ . - age - indus + lstat:ptratio, data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.0005  -2.8065  -0.7496   1.9732  24.8779 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    46.096123   7.456654   6.182 1.81e-09 ***
## crim           -0.119896   0.036079  -3.323 0.000986 ***
## zn              0.044690   0.017865   2.502 0.012833 *  
## chas            2.477741   1.074635   2.306 0.021729 *  
## nox           -14.759380   4.844282  -3.047 0.002494 ** 
## rm              3.285472   0.508063   6.467 3.47e-10 ***
## dis            -1.433696   0.240409  -5.964 6.17e-09 ***
## rad             0.312116   0.084766   3.682 0.000269 ***
## tax            -0.011862   0.004581  -2.590 0.010018 *  
## ptratio        -1.290531   0.280122  -4.607 5.78e-06 ***
## black           0.007446   0.003294   2.260 0.024428 *  
## lstat          -1.198218   0.406072  -2.951 0.003389 ** 
## ptratio:lstat   0.031831   0.020995   1.516 0.130427    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.013 on 341 degrees of freedom
## Multiple R-squared:  0.7361, Adjusted R-squared:  0.7268 
## F-statistic: 79.25 on 12 and 341 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>How to write high level interactions:</p>
<pre class="r"><code>model5 &lt;- lm(data = df_train, formula = medv ~ lstat*ptratio*black)
summary(model5)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat * ptratio * black, data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.2771  -3.4380  -0.8151   1.6208  27.5704 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)         -2.747e+01  5.169e+01  -0.532   0.5954  
## lstat                1.842e+00  2.797e+00   0.659   0.5105  
## ptratio              2.330e+00  2.600e+00   0.896   0.3708  
## black                2.620e-01  1.352e-01   1.938   0.0535 .
## lstat:ptratio       -1.062e-01  1.402e-01  -0.758   0.4490  
## lstat:black         -1.110e-02  7.365e-03  -1.507   0.1328  
## ptratio:black       -1.168e-02  6.818e-03  -1.714   0.0875 .
## lstat:ptratio:black  4.956e-04  3.701e-04   1.339   0.1814  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.739 on 346 degrees of freedom
## Multiple R-squared:  0.649,  Adjusted R-squared:  0.6419 
## F-statistic:  91.4 on 7 and 346 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This is what our data looks like:</p>
<pre class="r"><code>ggplot(data = df_train, aes(x = lstat, y = medv)) +
  geom_point() +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Nonlinear%20factors-1.png" width="576" /></p>
<p>How to add a funciton of a variable as a predictor and how to make simple visualizations without adding predictions:</p>
<pre class="r"><code>model6 &lt;- lm(data = df_train, formula = medv ~ lstat + I(lstat^2))
summary(model6)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat + I(lstat^2), data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.5962  -4.1056  -0.4449   2.4601  25.1322 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 43.21406    1.04578  41.322   &lt;2e-16 ***
## lstat       -2.32680    0.14811 -15.710   &lt;2e-16 ***
## I(lstat^2)   0.04215    0.00442   9.535   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.736 on 351 degrees of freedom
## Multiple R-squared:  0.6444, Adjusted R-squared:  0.6424 
## F-statistic:   318 on 2 and 351 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>ggplot(data = df_train, aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2)) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/unnamed-chunk-2-1.png" width="576" /></p>
<p>How to fit higher degree polynomials and visualize them:</p>
<pre class="r"><code>model7 &lt;- lm(data = df_train, formula = medv ~ poly(lstat, degree = 5))
summary(model7)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ poly(lstat, degree = 5), data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.4603  -3.3061  -0.5051   2.1598  26.7790 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                22.8492     0.2905  78.663  &lt; 2e-16 ***
## poly(lstat, degree = 5)1 -133.9176     5.4651 -24.504  &lt; 2e-16 ***
## poly(lstat, degree = 5)2   54.6901     5.4651  10.007  &lt; 2e-16 ***
## poly(lstat, degree = 5)3  -21.5431     5.4651  -3.942 9.77e-05 ***
## poly(lstat, degree = 5)4   21.6483     5.4651   3.961 9.05e-05 ***
## poly(lstat, degree = 5)5  -14.8336     5.4651  -2.714  0.00697 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.465 on 348 degrees of freedom
## Multiple R-squared:  0.6799, Adjusted R-squared:  0.6753 
## F-statistic: 147.8 on 5 and 348 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>ggplot(data = df_train, aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, degree = 5)) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/unnamed-chunk-3-1.png" width="576" /></p>
<p>How to change plotting settings in base R (For tidyverse you should use gridExtra)</p>
<pre class="r"><code>par(mfrow = c(2,2))
plot(model6)</code></pre>
<p><img src="STA314F19_files/figure-html/parmfrow-1.png" width="576" /></p>
<p>Fitting character variables:</p>
<pre class="r"><code>df2 &lt;- ISLR::Carseats

df2_train &lt;- sample_n(df2, size = length(df2$Sales) * 0.7)
df2_test &lt;- setdiff(df2, df2_train)

glimpse(df2_train)</code></pre>
<pre><code>## Observations: 280
## Variables: 11
## $ Sales       &lt;dbl&gt; 3.89, 9.32, 8.54, 6.87, 6.20, 8.19, 12.85, 7.82, 6.8…
## $ CompPrice   &lt;dbl&gt; 123, 141, 139, 128, 128, 127, 123, 124, 132, 121, 15…
## $ Income      &lt;dbl&gt; 105, 34, 35, 105, 93, 103, 37, 25, 61, 42, 53, 89, 9…
## $ Advertising &lt;dbl&gt; 0, 16, 0, 11, 0, 0, 15, 13, 0, 4, 0, 10, 15, 11, 9, …
## $ Population  &lt;dbl&gt; 149, 361, 95, 249, 89, 125, 348, 87, 263, 188, 403, …
## $ Price       &lt;dbl&gt; 118, 108, 129, 131, 118, 155, 112, 110, 125, 118, 12…
## $ ShelveLoc   &lt;fct&gt; Bad, Medium, Medium, Medium, Medium, Good, Good, Med…
## $ Age         &lt;dbl&gt; 62, 69, 42, 63, 34, 29, 28, 57, 41, 54, 58, 28, 58, …
## $ Education   &lt;dbl&gt; 16, 10, 13, 13, 18, 15, 12, 10, 12, 15, 16, 10, 17, …
## $ Urban       &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, No, Yes, Yes,…
## $ US          &lt;fct&gt; Yes, Yes, No, Yes, No, Yes, Yes, Yes, No, Yes, No, Y…</code></pre>
<pre class="r"><code># Fit a model with a catergorical variable
model7 &lt;- lm(data = df2_train, formula = Sales ~ ShelveLoc)
summary(model7)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Sales ~ ShelveLoc, data = df2_train)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -7.184 -1.718  0.091  1.643  6.055 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       5.6154     0.2910  19.294  &lt; 2e-16 ***
## ShelveLocGood     4.7972     0.4238  11.319  &lt; 2e-16 ***
## ShelveLocMedium   1.7286     0.3461   4.995 1.04e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.346 on 277 degrees of freedom
## Multiple R-squared:  0.3222, Adjusted R-squared:  0.3173 
## F-statistic: 65.84 on 2 and 277 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># What are ShelveLocGood and ShelveLocMedium 
contrasts(df2$ShelveLoc)</code></pre>
<pre><code>##        Good Medium
## Bad       0      0
## Good      1      0
## Medium    0      1</code></pre>
<pre class="r"><code>Load_packages &lt;- function(){
  library(tidyverse)
  library(MASS)
  library(ISLR)
}</code></pre>
<pre class="r"><code>Load_packages()</code></pre>
<p>If you would like to see some more advanced uses of the techniques; I <a href="https://michalmalyska.github.io/STA314.html#r_for_data_science_chapter_25:_many_models">covered</a> the same lab and a chapter of the R for data science textbook that deals with fitting multiple models to the same dataset using tidyverse functions.</p>
</div>
</div>
<div id="tutorial-4" class="section level1">
<h1>Tutorial 4</h1>
<div id="homework-solutions-overview" class="section level2">
<h2>Homework Solutions Overview:</h2>
<div id="question-1" class="section level3">
<h3>Question 1</h3>
<ol style="list-style-type: lower-alpha">
<li><p>We expect that with a very large number of measurements n, a flexible learning method would be able to learn the signal without as much fear of overfitting.</p></li>
<li><p>If the number of predictors p is very large and n is small then there is a greater possibility that a flexible learning method would overfit. Then we expect the inflexible method to be better in this case.</p></li>
<li><p>A highly non-linear relationship would most likely need a flexible statistical learning method to perform optimally.</p></li>
<li><p>With a very large error term variance <span class="math inline">\(\sigma^2\)</span> there is more worry about overfitting with flexible methods and thus an inflexible method would perform better.</p></li>
</ol>
</div>
<div id="question-2" class="section level3">
<h3>Question 2</h3>
<p>Bias Variance decomposition:</p>
<p><span class="math display">\[ 
\mathbb{E}  \left[(y_0 - \hat{f}(x_0))^2 \right] =
\mathbb{V}ar(\hat{f}(x_0)) + \left( \mathbb{E} \hat{f}(x_0) - \mathbb{E}y_0 \right)^2 + \mathbb{V}ar(\epsilon)
\]</span> Or equivalently:</p>
<p><span class="math display">\[ 
\mathbb{E}  \left[(y_0 - \hat{y}_0)^2 \right] =
\mathbb{V}ar(\hat{y}_0) + \left( \mathbb{E}\hat{y}_0 - \mathbb{E}y_0 \right)^2 + \mathbb{V}ar(\epsilon)
\]</span></p>
<p>The left-hand-side of the above is the expected mean square error (MSE) or a measure of how well on average our approximation function at <span class="math inline">\(x_0\)</span> is estimating the true value <span class="math inline">\(y_0\)</span>. The first term on the right-hand-side of the above expression is the error in the MSE due to errors in “fitting” the true <span class="math inline">\(f\)</span> with our approximate <span class="math inline">\(\hat{f}\)</span>. This error come from the sensitivity of the learning procedure to the finite training data set. Typically more flexible fitting methods will be more sensitive to the errors and noise in the given training set. The second term on the right-hand-side is the error in <span class="math inline">\(f \neq \hat{f}\)</span> due to using a learning algorithm that might not be able to represent the complexities in <span class="math inline">\(f\)</span>. For example, taking to be linear when the true underlying function <span class="math inline">\(f\)</span> is non-linear. The third term on the right-hand-side represents un-learnable error due to either not having all the predictive variables in our model (predictors that if we could get values for would improve our ability to learn the function <span class="math inline">\(\hat{f}\)</span> or error that is just intrinsic to the process which we are trying to model. In either case, given the data we have there is no way to reduce this component of the MSE error. A typical plots of the things suggested look like pieces from figures on pages 3 and 7 from the Week 2 Tuesday lecture notes. In figure on page 3 (right-hand-side) we have plots of the training error, testing error and the irreducible error curves. The training error shows a steady decrease (improvement) as the flexibility of the learning method increases. The test error is the red curve that initially decreases as the flexibility increase but then begins to increase again for continued increase in flexibility. The irreducible error is the constant dotted line. Notice that the point where the testing error is as close to the irreducible error would be the optimal operating point for this system. The distance between the lowest point on the testing error curve and the irreducible error gives an indication of how much bias there is in the given learning procedure, i.e. how far the best function <span class="math inline">\(\hat{f}\)</span> will be from <span class="math inline">\(f\)</span>. In figure on page 7 (left plot) we have curves representing the individual components of the bias-variance decomposition. The blue curve is the squared bias which we see gets smaller as the complexity of the model increase (we are able to model more and more complicated patters in <span class="math inline">\(\hat{f}\)</span>. The orange curve shows the variance of the learned model i.e. as we add complexity (more flexibility), the dependence on the dataset increases. So what function we get out of our learning procedure gets more sensitive to errors/ noise in the training dataset (and the error increase). The horizontal line is the irreducible error again.</p>
</div>
<div id="question-3" class="section level3">
<h3>Question 3</h3>
<p>A very flexible fitting procedure will fit non-linear functions better (if that is indeed the model generation process that is generating your data) but will be more susceptible to errors/ noise in the training dataset. A less flexible approach exchanges where it makes errors. That is a less flexible fitting procedure is unable to model the exact non-linear f but its predictions are also likely to be more stable to errors / noise in the training dataset.</p>
</div>
<div id="question-4" class="section level3">
<h3>Question 4</h3>
<p>A parametric learning procedure means the functional form of the mapping <span class="math inline">\(f\)</span> is specified, except for the parameter values, which the learning procedure must estimate. A non-parametric learning procedure is much more flexible in the forms of <span class="math inline">\(f\)</span> it can model and the learning procedure must “learn more” from the data (the functional form that should take) and then the parameters needed to estimate it. A parametric approach is generally a less flexible fitting method while a non-parametric approach is a more flexible method with the trade-offs that that characterization contains.</p>
</div>
</div>
<div id="islr-chapter-3-exercise-5" class="section level2">
<h2>ISLR Chapter 3 Exercise 5</h2>
<p><span class="math display">\[
\hat{y}_{i} =
x_{i} \frac{\sum_{i&#39;=1}^{n}\left( x_{i&#39;} y_{i&#39;} \right)}{\sum_{j=1}^{n} x_{j}^{2}}
=\sum_{i&#39;=1}^n \frac{\left(x_i x_{i&#39;}  \right)}{\sum_{j=1}^n x_j^2}y_{i&#39;} = 
\sum_{i&#39;=1}^n a_{i&#39;} y_{i&#39;}
\]</span></p>
</div>
<div id="islr-chapter-3-exercise-7" class="section level2">
<h2>ISLR Chapter 3 Exercise 7</h2>
<p>Given that <span class="math inline">\(\bar{x} = \bar{y} = 0\)</span></p>
<p><span class="math display">\[
R^2 = \frac{TSS - RSS}{TSS} = 1-  \frac{RSS}{TSS}
\]</span></p>
<p><span class="math display">\[
TSS = \sum_i \left( y_i - \bar{y} \right)^ 2 = \sum_i y_i^2
\]</span></p>
<p><span class="math display">\[ 
RSS = \sum_i \left(y_i - \hat{y_i} \right)^2 =
\sum_i \left(y_i - (\hat{\beta_0} + \hat{\beta_1} x_i) \right)^2
\]</span></p>
<p>now noting that:</p>
<p><span class="math display">\[
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x} = 0
\]</span></p>
<p>We simplify to get</p>
<p><span class="math display">\[
\sum_i \left(y_i - (\hat{\beta_0} + \hat{\beta_1} x_i) \right)^2 =
\sum_i \left(y_i - (\hat{\beta_1} x_i) \right)^2
\]</span></p>
<p>Then looking at formula for <span class="math inline">\(\hat{\beta_1}\)</span>:</p>
<p><span class="math display">\[
\hat{\beta_1} = \frac{\sum_i (x_i - \bar{x}) (y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} = \\
\frac{\sum_i x_iy_i}{\sum_i x_i^2}
\]</span></p>
<p>Plugging in and expanding</p>
<p><span class="math display">\[
\sum_i \left(y_i - (\hat{\beta_1} x_i) \right)^2 = 
\sum_i \left(y_i^2 - 2(\hat{\beta_1} x_i)y_i + (\hat{\beta_1} x_i)^2 \right) = \\
= \sum_i \left(y_i^2 - 2(\frac{\sum_i x_iy_i}{\sum_i x_i^2} x_i)y_i + (\frac{\sum_i x_iy_i}{\sum_i x_i^2} x_i)^2 \right) = \\
= \sum_i y_i^2 - 2 \frac{\sum_i x_iy_i}{\sum_i x_i^2} \sum_i x_i y_i +
\left( \frac{\sum_i x_iy_i}{\sum_i x_i^2} \right)^2 \sum_ix_i^2 = \\
= \sum_i y_i^2 - \frac{(\sum_i x_iy_i)^2}{\sum_i x_i^2}
\]</span></p>
<p>Plugging into the <span class="math inline">\(R^2\)</span> formula:</p>
<p><span class="math display">\[ 
R^2 =  \frac{TSS - RSS}{TSS} = \frac{\sum_i y_i^2 - \sum_i y_i^2 +
\frac{(\sum_i x_iy_i)^2}{\sum_i x_i^2}}{\sum_i y_i^2} = 
\frac{(\sum_i x_iy_i)^2}{\sum_i x_i^2 \sum_i y_i^2}
\]</span></p>
<p>Correlation is (since <span class="math inline">\(\bar{x} = \bar{y} = 0\)</span>):</p>
<p><span class="math display">\[ 
Corr(X,Y) = \frac{(\sum_i x_iy_i)}{\sqrt{\sum_i x_i^2}\sqrt{\sum_i y_i^2}}
\]</span></p>
</div>
<div id="additional-materials" class="section level2">
<h2>Additional Materials</h2>
<p>Great <a href="https://www.alexpghayes.com/blog/understanding-multinomial-regression-with-partial-dependence-plots/">blogpost</a> on understanding Mutinomial regression with Partial Dependence plots.</p>
<p>If you want to type up your homework 2 in RMarkdown and save on time I would suggest taking a look at the equatiomatic package. (This is also super useful for STA303)</p>
</div>
<div id="tutorial-questions" class="section level2">
<h2>Tutorial Questions</h2>
<ul>
<li>Why does logistic regression become unstable for linearly separable data?</li>
</ul>
<p>Answer:</p>
<p>If the data is perfectly linearly separable (meaning that you can draw a hyperplane such that all datapoints of either class are on different sides of it). Then the MLE solution to logistic regression does not exist. What this would mean when you are estimating coefficients is that as you train your model the estimates are going to diverge to infinity as your logistic regression is trying to approximate a step function. A good description of this can be found <a href="https://stats.stackexchange.com/a/254266">here</a></p>
</div>
</div>
<div id="tutorial-5" class="section level1">
<h1>Tutorial 5</h1>
<div id="lab-4-islr" class="section level2">
<h2>Lab 4 ISLR</h2>
<pre class="r"><code># Load the data
data_orig &lt;- ISLR::Smarket
df &lt;- data_orig

df_train &lt;- df %&gt;% sample_n(size = 0.7*1250)
df_test &lt;- setdiff(df, df_train)</code></pre>
<p>Some simple data exploration</p>
<pre class="r"><code># Tidyverse way of subsetting data and computing correlations
corrs &lt;- df %&gt;%
  dplyr::select(-Direction) %&gt;%
  cor()
corrs</code></pre>
<pre><code>##              Year         Lag1         Lag2         Lag3         Lag4
## Year   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718
## Lag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911
## Lag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533
## Lag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036
## Lag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000
## Lag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641
## Volume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246
## Today  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527
##                Lag5      Volume        Today
## Year    0.029787995  0.53900647  0.030095229
## Lag1   -0.005674606  0.04090991 -0.026155045
## Lag2   -0.003557949 -0.04338321 -0.010250033
## Lag3   -0.018808338 -0.04182369 -0.002447647
## Lag4   -0.027083641 -0.04841425 -0.006899527
## Lag5    1.000000000 -0.02200231 -0.034860083
## Volume -0.022002315  1.00000000  0.014591823
## Today  -0.034860083  0.01459182  1.000000000</code></pre>
<pre class="r"><code># Easy way to visualize correlations
corrplot(corrs, method = &quot;color&quot;, type = &quot;lower&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/Lab%204%20correlations-1.png" width="576" /></p>
<p>Pretty much no correlations between our predictors</p>
<p>Below is the “hacky” way to do correlations for variables that are not numerical. You should always stop and think whether what you are doing makes sense.</p>
<pre class="r"><code>corrs &lt;- df %&gt;%
  mutate(Direction = if_else(Direction == &quot;Up&quot;, 1, 0)) %&gt;% # Convert the string to a number
  cor()

corrplot(corrs, method = &quot;color&quot;, type = &quot;lower&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/Hacky%20way%20to%20do%20correlations-1.png" width="576" /></p>
<p>Our response is only correlated with Today. This means that we probably won’t see much statistical significance. (Note that this is not really a very rigorous approach but it’s a good benchmark to see which variables have the highest potential)</p>
<p>Fit a Logistic Regression using all the lag variables and volume.</p>
<pre class="r"><code>log_reg &lt;- glm(Direction ~ . - Today ,data = df_train , family = binomial)
summary(log_reg)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Direction ~ . - Today, family = binomial, data = df_train)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.372  -1.184   1.024   1.156   1.378  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -1.613e+02  1.144e+02  -1.410    0.158
## Year         8.047e-02  5.719e-02   1.407    0.159
## Lag1        -7.372e-02  5.817e-02  -1.267    0.205
## Lag2        -2.311e-02  5.990e-02  -0.386    0.700
## Lag3        -1.255e-03  6.108e-02  -0.021    0.984
## Lag4         3.672e-02  5.949e-02   0.617    0.537
## Lag5         5.592e-03  6.040e-02   0.093    0.926
## Volume       1.058e-01  2.214e-01   0.478    0.633
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1212.9  on 874  degrees of freedom
## Residual deviance: 1206.6  on 867  degrees of freedom
## AIC: 1222.6
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<p>We can either make prediction on just the movement direction (by higher probability) or we can return a vector of probaabilities for each case. In the binary case it is enough to specify one. Specifying type = “response” makes sure that instead of returning the log odds, we get the probabilites directly.</p>
<pre class="r"><code>probabilities = predict(log_reg, type = &quot;response&quot;, newdata = df_test) # Extract the probabilites
glimpse(probabilities)</code></pre>
<pre><code>##  Named num [1:375] 0.436 0.462 0.464 0.469 0.476 ...
##  - attr(*, &quot;names&quot;)= chr [1:375] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...</code></pre>
<pre class="r"><code>contrasts(data_orig$Direction) # Which way is Up?</code></pre>
<pre><code>##      Up
## Down  0
## Up    1</code></pre>
<pre class="r"><code># Set Threshold
threshold &lt;- 0.5

# Pick predictions above the threshold
predictions &lt;- rep(&quot;Down&quot;,length(df_test$Year)) #initialize a vector of all predicitons to be Down
predictions[probabilities &gt; 0.5] &lt;- &quot;Up&quot; # Change those with probabilities higher
# than threshold to be Up

# Actual values
actual &lt;- df_test$Direction

# Generate a truth table for our predictions
table(predictions, actual)</code></pre>
<pre><code>##            actual
## predictions Down  Up
##        Down   67  63
##        Up    103 142</code></pre>
<p>This means we missed 113 + 66 out of the total of 375 on our test set. We separate them because cost of making an error in one way may be higher than the other.</p>
<p>Now let’s train an LDA model on the same variables (this is not the same as in the lab)</p>
<pre class="r"><code># Train the model
lda_model &lt;- MASS::lda(Direction~. - Today, data = df_train)
# Print out the model summary.
lda_model</code></pre>
<pre><code>## Call:
## lda(Direction ~ . - Today, data = df_train)
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4937143 0.5062857 
## 
## Group means:
##          Year        Lag1         Lag2        Lag3          Lag4
## Down 2002.861  0.06133333  0.021627315 -0.02647222 -0.0560856481
## Up   2003.050 -0.03654853 -0.004765237 -0.01984199  0.0002167043
##             Lag5   Volume
## Down -0.02420833 1.452340
## Up   -0.01636795 1.486261
## 
## Coefficients of linear discriminants:
##                 LD1
## Year    0.475809419
## Lag1   -0.434358297
## Lag2   -0.136352827
## Lag3   -0.007657301
## Lag4    0.216066751
## Lag5    0.032644613
## Volume  0.621508949</code></pre>
<pre class="r"><code># If you work with the object explorer in R you can get these formulas for how 
# to extract predictions classes and probabilities
predictions &lt;- predict(lda_model, newdata = df_test)[[&quot;class&quot;]]
probabilities &lt;- predict(lda_model, newdata = df_test)[[&quot;posterior&quot;]]
# Use Base R to find probabilities for Up
probabilities &lt;- probabilities[,2]

predictions &lt;- rep(&quot;Down&quot;,length(df_test$Year)) #initialize a vector of all predicitons to be Down
predictions[probabilities &gt; 0.5] &lt;- &quot;Up&quot; # Change those with probabilities higher

table(predictions, actual)</code></pre>
<pre><code>##            actual
## predictions Down  Up
##        Down   67  63
##        Up    103 142</code></pre>
<p>In the similar way we can do QDA:</p>
<pre class="r"><code># Train the model
qda_model &lt;- MASS::qda(Direction~. - Today, data = df_train)
# Print out the model summary.
qda_model</code></pre>
<pre><code>## Call:
## qda(Direction ~ . - Today, data = df_train)
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4937143 0.5062857 
## 
## Group means:
##          Year        Lag1         Lag2        Lag3          Lag4
## Down 2002.861  0.06133333  0.021627315 -0.02647222 -0.0560856481
## Up   2003.050 -0.03654853 -0.004765237 -0.01984199  0.0002167043
##             Lag5   Volume
## Down -0.02420833 1.452340
## Up   -0.01636795 1.486261</code></pre>
<pre class="r"><code># If you work with the object explorer in R you can get these formulas for how 
# to extract predictions classes and probabilities
predictions &lt;- predict(qda_model, newdata = df_test)[[&quot;class&quot;]]
probabilities &lt;- predict(qda_model, newdata = df_test)[[&quot;posterior&quot;]]
# Use Base R to find probabilities for Up
probabilities &lt;- probabilities[,2]

predictions &lt;- rep(&quot;Down&quot;,length(df_test$Year)) #initialize a vector of all predicitons to be Down
predictions[probabilities &gt; 0.5] &lt;- &quot;Up&quot; # Change those with probabilities higher

table(predictions, actual)</code></pre>
<pre><code>##            actual
## predictions Down  Up
##        Down   55  85
##        Up    115 120</code></pre>
<p>Finally , we will look at the K-nearest neighbourghs algorithm (KNN). It is inceredibly simple and used a lot in practice for a number of tasks. Not so much just simple classification but nevertheless it’s an extremely important algorithm for you to know.</p>
<p>Because it uses euclidean distance you need to standardize all of your variables.</p>
<pre class="r"><code># Create the scaling function cause the regular one outputs a matrix instead 
# of a vector
scale2 &lt;- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)


df_train_std &lt;- df_train %&gt;%
  dplyr::select(-Direction , -Today) %&gt;% # Remove the variables we are not considering and response
  mutate_all(.funs = scale2) # I apply the scale2() function to all other variables.
df_test_std &lt;- df_test %&gt;%
  dplyr::select(-Direction , -Today) %&gt;% # Remove the variables we are not considering and response
  mutate_all(.funs = scale2) # I apply the scale2() function to all other variables.</code></pre>
<p>Now I will use the KNN function from “class” library. To fit a knn model. It requires a training and testing dataset which is what we should have been using this entire tutorial.</p>
<pre class="r"><code>predictions &lt;- knn(train = df_train_std, # Training Set
                   test = df_test_std, # Test Set
                   cl = df_train$Direction, # Labels for Training
                   k = 3) # 3 nearest Neighbours
table(predictions, actual)</code></pre>
<pre><code>##            actual
## predictions Down  Up
##        Down   79  94
##        Up     91 111</code></pre>
<p>It’s not a model you train that has parameters so there is no summary, no coefficients, no p-values. How do you pick k? Looking at a validation set seems like a reasonable approach.</p>
<pre class="r"><code>val_indices &lt;- sample.int(n = 0.7*nrow(df_train), replace = FALSE)

df_train_std_v &lt;- df_train_std[val_indices,]
df_val_std &lt;- df_train_std[-val_indices,]
train_v_labels &lt;- df_train[val_indices,] %&gt;% pull(Direction)
val_labels &lt;- df_train[-val_indices,] %&gt;% pull(Direction)

for (k in 1:15) {
  predictions &lt;- knn(train = df_train_std_v, # Training Set
                   test = df_val_std, # Test Set
                   cl = train_v_labels, # Labels for Training
                   k = k,# k nearest Neighbours
                   use.all = TRUE) # if there is a Tie use all that tied.
  
cat(&quot;k is&quot;, k, &quot;\n&quot;)
# Accuracy
print(mean(predictions == val_labels))
# Truth Tables
print(table(predictions, val_labels))
}</code></pre>
<pre><code>## k is 1 
## [1] 0.4562738
##            val_labels
## predictions Down Up
##        Down   57 61
##        Up     82 63
## k is 2 
## [1] 0.5095057
##            val_labels
## predictions Down Up
##        Down   63 53
##        Up     76 71
## k is 3 
## [1] 0.4562738
##            val_labels
## predictions Down Up
##        Down   51 55
##        Up     88 69
## k is 4 
## [1] 0.4752852
##            val_labels
## predictions Down Up
##        Down   53 52
##        Up     86 72
## k is 5 
## [1] 0.4790875
##            val_labels
## predictions Down Up
##        Down   55 53
##        Up     84 71
## k is 6 
## [1] 0.5247148
##            val_labels
## predictions Down Up
##        Down   65 51
##        Up     74 73
## k is 7 
## [1] 0.513308
##            val_labels
## predictions Down Up
##        Down   60 49
##        Up     79 75
## k is 8 
## [1] 0.4828897
##            val_labels
## predictions Down Up
##        Down   59 56
##        Up     80 68
## k is 9 
## [1] 0.4904943
##            val_labels
## predictions Down Up
##        Down   53 48
##        Up     86 76
## k is 10 
## [1] 0.4942966
##            val_labels
## predictions Down Up
##        Down   51 45
##        Up     88 79
## k is 11 
## [1] 0.4980989
##            val_labels
## predictions Down Up
##        Down   52 45
##        Up     87 79
## k is 12 
## [1] 0.486692
##            val_labels
## predictions Down Up
##        Down   53 49
##        Up     86 75
## k is 13 
## [1] 0.4790875
##            val_labels
## predictions Down Up
##        Down   50 48
##        Up     89 76
## k is 14 
## [1] 0.4676806
##            val_labels
## predictions Down Up
##        Down   50 51
##        Up     89 73
## k is 15 
## [1] 0.4942966
##            val_labels
## predictions Down Up
##        Down   52 46
##        Up     87 78</code></pre>
<p>k vary a lot between runs so I would probably pick something around 4-5.</p>
</div>
</div>
<div id="tutorial-6" class="section level1">
<h1>Tutorial 6</h1>
<div id="agenda" class="section level2">
<h2>Agenda:</h2>
<ul>
<li><p>Review of logistic regression etc. (now with tidymodels)</p></li>
<li><p>ROC and AUC</p></li>
</ul>
</div>
<div id="tidymodels" class="section level2">
<h2>Tidymodels</h2>
<p>This will be a tiny bit slower than usual as I am new to tidymodels.</p>
<p>Goals:</p>
<ul>
<li><p>Prepare data (recipes)</p></li>
<li><p>Split data (rsample)</p></li>
<li><p>Fit models</p></li>
<li><p>Analyze models (broom)</p></li>
<li><p>Show ROC and AUC metrics (yardstick)</p></li>
</ul>
</div>
<div id="prepare-data" class="section level2">
<h2>Prepare Data</h2>
<pre class="r"><code>df &lt;- data_orig # I will re-load the original dataset from last week
df &lt;- df %&gt;% dplyr::select(-Today) # remove the column we don&#39;t care about</code></pre>
<p>Create the data split for Training and Testing</p>
<pre class="r"><code>df_split &lt;- df %&gt;% rsample::initial_split(prop = 0.7)
df_split # This outputs numbers of rows for &lt;training, testing, total&gt;</code></pre>
<pre><code>## &lt;875/375/1250&gt;</code></pre>
<pre class="r"><code># Can access the datasets through training() or testing() functions
df_split %&gt;% training() %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 875
## Variables: 8
## $ Year      &lt;dbl&gt; 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, …
## $ Lag1      &lt;dbl&gt; 0.381, 0.959, 1.032, -0.623, 0.213, 1.392, 0.027, 0.28…
## $ Lag2      &lt;dbl&gt; -0.192, 0.381, 0.959, 1.032, 0.614, 0.213, -0.403, 1.3…
## $ Lag3      &lt;dbl&gt; -2.624, -0.192, 0.381, 0.959, -0.623, 0.614, 1.392, 0.…
## $ Lag4      &lt;dbl&gt; -1.055, -2.624, -0.192, 0.381, 1.032, -0.623, 0.213, -…
## $ Lag5      &lt;dbl&gt; 5.010, -1.055, -2.624, -0.192, 0.959, 1.032, 0.614, 1.…
## $ Volume    &lt;dbl&gt; 1.1913, 1.2965, 1.4112, 1.2760, 1.3491, 1.4450, 1.1640…
## $ Direction &lt;fct&gt; Up, Up, Down, Up, Up, Down, Up, Down, Up, Up, Down, Do…</code></pre>
<pre class="r"><code>df_split %&gt;% testing() %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 375
## Variables: 8
## $ Year      &lt;dbl&gt; 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, …
## $ Lag1      &lt;dbl&gt; 0.614, -0.403, 1.303, -0.498, 0.680, 0.701, -0.562, 0.…
## $ Lag2      &lt;dbl&gt; -0.623, 1.392, 0.027, 0.287, -0.189, 0.680, 0.701, -0.…
## $ Lag3      &lt;dbl&gt; 1.032, 0.213, -0.403, 1.303, -0.498, -0.189, 0.680, 0.…
## $ Lag4      &lt;dbl&gt; 0.959, 0.614, 1.392, 0.027, 0.287, -0.498, -0.189, 0.6…
## $ Lag5      &lt;dbl&gt; 0.381, -0.623, 0.213, -0.403, 1.303, 0.287, -0.498, -0…
## $ Volume    &lt;dbl&gt; 1.2057, 1.4078, 1.2326, 1.2580, 1.0531, 1.1498, 1.2953…
## $ Direction &lt;fct&gt; Up, Up, Up, Down, Up, Down, Up, Down, Down, Down, Up, …</code></pre>
<pre class="r"><code># This can be used to make sure that your samples are reasonable by using 
# summarize.

df_split %&gt;% training() %&gt;%
  summarise_if(is.numeric,list(~min(.),
                               ~max(.),
                               ~mean(.),
                               ~median(.),
                               ~sd(.))) -&gt; sum1

df_split %&gt;% testing() %&gt;%
  summarise_if(is.numeric,list(~min(.),
                               ~max(.),
                               ~mean(.),
                               ~median(.),
                               ~sd(.))) -&gt; sum2


# Or tidyverse way
rbind(sum1, sum2) %&gt;% # Combine the two datasets
  rownames_to_column() %&gt;%
  gather(var, value, -rowname) %&gt;% 
  spread(rowname, value) # Transpose</code></pre>
<pre><code>##              var             1             2
## 1       Lag1_max  5.733000e+00  4.368000e+00
## 2      Lag1_mean -2.292914e-02  6.628267e-02
## 3    Lag1_median  1.000000e-03  1.000000e-01
## 4       Lag1_min -4.318000e+00 -4.922000e+00
## 5        Lag1_sd  1.146519e+00  1.111089e+00
## 6       Lag2_max  5.408000e+00  5.733000e+00
## 7      Lag2_mean  7.024000e-03 -3.325333e-03
## 8    Lag2_median  4.600000e-02  1.400000e-02
## 9       Lag2_min -4.922000e+00 -4.318000e+00
## 10       Lag2_sd  1.163373e+00  1.071872e+00
## 11      Lag3_max  5.733000e+00  5.408000e+00
## 12     Lag3_mean -2.259429e-03  1.099200e-02
## 13   Lag3_median  1.600000e-02  5.400000e-02
## 14      Lag3_min -4.922000e+00 -4.154000e+00
## 15       Lag3_sd  1.135282e+00  1.148115e+00
## 16      Lag4_max  5.733000e+00  4.734000e+00
## 17     Lag4_mean -2.017143e-02  5.252000e-02
## 18   Lag4_median  2.700000e-02  5.700000e-02
## 19      Lag4_min -4.922000e+00 -3.523000e+00
## 20       Lag4_sd  1.180201e+00  1.035404e+00
## 21      Lag5_max  5.733000e+00  4.368000e+00
## 22     Lag5_mean  3.102286e-02 -5.368800e-02
## 23   Lag5_median  5.900000e-02 -2.400000e-02
## 24      Lag5_min -4.318000e+00 -4.922000e+00
## 25       Lag5_sd  1.160736e+00  1.115449e+00
## 26    Volume_max  3.152470e+00  2.792030e+00
## 27   Volume_mean  1.476949e+00  1.481470e+00
## 28 Volume_median  1.413700e+00  1.445800e+00
## 29    Volume_min  4.396700e-01  3.560700e-01
## 30     Volume_sd  3.520603e-01  3.794773e-01
## 31      Year_max  2.005000e+03  2.005000e+03
## 32     Year_mean  2.003007e+03  2.003037e+03
## 33   Year_median  2.003000e+03  2.003000e+03
## 34      Year_min  2.001000e+03  2.001000e+03
## 35       Year_sd  1.418236e+00  1.388915e+00</code></pre>
<p>Now we would like to preprocess our data to be mean 0 and standard deviation of 1. We will be using the recipe package.</p>
<pre class="r"><code>df_recipe &lt;- df_split %&gt;% 
  training() %&gt;%
  recipe(Direction ~ .) %&gt;%
  step_center(all_predictors(), -all_outcomes()) %&gt;%
  step_scale(all_predictors(), -all_outcomes()) %&gt;% # I have no idea why this did not run during tutorial
  prep(retain = TRUE)
df_recipe</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          7
## 
## Training data contained 875 data points and no missing data.
## 
## Operations:
## 
## Centering for Year, Lag1, Lag2, Lag3, Lag4, Lag5, Volume [trained]
## Scaling for Year, Lag1, Lag2, Lag3, Lag4, Lag5, Volume [trained]</code></pre>
<p>Now we would like to apply the same transformations to the testing dataset. The names are lovely, I know.</p>
<pre class="r"><code># Create a test data set
df_test &lt;- df_recipe %&gt;% bake(testing(df_split))
# Create the training data set
df_train &lt;- juice(df_recipe)</code></pre>
</div>
<div id="prepare-the-model" class="section level2">
<h2>Prepare the model</h2>
<p>Now that we have prepared our training and testing datasets we should prepare our models. We will be using the parsnip library which unifies the calls to models. So you no longer will have to remember that for a glm you have to specify type = “response”, and for LDA you don’t. The way we define models will come in handy when we do multiple flavours of the same model with different regularizations.</p>
<pre class="r"><code># We start with logistic regression
dir_model_logreg &lt;- logistic_reg() # Specify what kind of model we are building
dir_model_logreg # Model Specification</code></pre>
<pre><code>## Logistic Regression Model Specification (classification)</code></pre>
<pre class="r"><code># We can pick which &quot;engine&quot; we will use for the model. For logistic regression
# That should be one of glm or glmnet. 
dir_model_logreg &lt;- dir_model_logreg %&gt;%
  set_engine(&quot;glm&quot;)
dir_model_logreg</code></pre>
<pre><code>## Logistic Regression Model Specification (classification)
## 
## Computational engine: glm</code></pre>
<pre class="r"><code># Fit the model
logreg_fit &lt;- dir_model_logreg %&gt;% fit(Direction ~ ., data = df_train)
# Instead of using the summary() function we can use the tidy() function from
# broom to get a data_frame. 
tidy(logreg_fit)</code></pre>
<pre><code>## # A tibble: 8 x 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)  0.0894     0.0681    1.31    0.189 
## 2 Year         0.208      0.0810    2.57    0.0101
## 3 Lag1        -0.135      0.0692   -1.95    0.0511
## 4 Lag2        -0.0720     0.0687   -1.05    0.294 
## 5 Lag3         0.00191    0.0686    0.0279  0.978 
## 6 Lag4        -0.0142     0.0691   -0.205   0.837 
## 7 Lag5         0.0217     0.0684    0.317   0.752 
## 8 Volume      -0.133      0.0814   -1.63    0.103</code></pre>
</div>
<div id="make-predictions" class="section level2">
<h2>Make Predictions</h2>
<p>Now we would like to make predictions and evaluate them.</p>
<pre class="r"><code># We can combine everything into one pipe!
logreg_results &lt;- df_test %&gt;%
  dplyr::select(Direction) %&gt;% # Pick just the labels
  as_tibble() %&gt;% # Make sure they are a tibble
  mutate(
    log_reg_class = predict(logreg_fit, new_data = df_test) %&gt;% pull(.pred_class), # Extracts the class predictions
    log_reg_prob = predict(logreg_fit, new_data = df_test, type = &quot;prob&quot;) %&gt;% pull(.pred_Up) # Extracts the probiablities 
  )

predict(logreg_fit, new_data = df_test) # See what the predictions look like</code></pre>
<pre><code>## # A tibble: 375 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 Down       
##  2 Down       
##  3 Down       
##  4 Down       
##  5 Down       
##  6 Down       
##  7 Down       
##  8 Down       
##  9 Up         
## 10 Up         
## # … with 365 more rows</code></pre>
<pre class="r"><code>predict(logreg_fit, new_data = df_test, type = &quot;prob&quot;) # See what the probabilites look like</code></pre>
<pre><code>## # A tibble: 375 x 2
##    .pred_Down .pred_Up
##         &lt;dbl&gt;    &lt;dbl&gt;
##  1      0.536    0.464
##  2      0.560    0.440
##  3      0.571    0.429
##  4      0.523    0.477
##  5      0.524    0.476
##  6      0.550    0.450
##  7      0.531    0.469
##  8      0.528    0.472
##  9      0.441    0.559
## 10      0.450    0.550
## # … with 365 more rows</code></pre>
<pre class="r"><code>glimpse(logreg_results)</code></pre>
<pre><code>## Observations: 375
## Variables: 3
## $ Direction     &lt;fct&gt; Up, Up, Up, Down, Up, Down, Up, Down, Down, Down, …
## $ log_reg_class &lt;fct&gt; Down, Down, Down, Down, Down, Down, Down, Down, Up…
## $ log_reg_prob  &lt;dbl&gt; 0.4643367, 0.4402919, 0.4291951, 0.4772689, 0.4756…</code></pre>
</div>
<div id="evaluate-performance" class="section level2">
<h2>Evaluate performance</h2>
<p>Now we would like to see how well we did. For this we can use another package from tidymodels - yardstick.</p>
<pre class="r"><code># AUC 
logreg_results %&gt;% roc_auc(truth = Direction, log_reg_prob)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.507</code></pre>
<pre class="r"><code># Accuracy
logreg_results %&gt;% accuracy(truth = Direction, log_reg_class)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.515</code></pre>
<pre class="r"><code># Truth Table / Confidence Matrix
logreg_results %&gt;% conf_mat(truth = Direction, log_reg_class)</code></pre>
<pre><code>##           Truth
## Prediction Down  Up
##       Down   60  58
##       Up    124 133</code></pre>
<pre class="r"><code># Nicer Confidence Matrix 
logreg_results %&gt;%
  conf_mat(truth = Direction, log_reg_class) %&gt;%
  pluck(1) %&gt;%
  as_tibble() %&gt;%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = &quot;white&quot;, alpha = 1, size = 8) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Evaluation%20Lab4-1.png" width="576" /></p>
<pre class="r"><code># ROC - There is a minor problem with yardstick right now that makes this code 
# require the development version of the package. With next update it should 
# be included

roc_curve(logreg_results, truth = Direction, log_reg_prob) %&gt;%
  autoplot()</code></pre>
<p><img src="STA314F19_files/figure-html/Evaluation%20Lab4-2.png" width="576" /></p>
<pre class="r"><code># or Smoothed:
roc_curve(logreg_results, truth = Direction, log_reg_prob,
          options = list(smooth = TRUE)) %&gt;%
  autoplot()</code></pre>
<p><img src="STA314F19_files/figure-html/Evaluation%20Lab4-3.png" width="576" /></p>
<p>You can notice that our ROC curve dips below the reference line for a moment. That means it is worse than random guessing in that region! Overall our model is very bad. We can run a hedge fund now.</p>
<p>Another way to think about ROC curves is in terms of True positive rate and False positive rate (since those are the same as the axes). For each threshold you can choose you will receive a single point in the space of (TPR, FPR). Plotting those points for all unique values of threshold and connecting results in the ROC curve.</p>
<pre class="r"><code>rm(list = ls())</code></pre>
</div>
</div>
<div id="tutorial-7" class="section level1">
<h1>Tutorial 7</h1>
<div id="writing-functions" class="section level2">
<h2>Writing functions</h2>
<ul>
<li>Arguments (Default arguments, unnamed arguments)</li>
</ul>
<pre class="r"><code>first_function &lt;- function(argument1, argument2) {
  result &lt;- argument1 + argument2
  
  #return this
  result
}

first_function(1, 2)</code></pre>
<pre><code>## [1] 3</code></pre>
<pre class="r"><code>second_function &lt;- function(argument1=1, argument2=1){
  argument1 + argument2
}

second_function(2)</code></pre>
<pre><code>## [1] 3</code></pre>
<pre class="r"><code>third_function &lt;- function(val1, val2){
  val1 / val2
}

third_function(2, 1)</code></pre>
<pre><code>## [1] 2</code></pre>
<pre class="r"><code>third_function(val2 = 2, val1 = 1)</code></pre>
<pre><code>## [1] 0.5</code></pre>
<p>You can also let the call to the function include unnamed arguments that are then (potentially) passed on to other functions</p>
<pre class="r"><code>red_plot &lt;- function(x, y, ...){
  plot(x, y, col = &quot;red&quot;, ...)
}

red_plot(1:10, 1:10, xlab = &quot;X axis name&quot;, ylab = &quot;Y axis name&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/Additional%20Inputs-1.png" width="576" /></p>
<pre class="r"><code># eval=FALSE means don&#39;t evaluate this chunk
# include = TRUE means show it in the file
# echo = FALSE means it will show just the output and not the code but runs the
# code

params &lt;- list(sample = 10, repetitions = 100, lr = 0.05)

model_function &lt;- function(params){
  for (i in c(1:params$repetitions)) {
    do sth
  }
}</code></pre>
<ul>
<li>Returns</li>
</ul>
<pre class="r"><code>f4 &lt;- function(x, y) {
  sum &lt;- x + y
  difference &lt;- x - y
  results &lt;- list(sum = sum, difference = difference)
  
  return(results)
}

a &lt;- f4(1,2)</code></pre>
<p>If I have a file containing the functions I would like to use (In this case it is the function_utils.R file) you can load in the functions using the source() command.</p>
<ul>
<li>Sourcing</li>
</ul>
<pre class="r"><code>source(file = &quot;function_utils.R&quot;)
my_function(args = list(a = 1, b = 2))</code></pre>
<pre><code>## [1] 3</code></pre>
</div>
<div id="for-loops" class="section level2">
<h2>For Loops</h2>
<ul>
<li>Basic For loop</li>
</ul>
<pre class="r"><code>samples &lt;- runif(10)

for (sample in samples) {
  print(sample + 5)
}</code></pre>
<pre><code>## [1] 5.456576
## [1] 5.494118
## [1] 5.609299
## [1] 5.024747
## [1] 5.815657
## [1] 5.682413
## [1] 5.034818
## [1] 5.134886
## [1] 5.100857
## [1] 5.532696</code></pre>
<ul>
<li>Don’t override vectors with vectors: vec &lt;- c(vec, num)</li>
</ul>
<p>Say we would like to look at the variance of the expected mean of some distribution as we take random samples of varying size.</p>
<pre class="r"><code>samp_means &lt;- c()

reps &lt;- 1000
size &lt;- 1000

# Don&#39;t do this
for (i in 1:reps) {
  samp_means &lt;- c(samp_means, mean(rnorm(n = size)))
}

var(samp_means)</code></pre>
<pre><code>## [1] 0.0009922266</code></pre>
<p>This results in us having to allocate new memory to the vector samp_means at every iteration which results in incredibly slow code. Instead if we know the length of the output we are expecting to get, we begin by creating an empty vector of that length and overwriting the values within it. This makes sure that we don’t have to move the vector in memory and doesn’t slow us down at all.</p>
<pre class="r"><code># Do this instead
reps &lt;- 1000
size &lt;- 1000

samples &lt;- rep(NA, reps)

for (i in c(1:reps)) {
  samp_means[i] &lt;- mean(rnorm(n = size))
}

var(samp_means)</code></pre>
<pre><code>## [1] 0.0009273014</code></pre>
<ul>
<li>for (i in 1:length(a)) allows us to interact with elements of a in an iterative way while keeping just an index instead of extracting the inside of the vector.</li>
</ul>
<pre class="r"><code>a &lt;- runif(100)

# Change all but the last 2 elements to something 
for (i in (1:(length(a) - 2))) {
  a[i] &lt;- NA
}

a</code></pre>
<pre><code>##   [1]        NA        NA        NA        NA        NA        NA        NA
##   [8]        NA        NA        NA        NA        NA        NA        NA
##  [15]        NA        NA        NA        NA        NA        NA        NA
##  [22]        NA        NA        NA        NA        NA        NA        NA
##  [29]        NA        NA        NA        NA        NA        NA        NA
##  [36]        NA        NA        NA        NA        NA        NA        NA
##  [43]        NA        NA        NA        NA        NA        NA        NA
##  [50]        NA        NA        NA        NA        NA        NA        NA
##  [57]        NA        NA        NA        NA        NA        NA        NA
##  [64]        NA        NA        NA        NA        NA        NA        NA
##  [71]        NA        NA        NA        NA        NA        NA        NA
##  [78]        NA        NA        NA        NA        NA        NA        NA
##  [85]        NA        NA        NA        NA        NA        NA        NA
##  [92]        NA        NA        NA        NA        NA        NA        NA
##  [99] 0.8062991 0.8434484</code></pre>
<ul>
<li>for (i in vec) allows us to iterate over things inside of a vector.</li>
</ul>
<pre class="r"><code>vec &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)

for (string in vec) {
  print(string)
}</code></pre>
<pre><code>## [1] &quot;a&quot;
## [1] &quot;b&quot;
## [1] &quot;c&quot;</code></pre>
<ul>
<li>for (i in list) allows us to iterate over lists (named or not) and therefore interact with multiple types in the same loop. This requires very rigorous type checking to work properly.</li>
</ul>
<pre class="r"><code>params &lt;- list(a = &quot;a&quot;, lr = 0.05)

for (param in params) {
  print(param)
}</code></pre>
<pre><code>## [1] &quot;a&quot;
## [1] 0.05</code></pre>
<ul>
<li>over matrices</li>
</ul>
<pre class="r"><code>matrix &lt;- matrix(1:24, nrow = 3)

for (i in 1:nrow(matrix)) {
  for (j in 1:ncol(matrix)) {
    print(matrix[i,j])
  }
}</code></pre>
<pre><code>## [1] 1
## [1] 4
## [1] 7
## [1] 10
## [1] 13
## [1] 16
## [1] 19
## [1] 22
## [1] 2
## [1] 5
## [1] 8
## [1] 11
## [1] 14
## [1] 17
## [1] 20
## [1] 23
## [1] 3
## [1] 6
## [1] 9
## [1] 12
## [1] 15
## [1] 18
## [1] 21
## [1] 24</code></pre>
<ul>
<li>over columns in tidyverse (don’t). Columns in tidyverse can contain pretty much any other object (including data tables) as observations. Iterating over the columns by themselves is not a good idea unless you really know what to expect.</li>
</ul>
<pre class="r"><code>df &lt;- as_tibble(MASS::Boston)

df %&gt;% pull(age) -&gt; vector


# Prints about the first 1/25 of the observations in Boston$age

for (i in (1:floor(length(vector)/25))) {
  print(vector[i])
}</code></pre>
<pre><code>## [1] 65.2
## [1] 78.9
## [1] 61.1
## [1] 45.8
## [1] 54.2
## [1] 58.7
## [1] 66.6
## [1] 96.1
## [1] 100
## [1] 85.9
## [1] 94.3
## [1] 82.9
## [1] 39
## [1] 61.8
## [1] 84.5
## [1] 56.5
## [1] 29.3
## [1] 81.7
## [1] 36.6
## [1] 69.5</code></pre>
<pre class="r"><code>rm(list = ls())</code></pre>
</div>
</div>
<div id="tutorial-8" class="section level1">
<h1>Tutorial 8</h1>
<p><em>Pick up your midterms at the start of tutorial!</em></p>
<p>We are covering the 6.5 Lab 1 - Subset Selection Methods from ISLR.</p>
<pre class="r"><code>df_orig &lt;- ISLR::Hitters # original data 

df &lt;- df_orig #Working copy</code></pre>
<p>First, we would like to figure out if there are any missing observations:</p>
<pre class="r"><code>df %&gt;% summarize_all(funs(sum(is.na(.)))) %&gt;% t()</code></pre>
<pre><code>## Warning: funs() is soft deprecated as of dplyr 0.8.0
## Please use a list of either functions or lambdas: 
## 
##   # Simple named list: 
##   list(mean = mean, median = median)
## 
##   # Auto named with `tibble::lst()`: 
##   tibble::lst(mean, median)
## 
##   # Using lambdas
##   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))
## This warning is displayed once per session.</code></pre>
<pre><code>##           [,1]
## AtBat        0
## Hits         0
## HmRun        0
## Runs         0
## RBI          0
## Walks        0
## Years        0
## CAtBat       0
## CHits        0
## CHmRun       0
## CRuns        0
## CRBI         0
## CWalks       0
## League       0
## Division     0
## PutOuts      0
## Assists      0
## Errors       0
## Salary      59
## NewLeague    0</code></pre>
<p>We can see that there are 59 observations that are missing the Salary value and nothing else. This is quite a large proportion at 0.1832298 which is just a bit less than <span class="math inline">\(\frac{1}{5}\)</span>. We could try to fill those observations in (this is called imputation and has a ton of different options), but for now we can just filter them out. Here we will be using salary as the target so we cannot fill it in beforehand.</p>
<pre class="r"><code>df &lt;- df %&gt;% filter_all(all_vars(!is.na(.)))</code></pre>
<p>We can use the library leaps to perform the best subset selection using lowest RSS. Let’s use it to see which model performs “best”. By default it includes up to 8 variables. The summary lists models in order of increasing number of variables only including the best model for each given number.</p>
<pre class="r"><code>regfit_full = regsubsets(Salary~., data = df)
summary(regfit_full)</code></pre>
<pre><code>## Subset selection object
## Call: regsubsets.formula(Salary ~ ., data = df)
## 19 Variables  (and intercept)
##            Forced in Forced out
## AtBat          FALSE      FALSE
## Hits           FALSE      FALSE
## HmRun          FALSE      FALSE
## Runs           FALSE      FALSE
## RBI            FALSE      FALSE
## Walks          FALSE      FALSE
## Years          FALSE      FALSE
## CAtBat         FALSE      FALSE
## CHits          FALSE      FALSE
## CHmRun         FALSE      FALSE
## CRuns          FALSE      FALSE
## CRBI           FALSE      FALSE
## CWalks         FALSE      FALSE
## LeagueN        FALSE      FALSE
## DivisionW      FALSE      FALSE
## PutOuts        FALSE      FALSE
## Assists        FALSE      FALSE
## Errors         FALSE      FALSE
## NewLeagueN     FALSE      FALSE
## 1 subsets of each size up to 8
## Selection Algorithm: exhaustive
##          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns
## 1  ( 1 ) &quot; &quot;   &quot; &quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 2  ( 1 ) &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 3  ( 1 ) &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 4  ( 1 ) &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 5  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 6  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 7  ( 1 ) &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot; &quot;  
## 8  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot;    &quot;*&quot;  
##          CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
## 1  ( 1 ) &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 2  ( 1 ) &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 3  ( 1 ) &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 4  ( 1 ) &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 5  ( 1 ) &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 6  ( 1 ) &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 7  ( 1 ) &quot; &quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 8  ( 1 ) &quot; &quot;  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;</code></pre>
<p>So we can see that the best one variable model uses CRBI and two variable uses CRBI and Hits.</p>
<p>Now let’s use more than 8 variables.</p>
<pre class="r"><code>regfit_full &lt;- leaps::regsubsets(Salary~., data = df, nvmax = 19)
summary(regfit_full)</code></pre>
<pre><code>## Subset selection object
## Call: regsubsets.formula(Salary ~ ., data = df, nvmax = 19)
## 19 Variables  (and intercept)
##            Forced in Forced out
## AtBat          FALSE      FALSE
## Hits           FALSE      FALSE
## HmRun          FALSE      FALSE
## Runs           FALSE      FALSE
## RBI            FALSE      FALSE
## Walks          FALSE      FALSE
## Years          FALSE      FALSE
## CAtBat         FALSE      FALSE
## CHits          FALSE      FALSE
## CHmRun         FALSE      FALSE
## CRuns          FALSE      FALSE
## CRBI           FALSE      FALSE
## CWalks         FALSE      FALSE
## LeagueN        FALSE      FALSE
## DivisionW      FALSE      FALSE
## PutOuts        FALSE      FALSE
## Assists        FALSE      FALSE
## Errors         FALSE      FALSE
## NewLeagueN     FALSE      FALSE
## 1 subsets of each size up to 19
## Selection Algorithm: exhaustive
##           AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns
## 1  ( 1 )  &quot; &quot;   &quot; &quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 2  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 3  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 4  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 5  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 6  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 7  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot; &quot;  
## 8  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot;    &quot;*&quot;  
## 9  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 10  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 11  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 12  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 13  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 14  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 15  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;  
## 16  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;  
## 17  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;  
## 18  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;  
## 19  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot;*&quot;  
##           CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
## 1  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 2  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 3  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 4  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 5  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 6  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 7  ( 1 )  &quot; &quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 8  ( 1 )  &quot; &quot;  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 9  ( 1 )  &quot;*&quot;  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 10  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 11  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 12  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 13  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 14  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 15  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 16  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 17  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 18  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 19  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;</code></pre>
<p>The summary actually contains more stuff that is not displayed so let’s explore that:</p>
<pre class="r"><code>regfit_summary &lt;- summary(regfit_full)
names(regfit_summary)</code></pre>
<pre><code>## [1] &quot;which&quot;  &quot;rsq&quot;    &quot;rss&quot;    &quot;adjr2&quot;  &quot;cp&quot;     &quot;bic&quot;    &quot;outmat&quot; &quot;obj&quot;</code></pre>
<pre class="r"><code># We can look at the RSS of the models as we increase the number of variables:
regfit_summary$rss</code></pre>
<pre><code>##  [1] 36179679 30646560 29249297 27970852 27149899 26194904 25906548
##  [8] 25136930 24814051 24500402 24387345 24333232 24289148 24248660
## [15] 24235177 24219377 24209447 24201837 24200700</code></pre>
<pre class="r"><code># Even plot it (on a log scale)
qplot(x = 1:19, y = log(regfit_summary$rss))</code></pre>
<p><img src="STA314F19_files/figure-html/summary%20of%20bss%20part%201-1.png" width="576" /></p>
<pre class="r"><code># Or R squared
qplot(x = 1:19, y = regfit_summary$adjr2)</code></pre>
<p><img src="STA314F19_files/figure-html/summary%20of%20bss%20part%201-2.png" width="576" /></p>
<p>We can see that adding more predictors beyond 10 gives very little lift to our model. We can also see that the highest we can get is around 0.55 <span class="math inline">\(R^2\)</span> and around 0.5225706 for adjusted <span class="math inline">\(R^2\)</span></p>
<p>The regsubsets function has a built in plot capability that we will now explore:</p>
<pre class="r"><code>plot(regfit_full, scale = &quot;adjr2&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/plotting%20bss-1.png" width="576" /></p>
<p>We can also access the coefficients of the model using the coef() function. Let’s look at the 5 variable model.</p>
<pre class="r"><code>coef(regfit_full, 5)</code></pre>
<pre><code>##  (Intercept)        AtBat         Hits         CRBI    DivisionW 
##   97.7684116   -1.4401428    7.1753197    0.6882079 -129.7319386 
##      PutOuts 
##    0.2905164</code></pre>
<p>How does it choose the best subsets of variables?</p>
<pre class="r"><code>?regsubsets</code></pre>
<p>Both Forward and Backwards selections are greedy algorithms.</p>
<p>How to do optim:</p>
<pre class="r"><code># Define some initial parameters. Don&#39;t name them just &quot;beta&quot; it will lead 
# to errors. Parameters need to be a VECTOR

beta_1 &lt;- 0
beta_2 &lt;- 2

par &lt;- c(beta_1, beta_2)

# Define our function in terms of parameters
xsquared &lt;- function(par) {
    result &lt;- sum(-(as.double(par[1]^2) - as.double(c(1:10)^2)))
  return(abs(result))
}


xsquared(par) # initial value</code></pre>
<pre><code>## [1] 385</code></pre>
<pre class="r"><code># Simple plots ( we can&#39;t do this for more than 1 variable easily!)
inputs = seq(from = 0, to = 10, by = 0.1)
outputs = sapply(inputs, xsquared)
qplot(inputs, outputs)</code></pre>
<p><img src="STA314F19_files/figure-html/Optim%20function-1.png" width="576" /></p>
<pre class="r"><code># Optimize
optim(par, xsquared, hessian = TRUE)</code></pre>
<pre><code>## $par
## [1]  6.2048368 -0.3770086
## 
## $value
## [1] 2.763466e-06
## 
## $counts
## function gradient 
##       85       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
## 
## $hessian
##          [,1] [,2]
## [1,] 124095.4    0
## [2,]      0.0    0</code></pre>
<p>Even though the second parameter is not used we make changes to it due to the method we are employing for optmization.</p>
<p>Below is an implementation of k-fold cross validation which I covered on the board in tutorial. I use the same KNN algorithm and dataset as in Tutorial 5.</p>
<pre class="r"><code># Load data

data_orig &lt;- ISLR::Smarket
df &lt;- data_orig

df_train &lt;- df %&gt;% sample_n(size = 0.7*1250)
df_test &lt;- setdiff(df, df_train)

# Prep for KNN
scale2 &lt;- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)

df_train_std &lt;- df_train %&gt;%
  dplyr::select(-Direction ,-Today) %&gt;% # Remove the variables we are not considering and response
  mutate_all(.funs = scale2) # I apply the scale2() function to all other variables.
df_test_std &lt;- df_test %&gt;%
  dplyr::select(-Direction ,-Today) %&gt;% # Remove the variables we are not considering and response
  mutate_all(.funs = scale2) # I apply the scale2() function to all other variables.

df_train_std$Direction &lt;- df_train$Direction
df_test_std$Direction &lt;- df_test$Direction

# Actual values
actual &lt;- df_test$Direction

# Fit the inital KNN

predictions &lt;- knn(train = df_train_std %&gt;% dplyr::select(-Direction), # Training Set
                   test = df_test_std %&gt;% dplyr::select(-Direction), # Test Set
                   cl = df_train$Direction, # Labels for Training
                   k = 3) # 3 nearest Neighbours

table(predictions, actual)</code></pre>
<pre><code>##            actual
## predictions Down  Up
##        Down   85  78
##        Up    103 109</code></pre>
<pre class="r"><code># Do k-fold CV on KNN. Note that the k in k-fold is not the same as the k in KNN.
# I will call it m-fold CV just to avoid confusion.

m &lt;- 5 # use 5 folds.

# We can create the training folds by adding a new variable to our training data

df_train_std$fold &lt;- sample(1:5, size = nrow(df_train_std), replace = TRUE)

# Create a matrix to store our errors for different values of k we want to try.

possible_k &lt;- 1:10

cv_errors = matrix(NA, m, length(possible_k),
                   dimnames = list(NULL, paste(1:length(possible_k))))

# loop over folds and report 

for (cv_fold in 1:m) {
  for (k in possible_k) {
    
    # make the train fold
    train_fold &lt;- df_train_std %&gt;% filter(fold != cv_fold)
    
    # make the test fold
    test_fold &lt;- df_train_std %&gt;% filter(fold == cv_fold)
    
    # make the labels for each
    train_labs &lt;- train_fold %&gt;% pull(Direction)
    test_labs &lt;- test_fold %&gt;% pull(Direction)
    
    # Update the train and test folds to exclude direction
    train_fold &lt;- train_fold %&gt;% dplyr::select(-Direction)
    test_fold &lt;- test_fold  %&gt;% dplyr::select(-Direction)

    # fit the model
    knn_preds &lt;- knn(train = train_fold,
                     test = test_fold,
                     cl = train_labs,
                     k = k,
                     use.all = TRUE)
    # Evaluate the result
    table_result &lt;- table(knn_preds, test_labs)
    table_result
    accuracy &lt;- sum(diag(table_result)) / sum(table_result)
    # Store result
    cv_errors[cv_fold, k] &lt;- accuracy
  }
}
cv_errors</code></pre>
<pre><code>##              1         2         3         4         5         6         7
## [1,] 0.5864198 0.5740741 0.5864198 0.5555556 0.5617284 0.5617284 0.5432099
## [2,] 0.4638554 0.5120482 0.5783133 0.5602410 0.5903614 0.5662651 0.5783133
## [3,] 0.4624277 0.5086705 0.5086705 0.5086705 0.5606936 0.5317919 0.5144509
## [4,] 0.4851485 0.4900990 0.4405941 0.4306931 0.4603960 0.4455446 0.4257426
## [5,] 0.5290698 0.4825581 0.4941860 0.5465116 0.4767442 0.4941860 0.4476744
##              8         9        10
## [1,] 0.5246914 0.5802469 0.5617284
## [2,] 0.5481928 0.6024096 0.5963855
## [3,] 0.5722543 0.5202312 0.4971098
## [4,] 0.4356436 0.4158416 0.4405941
## [5,] 0.4534884 0.4941860 0.5000000</code></pre>
<pre class="r"><code># Average Errors across different k&#39;s for KNN:

average_accuracy &lt;- apply(cv_errors, 2, mean)
average_accuracy</code></pre>
<pre><code>##         1         2         3         4         5         6         7 
## 0.5053842 0.5134900 0.5216367 0.5203343 0.5299847 0.5199032 0.5018782 
##         8         9        10 
## 0.5068541 0.5225831 0.5191636</code></pre>
<p>So the optimal choice is 5 according to 5-fold CV. Please note that this code is very much hacked together and not at all efficient.</p>
<p>For those of you who have not picked up their midterms: they were returned to the professor, you can contact her to arrange for a pick-up during office hours.</p>
</div>
<div id="tutorial-9" class="section level1">
<h1>Tutorial 9</h1>
<p>We will go through the ISLR lab for regularization which I have already covered a bit a few tutorials ago. We will be using the glmnet package.</p>
<pre class="r"><code>df_orig &lt;- Hitters
df &lt;- df_orig</code></pre>
<pre class="r"><code>x = model.matrix(Salary~.,Hitters)[,-1]
y &lt;- Hitters$Salary[!is.na(Hitters$Salary)]</code></pre>
<pre class="r"><code># Lasso 
model_lasso &lt;- glmnet(x, y, family = &quot;gaussian&quot;, alpha = 1)


# Ridge
grid &lt;- 10^seq(from = 10, to = -2,length = 100)

model_ridge &lt;- glmnet(x, y, family = &quot;gaussian&quot;, alpha = 0, lambda = grid)
coef(model_ridge)[,100]</code></pre>
<pre><code>##   (Intercept)         AtBat          Hits         HmRun          Runs 
##  164.11321606   -1.97386151    7.37772270    3.93660219   -2.19873625 
##           RBI         Walks         Years        CAtBat         CHits 
##   -0.91623008    6.20037718   -3.71403424   -0.17510063    0.21132772 
##        CHmRun         CRuns          CRBI        CWalks       LeagueN 
##    0.05629004    1.36605490    0.70965516   -0.79582173   63.40493257 
##     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
## -117.08243713    0.28202541    0.37318482   -3.42400281  -25.99081928</code></pre>
</div>
<div id="tutorial-10" class="section level1">
<h1>Tutorial 10</h1>
<p>A very good resource that uses a different package to fit splines is the lecture notes from James H Steiger from Vanderblit university which can be found <a href="http://www.statpower.net/Content/313/Lecture%20Notes/Splines.pdf">here</a></p>
<div id="polynomial-regression" class="section level2">
<h2>Polynomial Regression</h2>
<pre class="r"><code>rm(list = ls())
# The usual
df_orig &lt;- ISLR::Wage
df &lt;- df_orig</code></pre>
<p>Let’s fit a polynomial regression. The raw = TRUE parameter causes it to behave exactly like the stacked I() statements in the second model. Removing that results in it fitting orthogonal polynomials that include all lower levels as we discussed in tutorial.</p>
<pre class="r"><code>lm_1 &lt;- lm(wage~poly(age, degree = 4, raw = TRUE), data = df)
summary(lm_1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wage ~ poly(age, degree = 4, raw = TRUE), data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -98.707 -24.626  -4.993  15.217 203.693 
## 
## Coefficients:
##                                      Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                        -1.842e+02  6.004e+01  -3.067 0.002180
## poly(age, degree = 4, raw = TRUE)1  2.125e+01  5.887e+00   3.609 0.000312
## poly(age, degree = 4, raw = TRUE)2 -5.639e-01  2.061e-01  -2.736 0.006261
## poly(age, degree = 4, raw = TRUE)3  6.811e-03  3.066e-03   2.221 0.026398
## poly(age, degree = 4, raw = TRUE)4 -3.204e-05  1.641e-05  -1.952 0.051039
##                                       
## (Intercept)                        ** 
## poly(age, degree = 4, raw = TRUE)1 ***
## poly(age, degree = 4, raw = TRUE)2 ** 
## poly(age, degree = 4, raw = TRUE)3 *  
## poly(age, degree = 4, raw = TRUE)4 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 39.91 on 2995 degrees of freedom
## Multiple R-squared:  0.08626,    Adjusted R-squared:  0.08504 
## F-statistic: 70.69 on 4 and 2995 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>lm_2 &lt;- lm(wage~age + I(age^2) + I(age^3) +I(age^4), data = df)
summary(lm_2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wage ~ age + I(age^2) + I(age^3) + I(age^4), data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -98.707 -24.626  -4.993  15.217 203.693 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.842e+02  6.004e+01  -3.067 0.002180 ** 
## age          2.125e+01  5.887e+00   3.609 0.000312 ***
## I(age^2)    -5.639e-01  2.061e-01  -2.736 0.006261 ** 
## I(age^3)     6.811e-03  3.066e-03   2.221 0.026398 *  
## I(age^4)    -3.204e-05  1.641e-05  -1.952 0.051039 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 39.91 on 2995 degrees of freedom
## Multiple R-squared:  0.08626,    Adjusted R-squared:  0.08504 
## F-statistic: 70.69 on 4 and 2995 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can use anova to compare nested models. The resulting p-value is for a hypothesis test of the two models being the same / different. It is very useful for comparing nested models with large differences in number of parameters. If you want to learn more take STA303 next term.</p>
<pre class="r"><code>lm_3 &lt;- lm(wage ~ age + I(age^2), data = df)

anova(lm_3, lm_2)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: wage ~ age + I(age^2)
## Model 2: wage ~ age + I(age^2) + I(age^3) + I(age^4)
##   Res.Df     RSS Df Sum of Sq      F   Pr(&gt;F)   
## 1   2997 4793430                                
## 2   2995 4771604  2     21826 6.8497 0.001076 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can see how our polynomial has fit using the base R code from ISLR</p>
<pre class="r"><code>agelims = range(df$age)
age.grid = seq(from = agelims[1], to = agelims[2])
preds = predict(lm_2, newdata = list(age = age.grid), se = TRUE)
se.bands = cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)

par(
  mar = c(4.5, 4.5, 1, 1) ,
  oma = c(0, 0, 4, 0)
)
plot(df$age,
     df$wage,
     xlim = agelims ,
     cex = .5,
     col = &quot;darkgrey&quot;)
title(&quot;Degree -4Polynomial&quot;, outer = T)
lines(age.grid, preds$fit, lwd = 2, col = &quot;blue&quot;)
matlines(age.grid,
         se.bands,
         lwd = 1,
         col = &quot;blue&quot;,
         lty = 3)</code></pre>
<p><img src="STA314F19_files/figure-html/Plot%204%20degree%20polynomial-1.png" width="576" /></p>
<p>or just by using ggplot with a lot less hassle (note that this is NOT using any of the objects created before):</p>
<pre class="r"><code>df %&gt;%
  ggplot(aes(x = age, y = wage)) +
  geom_point(color = &quot;gray&quot;, alpha = 0.5) +
  geom_smooth(method = &quot;lm&quot;, formula = y~poly(x, 4)) +
  theme_minimal() +
  labs(title = &quot;Degree 4 Polynomial&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/GGplot%204%20degree%20polynomial-1.png" width="576" /></p>
<p>Now onto splines.</p>
</div>
<div id="splines" class="section level2">
<h2>Splines</h2>
<pre class="r"><code># we can fit a default spline
lm_spline &lt;- lm(wage ~ splines::bs(age), data = df)
summary(lm_spline)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wage ~ splines::bs(age), data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -99.693 -24.562  -5.222  15.096 206.119 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         58.689      4.013  14.625  &lt; 2e-16 ***
## splines::bs(age)1  102.644     11.449   8.965  &lt; 2e-16 ***
## splines::bs(age)2   48.762      8.625   5.654 1.72e-08 ***
## splines::bs(age)3   40.803     12.109   3.370 0.000762 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 39.93 on 2996 degrees of freedom
## Multiple R-squared:  0.0851, Adjusted R-squared:  0.08419 
## F-statistic: 92.89 on 3 and 2996 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># We can add our own knots and make sure it adds the intercept.
splines &lt;- bs(df$age, knots = c(37, 48 , 55), intercept = TRUE)
attr(splines, &quot;knots&quot;)</code></pre>
<pre><code>## [1] 37 48 55</code></pre>
<p>Since we already have made a spline dataset with an intercept, we should remove the intercept from our lm.</p>
<pre class="r"><code>lm_spline_manual &lt;- lm(wage ~ splines - 1, data = df)
summary(lm_spline_manual)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wage ~ splines - 1, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -98.819 -24.374  -5.171  15.638 202.348 
## 
## Coefficients:
##          Estimate Std. Error t value Pr(&gt;|t|)    
## splines1   58.784      6.527   9.006  &lt; 2e-16 ***
## splines2   85.291      5.936  14.370  &lt; 2e-16 ***
## splines3  124.415      4.881  25.492  &lt; 2e-16 ***
## splines4  116.918      2.983  39.192  &lt; 2e-16 ***
## splines5  121.133      6.050  20.021  &lt; 2e-16 ***
## splines6  111.423     13.179   8.455  &lt; 2e-16 ***
## splines7   77.806     16.091   4.835  1.4e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 39.91 on 2993 degrees of freedom
## Multiple R-squared:  0.8883, Adjusted R-squared:  0.888 
## F-statistic:  3399 on 7 and 2993 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Let’s see what the splines look like:</p>
<pre class="r"><code>df %&gt;%
  ggplot(aes(x = age, y = wage)) +
  geom_point(color = &quot;gray&quot;, alpha = 0.5) +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ bs(x, knots = c(37,48, 55))) +
  geom_vline(xintercept = c(37, 48, 55)) +
  theme_minimal() +
  labs(title = &quot;Regression with splines&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/Visualization%20for%20splines-1.png" width="576" /></p>
<p>Another good resource on understanding what the fitted splines equations look like is <a href="http://people.stat.sfu.ca/~cschwarz/Consulting/Trinity/Phase2/TrinityWorkshop/Workshop-handouts/TW-04-Intro-splines.pdf">this slide deck</a> from Carl Schwarz from SFU.</p>
</div>
</div>
<div id="additional-things" class="section level1">
<h1>Additional Things</h1>
<div id="pca" class="section level2">
<h2>PCA</h2>
<p><strong>With thanks to Alex Stringer for teaching me this properly in STA414</strong></p>
<div id="motivation" class="section level3">
<h3>Motivation:</h3>
<p>Sometimes we are given a dataset containing a much larger number of features than what we like for analysis. (We will see this specifically in lab 3). <em>Principal Component Analysis</em> is one way of reducing the number of features while maintaining as much information about the original data as possible.</p>
</div>
<div id="procedure" class="section level3">
<h3>Procedure:</h3>
<p>Given a <span class="math inline">\(n\)</span> datapoints with <span class="math inline">\(p\)</span> features each, PCA tries to find a low-dimensional <span class="math inline">\(d &lt; p\)</span> factorization of the data matrix <span class="math inline">\(X\)</span> that preserves the maximum possible variance.</p>
<p><span class="math display">\[ X = UZ \]</span> <span class="math display">\[ X \in \mathbb{R}^{n \times p} \]</span> <span class="math display">\[ Z \in \mathbb{R}^{d \times p} \]</span> <span class="math display">\[ U \in \mathbb{R}^{n \times d} \]</span></p>
<p>We estimate <span class="math inline">\(U\)</span> from the data, and call the associated <span class="math inline">\(Z\)</span> the principal components of <span class="math inline">\(X\)</span>.</p>
<p>PCA is thus states as follows:</p>
<center>
<p>for <span class="math inline">\(j = 1, ..., d\)</span></p>
</center>
<p><span class="math display">\[ \mathbf{u}_j = argmax(Var(\mathbf{u}^T\mathbf{x})) = argmax(\mathbf{u}^T\mathbf{Su}) \]</span></p>
<center>
<p>subject to:</p>
</center>
<p><span class="math display">\[ \mathbf{u}^T\mathbf{u} = 1 ,~~ \text{and } ~ \mathbf{u} \perp \mathbf{u}_k ~~ \text{for} ~~ k &lt; j \]</span></p>
<p>Where S is the sample covariance matrix: <span class="math display">\[ \mathbf{S} = \frac{1}{n}\sum_{i = 1}^{n}{(\mathbf{x_i} - \mathbf{\bar{x}})(\mathbf{x_i} - \mathbf{\bar{x}}) ^T} \]</span></p>
<p>Using lagrange multipliers we see the solution to the above problem must satisfy:</p>
<p><span class="math display">\[ \mathbf{S}\mathbf{u}_1 = \lambda \mathbf{u}_1 \]</span> Which means that <span class="math inline">\(\mathbf{u}_1\)</span> is an eigenvector of S with the eigenvalue <span class="math inline">\(\lambda\)</span>.</p>
<p>By definition of the problem <span class="math inline">\(\lambda\)</span> must be the largest eigenvalue. This is since it doesn’t the second constraint (as there is no previously selected vectors)</p>
<p>Solving this constrained optimization problem gives us an orthonormal basis where the basis vectors point in the directions of the principal axes of the sample covariance matrix, in decreasing order of length.</p>
<p>It’s equivalent to the rotation in the original input space!</p>
<p>We then proceed to “chop off” the <span class="math inline">\(d-p\)</span> dimensions with least variance. And call this the basis for our <span class="math inline">\(d\)</span> dimensional space.</p>
<p>So, the solution to the PCA problem is:</p>
<ol style="list-style-type: decimal">
<li><p>Choose <span class="math inline">\(\mathbf{u}_j\)</span> to be normalized eigenvector of <span class="math inline">\(\mathbf{S}\)</span> corresponding to the <span class="math inline">\(j\)</span>-th highest eigenvalue.</p></li>
<li><p>Choose <span class="math inline">\(\mathbf{U}\)</span> to be the matrix of orthonormal eigenvectors of S, so that <span class="math inline">\(\mathbf{U}^T\mathbf{U} = \mathbf{I}\)</span></p></li>
<li><p>Then <span class="math inline">\(\mathbf{Z} = \mathbf{XU}^T\)</span>.</p></li>
<li><p>Keep only the first d columns of <span class="math inline">\(\mathbf{Z}\)</span> and the corresponding <span class="math inline">\(d \times d\)</span> submatrix of <span class="math inline">\(\mathbf{U}\)</span></p></li>
<li><p>Reconstruct the data as <span class="math inline">\(\mathbf{X}^* = \mathbf{Z}^*\mathbf{U}^*\)</span></p></li>
</ol>
</div>
</div>
<div id="lab-1-pca" class="section level2">
<h2>Lab 1: PCA</h2>
<pre class="r"><code># See variable names
colnames(USArrests)</code></pre>
<pre><code>## [1] &quot;Murder&quot;   &quot;Assault&quot;  &quot;UrbanPop&quot; &quot;Rape&quot;</code></pre>
<pre class="r"><code># Display means of each row
USArrests %&gt;% summarize_all(funs(mean))</code></pre>
<pre><code>##   Murder Assault UrbanPop   Rape
## 1  7.788  170.76    65.54 21.232</code></pre>
<pre class="r"><code># Display means and variances of each row
USArrests %&gt;% summarize_all(.funs = c(Mean = mean, Variance = var))</code></pre>
<pre><code>##   Murder_Mean Assault_Mean UrbanPop_Mean Rape_Mean Murder_Variance
## 1       7.788       170.76         65.54    21.232        18.97047
##   Assault_Variance UrbanPop_Variance Rape_Variance
## 1         6945.166          209.5188      87.72916</code></pre>
<p>There is a built-in function in R to do the principal component analysis:</p>
<p>It automatically scales the data to have the mean of 0</p>
<p>There is an added parameter <em>scale</em> which will also scale the standard deviation to 1.</p>
<p>In general there are at most <span class="math inline">\(min(n-1,p)\)</span> informative principal components.</p>
<pre class="r"><code># compute the PCA
pca_out = prcomp(USArrests , scale = TRUE)
# output the rotation matrix
pca_out$rotation</code></pre>
<pre><code>##                 PC1        PC2        PC3         PC4
## Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
## Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
## UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
## Rape     -0.5434321 -0.1673186  0.8177779  0.08902432</code></pre>
<pre class="r"><code># output the reconstructed data in the new coordinates:
pca_out$x</code></pre>
<pre><code>##                        PC1         PC2         PC3          PC4
## Alabama        -0.97566045  1.12200121 -0.43980366  0.154696581
## Alaska         -1.93053788  1.06242692  2.01950027 -0.434175454
## Arizona        -1.74544285 -0.73845954  0.05423025 -0.826264240
## Arkansas        0.13999894  1.10854226  0.11342217 -0.180973554
## California     -2.49861285 -1.52742672  0.59254100 -0.338559240
## Colorado       -1.49934074 -0.97762966  1.08400162  0.001450164
## Connecticut     1.34499236 -1.07798362 -0.63679250 -0.117278736
## Delaware       -0.04722981 -0.32208890 -0.71141032 -0.873113315
## Florida        -2.98275967  0.03883425 -0.57103206 -0.095317042
## Georgia        -1.62280742  1.26608838 -0.33901818  1.065974459
## Hawaii          0.90348448 -1.55467609  0.05027151  0.893733198
## Idaho           1.62331903  0.20885253  0.25719021 -0.494087852
## Illinois       -1.36505197 -0.67498834 -0.67068647 -0.120794916
## Indiana         0.50038122 -0.15003926  0.22576277  0.420397595
## Iowa            2.23099579 -0.10300828  0.16291036  0.017379470
## Kansas          0.78887206 -0.26744941  0.02529648  0.204421034
## Kentucky        0.74331256  0.94880748 -0.02808429  0.663817237
## Louisiana      -1.54909076  0.86230011 -0.77560598  0.450157791
## Maine           2.37274014  0.37260865 -0.06502225 -0.327138529
## Maryland       -1.74564663  0.42335704 -0.15566968 -0.553450589
## Massachusetts   0.48128007 -1.45967706 -0.60337172 -0.177793902
## Michigan       -2.08725025 -0.15383500  0.38100046  0.101343128
## Minnesota       1.67566951 -0.62590670  0.15153200  0.066640316
## Mississippi    -0.98647919  2.36973712 -0.73336290  0.213342049
## Missouri       -0.68978426 -0.26070794  0.37365033  0.223554811
## Montana         1.17353751  0.53147851  0.24440796  0.122498555
## Nebraska        1.25291625 -0.19200440  0.17380930  0.015733156
## Nevada         -2.84550542 -0.76780502  1.15168793  0.311354436
## New Hampshire   2.35995585 -0.01790055  0.03648498 -0.032804291
## New Jersey     -0.17974128 -1.43493745 -0.75677041  0.240936580
## New Mexico     -1.96012351  0.14141308  0.18184598 -0.336121113
## New York       -1.66566662 -0.81491072 -0.63661186 -0.013348844
## North Carolina -1.11208808  2.20561081 -0.85489245 -0.944789648
## North Dakota    2.96215223  0.59309738  0.29824930 -0.251434626
## Ohio            0.22369436 -0.73477837 -0.03082616  0.469152817
## Oklahoma        0.30864928 -0.28496113 -0.01515592  0.010228476
## Oregon         -0.05852787 -0.53596999  0.93038718 -0.235390872
## Pennsylvania    0.87948680 -0.56536050 -0.39660218  0.355452378
## Rhode Island    0.85509072 -1.47698328 -1.35617705 -0.607402746
## South Carolina -1.30744986  1.91397297 -0.29751723 -0.130145378
## South Dakota    1.96779669  0.81506822  0.38538073 -0.108470512
## Tennessee      -0.98969377  0.85160534  0.18619262  0.646302674
## Texas          -1.34151838 -0.40833518 -0.48712332  0.636731051
## Utah            0.54503180 -1.45671524  0.29077592 -0.081486749
## Vermont         2.77325613  1.38819435  0.83280797 -0.143433697
## Virginia        0.09536670  0.19772785  0.01159482  0.209246429
## Washington      0.21472339 -0.96037394  0.61859067 -0.218628161
## West Virginia   2.08739306  1.41052627  0.10372163  0.130583080
## Wisconsin       2.05881199 -0.60512507 -0.13746933  0.182253407
## Wyoming         0.62310061  0.31778662 -0.23824049 -0.164976866</code></pre>
<pre class="r"><code># To get the variance explained by each of the prinicpal components we take the 
# square of the std deviation:
pca_var &lt;- pca_out$sdev^2
pca_var</code></pre>
<pre><code>## [1] 2.4802416 0.9897652 0.3565632 0.1734301</code></pre>
<pre class="r"><code># To get the proportion of variance explained we just need to divide by the sum:
pca_varprop &lt;- pca_var / sum(pca_var)
pca_varprop</code></pre>
<pre><code>## [1] 0.62006039 0.24744129 0.08914080 0.04335752</code></pre>
<pre class="r"><code># Plots using base R:
plot(pca_varprop , xlab = &quot; Principal Component &quot;,
     ylab = &quot;Proportion of Variance Explained &quot;,
     ylim = c(0,1), type = &quot;b&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/PCA%20Plots%201-1.png" width="576" /></p>
<pre class="r"><code>plot(cumsum(pca_varprop), xlab = &quot;Principal Component &quot;,
     ylab = &quot;Cumulative Proportion of Variance Explained&quot;,
     ylim = c(0,1), type = &quot;b&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/PCA%20Plots%201-2.png" width="576" /></p>
<pre class="r"><code># Plots using ggplot:
## Create the combined dataset:
df &lt;- tibble(PCA = 1:4, VarianceProportion = pca_varprop)

ggplot(data = df, aes(x = PCA, y = VarianceProportion)) +
    geom_line() +
    geom_point() +
    labs(x = &quot;Principal Component&quot;, y = &quot;Proportion of Variance Explained&quot;) +
    geom_text(aes(label = round(VarianceProportion,2)), vjust = -1) +
    scale_y_continuous(limits = c(0, 1))</code></pre>
<p><img src="STA314F19_files/figure-html/PCA%20Plots%201-3.png" width="576" /></p>
<pre class="r"><code>ggplot(data = df, aes(x = PCA, y = cumsum(VarianceProportion))) +
    geom_line() +
    geom_point() +
    labs(x = &quot;Principal Component&quot;, y = &quot;Cumulative Proportion of Variance Explained&quot;) +
    geom_text(aes(label = round(cumsum(VarianceProportion),2)), vjust = 2) +
    scale_y_continuous(limits = c(0, 1))</code></pre>
<p><img src="STA314F19_files/figure-html/PCA%20Plots%201-4.png" width="576" /></p>
<pre class="r"><code># standardize all the variables
USArrests_std &lt;- USArrests %&gt;% mutate_all(.funs = (scale))

varcov_matrix &lt;- cor(USArrests_std)
varcov_matrix</code></pre>
<pre><code>##              Murder   Assault   UrbanPop      Rape
## Murder   1.00000000 0.8018733 0.06957262 0.5635788
## Assault  0.80187331 1.0000000 0.25887170 0.6652412
## UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412
## Rape     0.56357883 0.6652412 0.41134124 1.0000000</code></pre>
<pre class="r"><code># look at the eigenvectors and eigenvalues of the var cov matrix
e &lt;- eigen(varcov_matrix)
e</code></pre>
<pre><code>## eigen() decomposition
## $values
## [1] 2.4802416 0.9897652 0.3565632 0.1734301
## 
## $vectors
##            [,1]       [,2]       [,3]        [,4]
## [1,] -0.5358995  0.4181809 -0.3412327  0.64922780
## [2,] -0.5831836  0.1879856 -0.2681484 -0.74340748
## [3,] -0.2781909 -0.8728062 -0.3780158  0.13387773
## [4,] -0.5434321 -0.1673186  0.8177779  0.08902432</code></pre>
<pre class="r"><code># Compute the eigenvector transformation

for (i in 1:length(names(USArrests_std))) {
    assign(paste0(&quot;PC&quot;, i), 
    as.vector(USArrests_std$Murder * e$vectors[1,i] +
    USArrests_std$Assault * e$vectors[2,i] + 
    USArrests_std$UrbanPop * e$vectors[3,i] +
    USArrests_std$Rape * e$vectors[4,i]))
}

manual_PCA &lt;- tibble(PC1 = PC1, PC2 = PC2, PC3 = PC3, PC4 = PC4)
auto_PCA &lt;- as.tibble(pca_out$x)</code></pre>
<pre><code>## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.</code></pre>
<pre class="r"><code>difference = manual_PCA - auto_PCA
difference</code></pre>
<pre><code>##              PC1           PC2           PC3           PC4
## 1  -5.551115e-16 -8.881784e-16 -3.330669e-16 -9.992007e-16
## 2   6.661338e-16 -8.881784e-16  4.440892e-16  1.942890e-15
## 3   1.110223e-15 -1.998401e-15  1.554312e-15  6.661338e-16
## 4  -8.326673e-16  2.220446e-16 -1.110223e-16 -3.885781e-16
## 5   2.220446e-15 -2.442491e-15  1.332268e-15  1.665335e-15
## 6   1.776357e-15 -1.110223e-15  2.220446e-16  1.859624e-15
## 7   0.000000e+00  8.881784e-16  2.220446e-16 -1.526557e-16
## 8   1.665335e-16 -4.996004e-16  1.110223e-15 -5.551115e-16
## 9   1.332268e-15 -3.080869e-15  8.881784e-16 -3.469447e-16
## 10 -2.220446e-16 -1.332268e-15 -1.110223e-15 -6.661338e-16
## 11  6.661338e-16  6.661338e-16 -8.326673e-16  6.661338e-16
## 12 -8.881784e-16  1.637579e-15  2.220446e-16  5.551115e-17
## 13  6.661338e-16 -1.665335e-15  6.661338e-16 -3.608225e-16
## 14 -2.220446e-16  6.106227e-16 -5.551115e-16  3.330669e-16
## 15 -4.440892e-16  2.137179e-15 -6.661338e-16  9.714451e-17
## 16 -2.220446e-16  7.771561e-16 -3.885781e-16  8.326673e-17
## 17 -1.110223e-15  9.992007e-16 -1.110223e-15 -5.551115e-16
## 18  0.000000e+00 -1.554312e-15 -4.440892e-16 -1.110223e-15
## 19 -8.881784e-16  2.442491e-15 -2.220446e-16 -4.440892e-16
## 20  4.440892e-16 -1.776357e-15  8.881784e-16 -4.440892e-16
## 21  5.551115e-16  2.220446e-16  4.440892e-16  0.000000e+00
## 22  4.440892e-16 -1.915135e-15  3.330669e-16  6.938894e-16
## 23  0.000000e+00  1.665335e-15 -3.330669e-16  2.914335e-16
## 24 -1.554312e-15 -8.881784e-16 -6.661338e-16 -1.887379e-15
## 25  5.551115e-16 -4.996004e-16 -5.551115e-17  6.106227e-16
## 26 -8.881784e-16  1.332268e-15 -4.440892e-16 -9.714451e-17
## 27 -2.220446e-16  1.304512e-15 -1.110223e-16  2.428613e-16
## 28  1.776357e-15 -2.442491e-15  2.220446e-16  1.776357e-15
## 29 -8.881784e-16  2.525757e-15 -4.440892e-16 -1.110223e-16
## 30  8.881784e-16 -6.661338e-16  2.220446e-16 -1.665335e-16
## 31  8.881784e-16 -1.915135e-15  7.771561e-16  3.885781e-16
## 32  1.554312e-15 -1.998401e-15  6.661338e-16 -2.289835e-16
## 33 -1.554312e-15 -1.332268e-15  9.992007e-16 -1.887379e-15
## 34 -1.776357e-15  2.997602e-15 -4.440892e-16 -2.220446e-16
## 35  3.608225e-16  0.000000e+00 -4.163336e-16  2.220446e-16
## 36  1.665335e-16  2.775558e-16 -2.775558e-17  1.110223e-16
## 37  6.106227e-16  1.110223e-16  3.330669e-16  1.387779e-15
## 38 -2.220446e-16  7.771561e-16 -4.440892e-16 -3.330669e-16
## 39  4.440892e-16  2.220446e-16  1.110223e-15 -8.881784e-16
## 40 -8.881784e-16 -8.881784e-16  5.551115e-17 -1.082467e-15
## 41 -1.110223e-15  2.220446e-15 -4.440892e-16 -5.551115e-17
## 42 -2.220446e-16 -5.551115e-16 -7.771561e-16 -1.110223e-16
## 43  6.661338e-16 -1.443290e-15 -3.330669e-16 -3.330669e-16
## 44  8.881784e-16  4.440892e-16  2.220446e-16  9.020562e-16
## 45 -2.220446e-15  3.108624e-15 -8.881784e-16  3.053113e-16
## 46 -2.081668e-16  1.665335e-16 -2.983724e-16 -8.326673e-17
## 47  8.326673e-16  2.220446e-16  3.330669e-16  1.110223e-15
## 48 -1.776357e-15  1.998401e-15 -8.881784e-16 -7.216450e-16
## 49 -4.440892e-16  1.776357e-15 -5.551115e-16 -5.551115e-17
## 50 -5.551115e-16  4.996004e-16  0.000000e+00 -4.718448e-16</code></pre>
</div>
<div id="lab-3-pca-and-clustering" class="section level2">
<h2>Lab 3: PCA and Clustering</h2>
<pre class="r"><code># Data Load
nci_labs &lt;- NCI60$labs
nci_data &lt;- NCI60$data

# Combine
nci &lt;- as.tibble(nci_data)
nci$labels &lt;- nci_labs

# It&#39;s a large dataset! 
dim(nci)</code></pre>
<pre><code>## [1]   64 6831</code></pre>
<pre class="r"><code># Let&#39;s see what are the possible labels
unique(nci$labels)</code></pre>
<pre><code>##  [1] &quot;CNS&quot;         &quot;RENAL&quot;       &quot;BREAST&quot;      &quot;NSCLC&quot;       &quot;UNKNOWN&quot;    
##  [6] &quot;OVARIAN&quot;     &quot;MELANOMA&quot;    &quot;PROSTATE&quot;    &quot;LEUKEMIA&quot;    &quot;K562B-repro&quot;
## [11] &quot;K562A-repro&quot; &quot;COLON&quot;       &quot;MCF7A-repro&quot; &quot;MCF7D-repro&quot;</code></pre>
<pre class="r"><code># Let&#39;s see how many of each label:
nci %&gt;% group_by(labels) %&gt;% summarize(n())</code></pre>
<pre><code>## # A tibble: 14 x 2
##    labels      `n()`
##    &lt;chr&gt;       &lt;int&gt;
##  1 BREAST          7
##  2 CNS             5
##  3 COLON           7
##  4 K562A-repro     1
##  5 K562B-repro     1
##  6 LEUKEMIA        6
##  7 MCF7A-repro     1
##  8 MCF7D-repro     1
##  9 MELANOMA        8
## 10 NSCLC           9
## 11 OVARIAN         6
## 12 PROSTATE        2
## 13 RENAL           9
## 14 UNKNOWN         1</code></pre>
<pre class="r"><code># Not very many data points, with a lot of features!</code></pre>
<p>Are all of those features really necessary? Let’s try doing PCA and seeing what proportion of variance can be explained by taking just a few:</p>
<pre class="r"><code># I don&#39;t want to rescale the labels (which are not numerical in the first place!)
nci_pca &lt;- nci_data %&gt;% prcomp(scale = TRUE)

# Since we will be plotting this data, we would like a function that assigns 
# a color based on the value of the label to each of the datapoints:

Colors = function(vec) {
    colors = rainbow(length(unique(vec)))
    return(colors[as.numeric(as.factor(vec))])
}

# Now let&#39;s plot the PCs 
par(mfrow = c(1,2))

# plot the first and second PC
plot(nci_pca$x[,1:2], col = Colors(nci_labs), pch = 19, xlab = &quot;PC1&quot;,ylab = &quot;PC2&quot;)
# plot the first and third PC
plot(nci_pca$x[,c(1,3)], col = Colors(nci_labs), pch = 19, xlab = &quot;PC1&quot;, ylab = &quot;PC3&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/NCI60%20PCA-1.png" width="576" /></p>
<pre class="r"><code># Get back to regular plotting:
par(mfrow = c(1,1))</code></pre>
<p>Now if we want to plot without having to write a function to assign colors each time, we can use ggplot:</p>
<pre class="r"><code># Save as tibble for convenience (you could probably get away with not doing this)
nci_pca_tb &lt;- as.tibble(nci_pca$x)
# Add back the labels 
nci_pca_tb$labels &lt;- nci$labels

# Plot (I&#39;m dropping the argument names beyond this point)
ggplot(nci_pca_tb, aes(x = PC1, y = PC2, color = labels)) +
    geom_point()</code></pre>
<p><img src="STA314F19_files/figure-html/Plotting%20PCA%20with%20ggplot-1.png" width="576" /></p>
<pre class="r"><code># Doesn&#39;t this look 100 times simpler to do ?
ggplot(nci_pca_tb, aes(x = PC1, y = PC3, color = labels)) +
    geom_point()</code></pre>
<p><img src="STA314F19_files/figure-html/Plotting%20PCA%20with%20ggplot-2.png" width="576" /></p>
<p>For this reason, beyond this point, I will be rewriting all of visualizations from the labs into tidyverse code and omit the base R code.</p>
<pre class="r"><code># Extract the variances
df &lt;- tibble(PCA = 1:length(nci_pca$sdev), VarianceProportion = nci_pca$sdev^2 / sum(nci_pca$sdev^2))

# Plot just the variance explanation proportions
ggplot(data = df, aes(x = PCA, y = VarianceProportion)) +
    geom_line() +
    geom_point() +
    labs(x = &quot;Principal Component&quot;, y = &quot;Proportion of Variance Explained&quot;) +
    scale_y_continuous(limits = c(0, 1))</code></pre>
<p><img src="STA314F19_files/figure-html/NCI60%20PCA%20proportion%20of%20variance-1.png" width="576" /></p>
<pre class="r"><code># Plot the cumulative variance explanation proportions 
ggplot(data = df, aes(x = PCA, y = cumsum(VarianceProportion))) +
    geom_line(color = if_else(cumsum(df$VarianceProportion) &gt; 0.9, &quot;red&quot;, &quot;green&quot;)) +
    geom_point(color = if_else(cumsum(df$VarianceProportion) &gt; 0.9, &quot;red&quot;, &quot;green&quot;)) +
    labs(x = &quot;Principal Component&quot;, y = &quot;Cumulative Proportion of Variance Explained&quot;) +
    scale_y_continuous(limits = c(0, 1)) +
    geom_line(y = 0.9)</code></pre>
<p><img src="STA314F19_files/figure-html/NCI60%20PCA%20proportion%20of%20variance-2.png" width="576" /></p>
<p>The line on the second plot shows at which point you can cut out to keep <span class="math inline">\(90\)</span>% of variance (<span class="math inline">\(90\)</span>% of information about data), which looks to be about 20 features.</p>
<p>Note that this is an improvent of an improvements since by just doing the PCA we have decreased the number of features from 6830 to 64 which is a 99.06% decrease, without losing much information!</p>
<p>Now let’s do some clustering to review what we were doing 2 weeks ago:</p>
<p>Again for simplicity of visualization I will be using the ggdendro package.</p>
<pre class="r"><code># Create a vector of types of Hierarchical clustering we covered
clustering_types &lt;- c(&quot;complete&quot;, &quot;average&quot;, &quot;single&quot;)

# Perform HC in a loop and plotting
for (method in clustering_types) {
    # Good example of why %&gt;% is a great operator and method chaining is important
    nci_data %&gt;% # Take the data
    scale %&gt;% # Rescale it
    dist %&gt;%  # Create a distance matrix 
    hclust(method = method) %&gt;% # Perform HC
    as.dendrogram %&gt;% # Change type to dendrogram
    set(&quot;labels&quot;, nci_labs) %&gt;% # Add the labels from the original data
    set(&quot;branches_k_color&quot;, k = 4) %&gt;% # Split the tree into 4 classes and color
    set(&quot;labels_cex&quot;, 0.5) %&gt;%
    plot(main = paste0(method,&quot; linkage&quot;))
}</code></pre>
<p><img src="STA314F19_files/figure-html/NCI60%20Hierarichical%20Clustering-1.png" width="576" /><img src="STA314F19_files/figure-html/NCI60%20Hierarichical%20Clustering-2.png" width="576" /><img src="STA314F19_files/figure-html/NCI60%20Hierarichical%20Clustering-3.png" width="576" /></p>
<p>This illustrates just how different results can be for the types of HC we learned. Complete and average linkages usually result in similarily sized clusters, while Single linkage usually results in a single large cluster that has single leaves added to it at each step. Now let’s compare Complete Linkage with K-Means clustering</p>
<pre class="r"><code># Create the complete linkage clustering
cl_clusters &lt;- nci_data %&gt;% # Take the data
    scale %&gt;% # Rescale it
    dist %&gt;%  # Create a distance matrix 
    hclust(method = &quot;complete&quot;) %&gt;% # Perform HC
    cutree(k = 4) # cut 4 clusters

# Create the kmeans clustering
kmeans_clusters &lt;- nci_data %&gt;% # Take the data
    scale %&gt;%
    kmeans(centers = 4, nstart = 20) # Perform K-Means for 4 clusters 20 times
    
# Create a comparison Table:
table(kmeans_clusters$cluster, cl_clusters)</code></pre>
<pre><code>##    cl_clusters
##      1  2  3  4
##   1  9  0  0  0
##   2 20  7  0  0
##   3  0  0  8  0
##   4 11  0  0  9</code></pre>
<p>From this table we can see that cluster 4 in k-means is the same as cluster 3 in complete linkage, but the other clusters are a mixture. For example cluster 3 in k-means is a combination of 11 elements from cluster 1 in CL and 9 elements of cluster 4 in CL.</p>
</div>
<div id="tree-methods" class="section level2">
<h2>Tree Methods</h2>
<p>If you are reading this and would like me to put some stuff on tree models that can’t already be found in my last years materials, let me know what is missing and I will try to put it up before the final exam.</p>
</div>
</div>
<div id="final-tutorial" class="section level1">
<h1>Final Tutorial</h1>
<p>I have been infomed that this indeed was our last tutorial. Thank you all! I talked to David about what you guys will need and the main thing you should all do over the winter break is get comfortable with basic python and numpy, or you could learn julia. If you are worried about the content, check out the first two weeks of STA414 from last year <a href="https://duvenaud.github.io/sta414/">here</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
