<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Michal Malyska" />


<title>STA314F19</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Michal Malyska</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-university"></span>
     
    Teaching
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="STA130F19.html">
        <span class="fa fa-book"></span>
         
        STA130 - Intro to Data Science Fall 2019
      </a>
    </li>
    <li>
      <a href="STA314F19.html">
        <span class="fa fa-book"></span>
         
        STA314 - Machine Learning I Fall 2019
      </a>
    </li>
    <li>
      <a href="STA220.html">
        <span class="fa fa-book"></span>
         
        STA220 - The Practice of Statistics I Summer 2019
      </a>
    </li>
    <li>
      <a href="STA314.html">
        <span class="fa fa-book"></span>
         
        STA314 - Machine Learning I Fall 2018
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-book"></span>
     
    Course Work
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Coursework_Summary.html">
        <span class="fa fa-book"></span>
         
        Coursework Summary
      </a>
    </li>
    <li>
      <a href="STA410_A1.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 1
      </a>
    </li>
    <li>
      <a href="STA410_A2.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 2
      </a>
    </li>
    <li>
      <a href="STA410_A3.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 3
      </a>
    </li>
    <li>
      <a href="STA410_A4.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 4
      </a>
    </li>
    <li>
      <a href="STA447_A2.html">
        <span class="fa fa-code"></span>
         
        STA447 - Stochastic Processes - Assignment 2
      </a>
    </li>
    <li>
      <a href="STA_490_ReactionTimes_Project.html">
        <span class="fa fa-code"></span>
         
        STA490 - Stats Consulting and Collaboration
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-pencil"></span>
     
    Personal Projects
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="ASNA2019CC_datacreation.html">
        <span class="fa fa-code"></span>
         
        ASNA2019 Case Competition
      </a>
    </li>
    <li>
      <a href="Kaggle.html">
        <span class="fa fa-code"></span>
         
        Kaggle Competitions
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-code"></span>
     
    Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="RResources.html">
        <span class="fa fa-book"></span>
         
        RStudio / R resources
      </a>
    </li>
    <li>
      <a href="StatsResources.html">
        <span class="fa fa-book"></span>
         
        General Statistics resources
      </a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    About me
  </a>
</li>
<li>
  <a href="resume.pdf">
    <span class="fa fa-file-pdf-o"></span>
     
    Resume
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">STA314F19</h1>
<h4 class="author">Michal Malyska</h4>

</div>


<div id="preliminaries-and-r-setup" class="section level1">
<h1>Preliminaries and R setup</h1>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ──────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 3.2.0     ✔ purrr   0.3.2
## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
## ✔ tidyr   0.8.3     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.4.0</code></pre>
<pre><code>## ── Conflicts ─────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(MASS)</code></pre>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     select</code></pre>
<pre class="r"><code>library(ISLR)
library(corrplot)</code></pre>
<pre><code>## corrplot 0.84 loaded</code></pre>
<pre class="r"><code>library(pROC)</code></pre>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<pre class="r"><code>library(class)</code></pre>
<div id="r-setup" class="section level2">
<h2>R Setup</h2>
<p>If you need help setting up R and Rstudio, I host a guide on my <a href="https://michalmalyska.github.io/RResources.html">resources page</a></p>
</div>
<div id="loading-data" class="section level2">
<h2>Loading Data</h2>
<p>You can find a fully commented guide on how to load in tabular data using the faster, tidyverse function read_csv() <a href="https://michalmalyska.github.io/RResources.html">here</a></p>
</div>
</div>
<div id="tutorial-1" class="section level1">
<h1>Tutorial 1</h1>
</div>
<div id="tutorial-2" class="section level1">
<h1>Tutorial 2</h1>
</div>
<div id="tutorial-3" class="section level1">
<h1>Tutorial 3</h1>
<div id="lab" class="section level2">
<h2>Lab</h2>
<pre class="r"><code>data_orig &lt;- MASS::Boston

df &lt;- as_tibble(data_orig)


df_train &lt;- df %&gt;% sample_n(size = length(df$chas) * 0.7)

df_test &lt;- setdiff(df, df_train)</code></pre>
<p>Let’s start by showing off what bad practice is and fit a linear model without doing any kind of previous work:</p>
<pre class="r"><code>model1 &lt;- lm(medv ~ lstat, data = df_train)
summary(model1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat, data = df_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.215  -3.958  -1.180   2.213  24.447 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 34.57179    0.66016   52.37   &lt;2e-16 ***
## lstat       -0.94632    0.04574  -20.69   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.163 on 352 degrees of freedom
## Multiple R-squared:  0.5488, Adjusted R-squared:  0.5475 
## F-statistic: 428.1 on 1 and 352 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>plot(model1)</code></pre>
<p><img src="STA314F19_files/figure-html/Model%20Assumptions-1.png" width="576" /><img src="STA314F19_files/figure-html/Model%20Assumptions-2.png" width="576" /><img src="STA314F19_files/figure-html/Model%20Assumptions-3.png" width="576" /><img src="STA314F19_files/figure-html/Model%20Assumptions-4.png" width="576" /></p>
<p>Assumptions are not satisfied!</p>
<pre class="r"><code>ggplot(data = df, aes(x = medv, y = ..density..)) +
  geom_density() +
  geom_histogram(bins = 75, alpha = 0.3, fill = &quot;red&quot;) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Looking%20at%20distribution%20of%20response-1.png" width="576" /></p>
<pre class="r"><code>ggplot(data = df, aes(sample = medv)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Looking%20at%20distribution%20of%20response-2.png" width="576" /></p>
<pre class="r"><code>ggplot(data = df, aes(y = medv, x = lstat)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) +
  geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cs&quot;), color = &quot;green&quot;) +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), color = &quot;blue&quot;) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Looking%20at%20distribution%20of%20response-3.png" width="576" /></p>
<p>How to make predictions:</p>
<pre class="r"><code>df_test$predictions &lt;- predict(model1, newdata = df_test)</code></pre>
<p>How to add multiple variables as predictors:</p>
<pre class="r"><code>model2 &lt;- lm(data = df_train, formula = medv ~ lstat + age)
summary(model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat + age, data = df_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.240  -3.786  -1.238   2.199  22.738 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 32.82493    0.85566  38.362  &lt; 2e-16 ***
## lstat       -1.05338    0.05651 -18.639  &lt; 2e-16 ***
## age          0.04475    0.01420   3.152  0.00176 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.087 on 351 degrees of freedom
## Multiple R-squared:  0.5612, Adjusted R-squared:  0.5587 
## F-statistic: 224.4 on 2 and 351 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>plot(model2)</code></pre>
<p><img src="STA314F19_files/figure-html/MLR-1.png" width="576" /><img src="STA314F19_files/figure-html/MLR-2.png" width="576" /><img src="STA314F19_files/figure-html/MLR-3.png" width="576" /><img src="STA314F19_files/figure-html/MLR-4.png" width="576" /></p>
<p>How to add everything (and subtract some) variables:</p>
<pre class="r"><code>model3 &lt;- lm(data = df_train, formula = medv ~ . -age -indus)
summary(model3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ . - age - indus, data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -16.2725  -2.6163  -0.6059   2.1039  24.9894 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  34.610681   5.963598   5.804 1.48e-08 ***
## crim         -0.122622   0.035139  -3.490 0.000547 ***
## zn            0.049093   0.016594   2.958 0.003308 ** 
## chas          2.969344   0.999299   2.971 0.003174 ** 
## nox         -17.555532   4.103385  -4.278 2.45e-05 ***
## rm            3.854892   0.489971   7.868 4.80e-14 ***
## dis          -1.464089   0.219273  -6.677 9.87e-11 ***
## rad           0.293201   0.074997   3.910 0.000112 ***
## tax          -0.011192   0.003961  -2.826 0.004997 ** 
## ptratio      -0.829099   0.155306  -5.338 1.71e-07 ***
## black         0.007305   0.003273   2.232 0.026268 *  
## lstat        -0.540472   0.056063  -9.640  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.755 on 342 degrees of freedom
## Multiple R-squared:  0.739,  Adjusted R-squared:  0.7306 
## F-statistic: 88.04 on 11 and 342 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>How to add interactions:</p>
<pre class="r"><code>model4 &lt;- lm(data = df_train, formula = medv ~ . -age -indus + lstat:ptratio )
summary(model4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ . - age - indus + lstat:ptratio, data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -16.0577  -2.6329  -0.6341   2.0187  25.1558 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    36.589512   7.192138   5.087 6.01e-07 ***
## crim           -0.123494   0.035222  -3.506 0.000515 ***
## zn              0.047463   0.016938   2.802 0.005366 ** 
## chas            3.014429   1.004570   3.001 0.002892 ** 
## nox           -16.714599   4.447437  -3.758 0.000201 ***
## rm              3.796580   0.504550   7.525 4.75e-13 ***
## dis            -1.432258   0.228799  -6.260 1.16e-09 ***
## rad             0.291335   0.075175   3.875 0.000128 ***
## tax            -0.011479   0.004008  -2.864 0.004440 ** 
## ptratio        -0.938570   0.270916  -3.464 0.000599 ***
## black           0.007312   0.003277   2.231 0.026300 *  
## lstat          -0.731034   0.390263  -1.873 0.061898 .  
## ptratio:lstat   0.009966   0.020197   0.493 0.622032    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.761 on 341 degrees of freedom
## Multiple R-squared:  0.7392, Adjusted R-squared:   0.73 
## F-statistic: 80.54 on 12 and 341 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>How to write high level interactions:</p>
<pre class="r"><code>model5 &lt;- lm(data = df_train, formula = medv ~ lstat*ptratio*black)
summary(model5)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat * ptratio * black, data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -12.7447  -3.4986  -0.7326   1.6692  27.4842 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)         -4.106e+01  5.052e+01  -0.813   0.4169  
## lstat                2.516e+00  2.736e+00   0.919   0.3585  
## ptratio              2.954e+00  2.549e+00   1.159   0.2474  
## black                2.817e-01  1.322e-01   2.131   0.0338 *
## lstat:ptratio       -1.370e-01  1.375e-01  -0.996   0.3197  
## lstat:black         -1.196e-02  7.206e-03  -1.660   0.0978 .
## ptratio:black       -1.250e-02  6.687e-03  -1.870   0.0623 .
## lstat:ptratio:black  5.303e-04  3.630e-04   1.461   0.1449  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.611 on 346 degrees of freedom
## Multiple R-squared:  0.6323, Adjusted R-squared:  0.6249 
## F-statistic: 85.01 on 7 and 346 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This is what our data looks like:</p>
<pre class="r"><code>ggplot(data = df_train, aes(x = lstat, y = medv)) +
  geom_point() +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/Nonlinear%20factors-1.png" width="576" /></p>
<p>How to add a funciton of a variable as a predictor and how to make simple visualizations without adding predictions:</p>
<pre class="r"><code>model6 &lt;- lm(data = df_train, formula = medv ~ lstat + I(lstat^2))
summary(model6)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat + I(lstat^2), data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.2670  -4.0100  -0.2982   2.3336  25.3675 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 42.367124   1.034271  40.963   &lt;2e-16 ***
## lstat       -2.253864   0.147919 -15.237   &lt;2e-16 ***
## I(lstat^2)   0.041232   0.004481   9.202   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.54 on 351 degrees of freedom
## Multiple R-squared:  0.6365, Adjusted R-squared:  0.6344 
## F-statistic: 307.3 on 2 and 351 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>ggplot(data = df_train, aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2)) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/unnamed-chunk-2-1.png" width="576" /></p>
<p>How to fit higher degree polynomials and visualize them:</p>
<pre class="r"><code>model7 &lt;- lm(data = df_train, formula = medv ~ poly(lstat, degree = 5))
summary(model7)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ poly(lstat, degree = 5), data = df_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.3038  -3.1736  -0.6387   2.1873  26.8217 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                22.714      0.283  80.257  &lt; 2e-16 ***
## poly(lstat, degree = 5)1 -127.518      5.325 -23.948  &lt; 2e-16 ***
## poly(lstat, degree = 5)2   50.980      5.325   9.574  &lt; 2e-16 ***
## poly(lstat, degree = 5)3  -16.865      5.325  -3.167 0.001675 ** 
## poly(lstat, degree = 5)4   20.432      5.325   3.837 0.000148 ***
## poly(lstat, degree = 5)5  -14.271      5.325  -2.680 0.007709 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.325 on 348 degrees of freedom
## Multiple R-squared:  0.667,  Adjusted R-squared:  0.6622 
## F-statistic: 139.4 on 5 and 348 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>ggplot(data = df_train, aes(x = lstat, y = medv)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, degree = 5)) +
  theme_minimal()</code></pre>
<p><img src="STA314F19_files/figure-html/unnamed-chunk-3-1.png" width="576" /></p>
<p>How to change plotting settings in base R (For tidyverse you should use gridExtra)</p>
<pre class="r"><code>par(mfrow = c(2,2))
plot(model6)</code></pre>
<p><img src="STA314F19_files/figure-html/parmfrow-1.png" width="576" /></p>
<p>Fitting character variables:</p>
<pre class="r"><code>df2 &lt;- ISLR::Carseats

df2_train &lt;- sample_n(df2, size = length(df2$Sales) * 0.7)
df2_test &lt;- setdiff(df2, df2_train)

glimpse(df2_train)</code></pre>
<pre><code>## Observations: 280
## Variables: 11
## $ Sales       &lt;dbl&gt; 4.15, 6.53, 3.89, 9.32, 8.54, 6.87, 6.20, 8.19, 12.8…
## $ CompPrice   &lt;dbl&gt; 141, 154, 123, 141, 139, 128, 128, 127, 123, 124, 13…
## $ Income      &lt;dbl&gt; 64, 30, 105, 34, 35, 105, 93, 103, 37, 25, 61, 42, 5…
## $ Advertising &lt;dbl&gt; 3, 0, 0, 16, 0, 11, 0, 0, 15, 13, 0, 4, 0, 10, 15, 1…
## $ Population  &lt;dbl&gt; 340, 122, 149, 361, 95, 249, 89, 125, 348, 87, 263, …
## $ Price       &lt;dbl&gt; 128, 162, 118, 108, 129, 131, 118, 155, 112, 110, 12…
## $ ShelveLoc   &lt;fct&gt; Bad, Medium, Bad, Medium, Medium, Medium, Medium, Go…
## $ Age         &lt;dbl&gt; 38, 57, 62, 69, 42, 63, 34, 29, 28, 57, 41, 54, 58, …
## $ Education   &lt;dbl&gt; 13, 17, 16, 10, 13, 13, 18, 15, 12, 10, 12, 15, 16, …
## $ Urban       &lt;fct&gt; Yes, No, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, No, …
## $ US          &lt;fct&gt; No, No, Yes, Yes, No, Yes, No, Yes, Yes, Yes, No, Ye…</code></pre>
<pre class="r"><code># Fit a model with a catergorical variable
model7 &lt;- lm(data = df2_train, formula = Sales ~ ShelveLoc)
summary(model7)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Sales ~ ShelveLoc, data = df2_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.2257 -1.6761  0.0587  1.4668  6.0926 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       5.5774     0.2913  19.145  &lt; 2e-16 ***
## ShelveLocGood     4.8596     0.4223  11.506  &lt; 2e-16 ***
## ShelveLocMedium   1.8083     0.3467   5.215  3.6e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.349 on 277 degrees of freedom
## Multiple R-squared:  0.3286, Adjusted R-squared:  0.3238 
## F-statistic: 67.79 on 2 and 277 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># What are ShelveLocGood and ShelveLocMedium 
contrasts(df2$ShelveLoc)</code></pre>
<pre><code>##        Good Medium
## Bad       0      0
## Good      1      0
## Medium    0      1</code></pre>
<pre class="r"><code>Load_packages &lt;- function(){
  library(tidyverse)
  library(MASS)
  library(ISLR)
}</code></pre>
<pre class="r"><code>Load_packages()</code></pre>
<p>If you would like to see some more advanced uses of the techniques; I <a href="https://michalmalyska.github.io/STA314.html#r_for_data_science_chapter_25:_many_models">covered</a> the same lab and a chapter of the R for data science textbook that deals with fitting multiple models to the same dataset using tidyverse functions.</p>
</div>
</div>
<div id="tutorial-4" class="section level1">
<h1>Tutorial 4</h1>
<div id="homework-solutions-overview" class="section level2">
<h2>Homework Solutions Overview:</h2>
<div id="question-1" class="section level3">
<h3>Question 1</h3>
<ol style="list-style-type: lower-alpha">
<li><p>We expect that with a very large number of measurements n, a flexible learning method would be able to learn the signal without as much fear of overfitting.</p></li>
<li><p>If the number of predictors p is very large and n is small then there is a greater possibility that a flexible learning method would overfit. Then we expect the inflexible method to be better in this case.</p></li>
<li><p>A highly non-linear relationship would most likely need a flexible statistical learning method to perform optimally.</p></li>
<li><p>With a very large error term variance <span class="math inline">\(\sigma^2\)</span> there is more worry about overfitting with flexible methods and thus an inflexible method would perform better.</p></li>
</ol>
</div>
<div id="question-2" class="section level3">
<h3>Question 2</h3>
<p>Bias Variance decomposition:</p>
<p><span class="math display">\[ 
\mathbb{E}  \left[(y_0 - \hat{f}(x_0))^2 \right] =
\mathbb{V}ar(\hat{f}(x_0)) + \left( \mathbb{E} \hat{f}(x_0) - \mathbb{E}y_0 \right)^2 + \mathbb{V}ar(\epsilon)
\]</span> Or equivalently:</p>
<p><span class="math display">\[ 
\mathbb{E}  \left[(y_0 - \hat{y}_0)^2 \right] =
\mathbb{V}ar(\hat{y}_0) + \left( \mathbb{E}\hat{y}_0 - \mathbb{E}y_0 \right)^2 + \mathbb{V}ar(\epsilon)
\]</span></p>
<p>The left-hand-side of the above is the expected mean square error (MSE) or a measure of how well on average our approximation function at <span class="math inline">\(x_0\)</span> is estimating the true value <span class="math inline">\(y_0\)</span>. The first term on the right-hand-side of the above expression is the error in the MSE due to errors in “fitting” the true <span class="math inline">\(f\)</span> with our approximate <span class="math inline">\(\hat{f}\)</span>. This error come from the sensitivity of the learning procedure to the finite training data set. Typically more flexible fitting methods will be more sensitive to the errors and noise in the given training set. The second term on the right-hand-side is the error in <span class="math inline">\(f \neq \hat{f}\)</span> due to using a learning algorithm that might not be able to represent the complexities in <span class="math inline">\(f\)</span>. For example, taking to be linear when the true underlying function <span class="math inline">\(f\)</span> is non-linear. The third term on the right-hand-side represents un-learnable error due to either not having all the predictive variables in our model (predictors that if we could get values for would improve our ability to learn the function <span class="math inline">\(\hat{f}\)</span> or error that is just intrinsic to the process which we are trying to model. In either case, given the data we have there is no way to reduce this component of the MSE error. A typical plots of the things suggested look like pieces from figures on pages 3 and 7 from the Week 2 Tuesday lecture notes. In figure on page 3 (right-hand-side) we have plots of the training error, testing error and the irreducible error curves. The training error shows a steady decrease (improvement) as the flexibility of the learning method increases. The test error is the red curve that initially decreases as the flexibility increase but then begins to increase again for continued increase in flexibility. The irreducible error is the constant dotted line. Notice that the point where the testing error is as close to the irreducible error would be the optimal operating point for this system. The distance between the lowest point on the testing error curve and the irreducible error gives an indication of how much bias there is in the given learning procedure, i.e. how far the best function <span class="math inline">\(\hat{f}\)</span> will be from <span class="math inline">\(f\)</span>. In figure on page 7 (left plot) we have curves representing the individual components of the bias-variance decomposition. The blue curve is the squared bias which we see gets smaller as the complexity of the model increase (we are able to model more and more complicated patters in <span class="math inline">\(\hat{f}\)</span>. The orange curve shows the variance of the learned model i.e. as we add complexity (more flexibility), the dependence on the dataset increases. So what function we get out of our learning procedure gets more sensitive to errors/ noise in the training dataset (and the error increase). The horizontal line is the irreducible error again.</p>
</div>
<div id="question-3" class="section level3">
<h3>Question 3</h3>
<p>A very flexible fitting procedure will fit non-linear functions better (if that is indeed the model generation process that is generating your data) but will be more susceptible to errors/ noise in the training dataset. A less flexible approach exchanges where it makes errors. That is a less flexible fitting procedure is unable to model the exact non-linear f but its predictions are also likely to be more stable to errors / noise in the training dataset.</p>
</div>
<div id="question-4" class="section level3">
<h3>Question 4</h3>
<p>A parametric learning procedure means the functional form of the mapping <span class="math inline">\(f\)</span> is specified, except for the parameter values, which the learning procedure must estimate. A non-parametric learning procedure is much more flexible in the forms of <span class="math inline">\(f\)</span> it can model and the learning procedure must “learn more” from the data (the functional form that should take) and then the parameters needed to estimate it. A parametric approach is generally a less flexible fitting method while a non-parametric approach is a more flexible method with the trade-offs that that characterization contains.</p>
</div>
</div>
<div id="islr-chapter-3-exercise-5" class="section level2">
<h2>ISLR Chapter 3 Exercise 5</h2>
<p><span class="math display">\[
\hat{y}_{i} =
x_{i} \frac{\sum_{i&#39;=1}^{n}\left( x_{i&#39;} y_{i&#39;} \right)}{\sum_{j=1}^{n} x_{j}^{2}}
=\sum_{i&#39;=1}^n \frac{\left(x_i x_{i&#39;}  \right)}{\sum_{j=1}^n x_j^2}y_{i&#39;} = 
\sum_{i&#39;=1}^n a_{i&#39;} y_{i&#39;}
\]</span></p>
</div>
<div id="islr-chapter-3-exercise-7" class="section level2">
<h2>ISLR Chapter 3 Exercise 7</h2>
<p>Given that <span class="math inline">\(\bar{x} = \bar{y} = 0\)</span></p>
<p><span class="math display">\[
R^2 = \frac{TSS - RSS}{TSS} = 1-  \frac{RSS}{TSS}
\]</span></p>
<p><span class="math display">\[
TSS = \sum_i \left( y_i - \bar{y} \right)^ 2 = \sum_i y_i^2
\]</span></p>
<p><span class="math display">\[ 
RSS = \sum_i \left(y_i - \hat{y_i} \right)^2 =
\sum_i \left(y_i - (\hat{\beta_0} + \hat{\beta_1} x_i) \right)^2
\]</span></p>
<p>now noting that:</p>
<p><span class="math display">\[
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x} = 0
\]</span></p>
<p>We simplify to get</p>
<p><span class="math display">\[
\sum_i \left(y_i - (\hat{\beta_0} + \hat{\beta_1} x_i) \right)^2 =
\sum_i \left(y_i - (\hat{\beta_1} x_i) \right)^2
\]</span></p>
<p>Then looking at formula for <span class="math inline">\(\hat{\beta_1}\)</span>:</p>
<p><span class="math display">\[
\hat{\beta_1} = \frac{\sum_i (x_i - \bar{x}) (y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} = \\
\frac{\sum_i x_iy_i}{\sum_i x_i^2}
\]</span></p>
<p>Plugging in and expanding</p>
<p><span class="math display">\[
\sum_i \left(y_i - (\hat{\beta_1} x_i) \right)^2 = 
\sum_i \left(y_i^2 - 2(\hat{\beta_1} x_i)y_i + (\hat{\beta_1} x_i)^2 \right) = \\
= \sum_i \left(y_i^2 - 2(\frac{\sum_i x_iy_i}{\sum_i x_i^2} x_i)y_i + (\frac{\sum_i x_iy_i}{\sum_i x_i^2} x_i)^2 \right) = \\
= \sum_i y_i^2 - 2 \frac{\sum_i x_iy_i}{\sum_i x_i^2} \sum_i x_i y_i +
\left( \frac{\sum_i x_iy_i}{\sum_i x_i^2} \right)^2 \sum_ix_i^2 = \\
= \sum_i y_i^2 - \frac{(\sum_i x_iy_i)^2}{\sum_i x_i^2}
\]</span></p>
<p>Plugging into the <span class="math inline">\(R^2\)</span> formula:</p>
<p><span class="math display">\[ 
R^2 =  \frac{TSS - RSS}{TSS} = \frac{\sum_i y_i^2 - \sum_i y_i^2 +
\frac{(\sum_i x_iy_i)^2}{\sum_i x_i^2}}{\sum_i y_i^2} = 
\frac{(\sum_i x_iy_i)^2}{\sum_i x_i^2 \sum_i y_i^2}
\]</span></p>
<p>Correlation is (since <span class="math inline">\(\bar{x} = \bar{y} = 0\)</span>):</p>
<p><span class="math display">\[ 
Corr(X,Y) = \frac{(\sum_i x_iy_i)}{\sqrt{\sum_i x_i^2}\sqrt{\sum_i y_i^2}}
\]</span></p>
</div>
<div id="additional-materials" class="section level2">
<h2>Additional Materials</h2>
<p>Great <a href="https://www.alexpghayes.com/blog/understanding-multinomial-regression-with-partial-dependence-plots/">blogpost</a> on understanding Mutinomial regression with Partial Dependence plots.</p>
<p>If you want to type up your homework 2 in RMarkdown and save on time I would suggest taking a look at the equatiomatic package. (This is also super useful for STA303)</p>
</div>
<div id="tutorial-questions" class="section level2">
<h2>Tutorial Questions</h2>
<ul>
<li>Why does logistic regression become unstable for linearly separable data?</li>
</ul>
<p>Answer:</p>
<p>If the data is perfectly linearly separable (meaning that you can draw a hyperplane such that all datapoints of either class are on different sides of it). Then the MLE solution to logistic regression does not exist. What this would mean when you are estimating coefficients is that as you train your model the estimates are going to diverge to infinity as your logistic regression is trying to approximate a step function. A good description of this can be found <a href="https://stats.stackexchange.com/a/254266">here</a></p>
</div>
</div>
<div id="tutorial-5" class="section level1">
<h1>Tutorial 5</h1>
<div id="lab-4-islr" class="section level2">
<h2>Lab 4 ISLR</h2>
<pre class="r"><code># Load the data
data_orig &lt;- ISLR::Smarket
df &lt;- data_orig

df_train &lt;- df %&gt;% sample_n(size = 0.7*1250)
df_test &lt;- setdiff(df, df_train)</code></pre>
<p>Some simple data exploration</p>
<pre class="r"><code># Tidyverse way of subsetting data and computing correlations
corrs &lt;- df %&gt;%
  dplyr::select(-Direction) %&gt;%
  cor()
corrs</code></pre>
<pre><code>##              Year         Lag1         Lag2         Lag3         Lag4
## Year   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718
## Lag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911
## Lag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533
## Lag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036
## Lag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000
## Lag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641
## Volume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246
## Today  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527
##                Lag5      Volume        Today
## Year    0.029787995  0.53900647  0.030095229
## Lag1   -0.005674606  0.04090991 -0.026155045
## Lag2   -0.003557949 -0.04338321 -0.010250033
## Lag3   -0.018808338 -0.04182369 -0.002447647
## Lag4   -0.027083641 -0.04841425 -0.006899527
## Lag5    1.000000000 -0.02200231 -0.034860083
## Volume -0.022002315  1.00000000  0.014591823
## Today  -0.034860083  0.01459182  1.000000000</code></pre>
<pre class="r"><code># Easy way to visualize correlations
corrplot(corrs, method = &quot;color&quot;, type = &quot;lower&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/Lab%204%20correlations-1.png" width="576" /></p>
<p>Pretty much no correlations between our predictors</p>
<p>Below is the “hacky” way to do correlations for variables that are not numerical. You should always stop and think whether what you are doing makes sense.</p>
<pre class="r"><code>corrs &lt;- df %&gt;%
  mutate(Direction = if_else(Direction == &quot;Up&quot;, 1, 0)) %&gt;% # Convert the string to a number
  cor()

corrplot(corrs, method = &quot;color&quot;, type = &quot;lower&quot;)</code></pre>
<p><img src="STA314F19_files/figure-html/Hacky%20way%20to%20do%20correlations-1.png" width="576" /></p>
<p>Our response is only correlated with Today. This means that we probably won’t see much statistical significance. (Note that this is not really a very rigorous approach but it’s a good benchmark to see which variables have the highest potential)</p>
<p>Fit a Logistic Regression using all the lag variables and volume.</p>
<pre class="r"><code>log_reg &lt;- glm(Direction ~ . - Today ,data = df_train , family = binomial)
summary(log_reg)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Direction ~ . - Today, family = binomial, data = df_train)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.396  -1.189   1.005   1.134   1.369  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -2.434e+02  1.151e+02  -2.115   0.0344 *
## Year         1.215e-01  5.754e-02   2.112   0.0347 *
## Lag1        -4.318e-02  5.843e-02  -0.739   0.4598  
## Lag2        -8.235e-03  6.028e-02  -0.137   0.8913  
## Lag3         8.761e-03  6.065e-02   0.144   0.8851  
## Lag4         6.095e-02  5.949e-02   1.025   0.3056  
## Lag5         2.999e-02  6.082e-02   0.493   0.6220  
## Volume       7.085e-02  2.237e-01   0.317   0.7515  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1211.9  on 874  degrees of freedom
## Residual deviance: 1202.1  on 867  degrees of freedom
## AIC: 1218.1
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>We can either make prediction on just the movement direction (by higher probability) or we can return a vector of probaabilities for each case. In the binary case it is enough to specify one. Specifying type = “response” makes sure that instead of returning the log odds, we get the probabilites directly.</p>
<pre class="r"><code>probabilities = predict(log_reg, type = &quot;response&quot;, newdata = df_test) # Extract the probabilites
glimpse(probabilities)</code></pre>
<pre><code>##  Named num [1:375] 0.424 0.469 0.465 0.46 0.476 ...
##  - attr(*, &quot;names&quot;)= chr [1:375] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...</code></pre>
<pre class="r"><code>contrasts(data_orig$Direction) # Which way is Up?</code></pre>
<pre><code>##      Up
## Down  0
## Up    1</code></pre>
<pre class="r"><code># Set Threshold
threshold &lt;- 0.5

# Pick predictions above the threshold
predictions &lt;- rep(&quot;Down&quot;,length(df_test$Year)) #initialize a vector of all predicitons to be Down
predictions[probabilities &gt; 0.5] &lt;- &quot;Up&quot; # Change those with probabilities higher
# than threshold to be Up

# Actual values
actual &lt;- df_test$Direction

# Generate a truth table for our predictions
table(predictions, actual)</code></pre>
<pre><code>##            actual
## predictions Down  Up
##        Down   55  55
##        Up    125 140</code></pre>
<p>This means we missed 113 + 66 out of the total of 375 on our test set. We separate them because cost of making an error in one way may be higher than the other.</p>
<p>Now let’s train an LDA model on the same variables (this is not the same as in the lab)</p>
<pre class="r"><code># Train the model
lda_model &lt;- MASS::lda(Direction~. - Today, data = df_train)
# Print out the model summary.
lda_model</code></pre>
<pre><code>## Call:
## lda(Direction ~ . - Today, data = df_train)
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4822857 0.5177143 
## 
## Group means:
##          Year        Lag1         Lag2        Lag3        Lag4        Lag5
## Down 2002.825  0.03877251 -0.006336493 -0.02943128 -0.07300000 -0.04856635
## Up   2003.095 -0.01965784 -0.009161148 -0.01168653  0.01851656 -0.01085651
##        Volume
## Down 1.451423
## Up   1.491795
## 
## Coefficients of linear discriminants:
##                LD1
## Year    0.57370134
## Lag1   -0.20293029
## Lag2   -0.03902997
## Lag3    0.04112472
## Lag4    0.28662321
## Lag5    0.14095625
## Volume  0.32790504</code></pre>
<pre class="r"><code># If you work with the object explorer in R you can get these formulas for how 
# to extract predictions classes and probabilities
predictions &lt;- predict(lda_model, data = data_orig, newdata = df_test)[[&quot;class&quot;]]
probabilities &lt;- predict(lda_model, data = data_orig, newdata = df_test)[[&quot;posterior&quot;]]
# Use Base R to find probabilities for Up
probabilities &lt;- probabilities[,2]

predictions &lt;- rep(&quot;Down&quot;,length(df_test$Year)) #initialize a vector of all predicitons to be Down
predictions[probabilities &gt; 0.5] &lt;- &quot;Up&quot; # Change those with probabilities higher

table(predictions, actual)</code></pre>
<pre><code>##            actual
## predictions Down  Up
##        Down   55  55
##        Up    125 140</code></pre>
<p>In the similar way we can do QDA:</p>
<pre class="r"><code># Train the model
qda_model &lt;- MASS::qda(Direction~. - Today, data = df_train)
# Print out the model summary.
qda_model</code></pre>
<pre><code>## Call:
## qda(Direction ~ . - Today, data = df_train)
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4822857 0.5177143 
## 
## Group means:
##          Year        Lag1         Lag2        Lag3        Lag4        Lag5
## Down 2002.825  0.03877251 -0.006336493 -0.02943128 -0.07300000 -0.04856635
## Up   2003.095 -0.01965784 -0.009161148 -0.01168653  0.01851656 -0.01085651
##        Volume
## Down 1.451423
## Up   1.491795</code></pre>
<pre class="r"><code># If you work with the object explorer in R you can get these formulas for how 
# to extract predictions classes and probabilities
predictions &lt;- predict(qda_model, data = data_orig, newdata = df_test)[[&quot;class&quot;]]
probabilities &lt;- predict(qda_model, data = data_orig, newdata = df_test)[[&quot;posterior&quot;]]
# Use Base R to find probabilities for Up
probabilities &lt;- probabilities[,2]

predictions &lt;- rep(&quot;Down&quot;,length(df_test$Year)) #initialize a vector of all predicitons to be Down
predictions[probabilities &gt; 0.5] &lt;- &quot;Up&quot; # Change those with probabilities higher

table(predictions, actual)</code></pre>
<pre><code>##            actual
## predictions Down  Up
##        Down   46  68
##        Up    134 127</code></pre>
<p>Finally , we will look at the K-nearest neighbourghs algorithm (KNN). It is inceredibly simple and used a lot in practice for a number of tasks. Not so much just simple classification but nevertheless it’s an extremely important algorithm for you to know.</p>
<p>Because it uses euclidean distance you need to standardize all of your variables.</p>
<pre class="r"><code># Create the scaling function cause the regular one outputs a matrix instead 
# of a vector
scale2 &lt;- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)


df_train_std &lt;- df_train %&gt;%
  dplyr::select(-Direction , -Today) %&gt;% # Remove the variables we are not considering and response
  mutate_all(.funs = scale2) # I apply the scale2() function to all other variables.
df_test_std &lt;- df_test %&gt;%
  dplyr::select(-Direction , -Today) %&gt;% # Remove the variables we are not considering and response
  mutate_all(.funs = scale2) # I apply the scale2() function to all other variables.</code></pre>
<p>Now I will use the KNN function from “class” library. To fit a knn model. It requires a training and testing dataset which is what we should have been using this entire tutorial.</p>
<pre class="r"><code>predictions &lt;- knn(train = df_train_std, # Training Set
                   test = df_test_std, # Test Set
                   cl = df_train$Direction, # Labels for Training
                   k = 3) # 3 nearest Neighbours
table(predictions, actual)</code></pre>
<pre><code>##            actual
## predictions Down  Up
##        Down   88  91
##        Up     92 104</code></pre>
<p>It’s not a model you train that has parameters so there is no summary, no coefficients, no p-values. How do you pick k? Looking at a validation set seems like a reasonable approach.</p>
<pre class="r"><code>val_indices &lt;- sample.int(n = 0.7*nrow(df_train), replace = FALSE)

df_train_std_v &lt;- df_train_std[val_indices,]
df_val_std &lt;- df_train_std[-val_indices,]
train_v_labels &lt;- df_train[val_indices,] %&gt;% pull(Direction)
val_labels &lt;- df_train[-val_indices,] %&gt;% pull(Direction)

for (k in 1:10) {
  predictions &lt;- knn(train = df_train_std_v, # Training Set
                   test = df_val_std, # Test Set
                   cl = train_v_labels, # Labels for Training
                   k = k,# 3 nearest Neighbours
                   use.all = TRUE) # if there is a Tie use all that tied.
  
cat(&quot;k is&quot;, k, &quot;\n&quot;)
# Accuracy
print(mean(predictions == val_labels))
# Truth Tables
print(table(predictions, val_labels))
}</code></pre>
<pre><code>## k is 1 
## [1] 0.4562738
##            val_labels
## predictions Down Up
##        Down   50 58
##        Up     85 70
## k is 2 
## [1] 0.4942966
##            val_labels
## predictions Down Up
##        Down   56 54
##        Up     79 74
## k is 3 
## [1] 0.4296578
##            val_labels
## predictions Down Up
##        Down   44 59
##        Up     91 69
## k is 4 
## [1] 0.4410646
##            val_labels
## predictions Down Up
##        Down   46 58
##        Up     89 70
## k is 5 
## [1] 0.4980989
##            val_labels
## predictions Down Up
##        Down   52 49
##        Up     83 79
## k is 6 
## [1] 0.5057034
##            val_labels
## predictions Down Up
##        Down   57 52
##        Up     78 76
## k is 7 
## [1] 0.486692
##            val_labels
## predictions Down Up
##        Down   48 48
##        Up     87 80
## k is 8 
## [1] 0.5285171
##            val_labels
## predictions Down Up
##        Down   55 44
##        Up     80 84
## k is 9 
## [1] 0.4980989
##            val_labels
## predictions Down Up
##        Down   47 44
##        Up     88 84
## k is 10 
## [1] 0.4942966
##            val_labels
## predictions Down Up
##        Down   51 49
##        Up     84 79</code></pre>
<p>k vary a lot between runs so I would probably pick something around 4-5.</p>
</div>
</div>
<div id="tutorial-6" class="section level1">
<h1>Tutorial 6</h1>
<div id="agenda" class="section level2">
<h2>Agenda:</h2>
<ul>
<li><p>Review of logistic regression etc. (now with tidymodels)</p></li>
<li><p>ROC and AUC</p></li>
</ul>
</div>
<div id="tidymodels" class="section level2">
<h2>Tidymodels</h2>
<p>This will be a tiny bit slower than usual as I am new to tidymodels.</p>
<p>Goals:</p>
<ul>
<li><p>Prepare data (recipes)</p></li>
<li><p>Split data (rsample)</p></li>
<li><p>Fit models</p></li>
<li><p>Analyze models (broom)</p></li>
<li><p>Tune models via Cross Validation (rsample)</p></li>
<li><p>Show ROC and AUC metrics (yardstick)</p></li>
</ul>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
