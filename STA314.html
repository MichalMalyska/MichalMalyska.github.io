<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Michal Malyska" />


<title>STA314 Materials</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}

.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Michal Malyska</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-university"></span>
     
    Teaching
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="STA303.html">
        <span class="fa fa-book"></span>
         
        STA303 - Methods of Data Analysis II
      </a>
    </li>
    <li>
      <a href="STA314.html">
        <span class="fa fa-book"></span>
         
        STA314 - Machine Learning I
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-book"></span>
     
    Course Work
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="STA410_A1.html">
        <span class="fa fa-code"></span>
         
        STA410 - Computational Statistics - Assignment 1
      </a>
    </li>
    <li>
      <a href="STA490_Project1.html">
        <span class="fa fa-code"></span>
         
        STA490 - Stats Consulting and Collaboration
      </a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-pencil"></span>
     
    Personal Projects
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="ASNA2019CC_datacreation.html">
        <span class="fa fa-code"></span>
         
        ASNA2019 Case Competition
      </a>
    </li>
    <li>
      <a href="Kaggle.html">
        <span class="fa fa-code"></span>
         
        Kaggle Competitions
      </a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">
    <span class="fa fa-info"></span>
     
    About me
  </a>
</li>
<li>
  <a href="resume.pdf">
    <span class="fa fa-file-pdf-o"></span>
     
    Resume
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">STA314 Materials</h1>
<h4 class="author"><em>Michal Malyska</em></h4>

</div>


<div id="preamble" class="section level1">
<h1>Preamble</h1>
<p>Most of the work in this file is the edited version of solutions provided by Prof. Daniel Simpson who is the instructor for this course.</p>
<div id="resources" class="section level2">
<h2>Resources</h2>
<p>There is a number of resources available for you to learn and master the tidyverse package. It’s technically not required for the course but it will make your life a lot easier.</p>
<p><a href="http://r4ds.had.co.nz/">Learn R with tidyverse</a> - this on it’s own should give you good enough background to handle most of the coding</p>
<p><a href="https://adv-r.hadley.nz/">Advanced R with tidyverse</a> - most likely far beyond the scope of what’s needed for the course</p>
<p><a href="https://www.rstudio.com/resources/cheatsheets/">Cheatsheets</a> - very useful set of cheatsheets</p>
<p>There is a bunch more teaching materials avaialble on Alex’s webiste:</p>
<p><a href="https://awstringer1.github.io/leaf2018/">Additional Labs</a></p>
</div>
<div id="library-load" class="section level2">
<h2>Library Load</h2>
<p>These are all the libraries we will be using in the course and some additional ones for the extra work done from the R for Data Science textbook:</p>
<pre class="r"><code>library(gridExtra)
library(MASS)
library(ISLR)
library(car)
library(modelr)
library(gapminder)
library(broom)
library(ggdendro)
library(dendextend)

# tidyverse goes last so that nothing overwrites the functions
library(tidyverse)
set.seed(217828)</code></pre>
</div>
</div>
<div id="week-1" class="section level1">
<h1>Week 1</h1>
<p>In the first tutorial we went over the great data notebook by Alex Stringer:</p>
<p><a href="https://awstringer1.github.io/leaf2018/prediction-rossman-store-sales.html">Week 1 Tutorial</a></p>
</div>
<div id="week-2" class="section level1">
<h1>Week 2</h1>
<div id="question-2" class="section level2">
<h2>Question 2</h2>
<pre class="r"><code>my_kmeans &lt;- function(data, k, n_starts) {
    
done = FALSE # Initialize the condition vector
n = dim(data)[1] #data is a matrix, where each row is one data point

if (k == 1) {
    cluster = rep(1, n) #this vector says which cluster each point is in
    
    centers = apply(
        X = data,
        MARGIN = 2,
        FUN = mean
        ) # Calculate the average distance
    
    cost = sum((data - centers[cluster]) ^ 2) # Compute the cost function for the single cluster
    
    return(list(cluster = cluster, cost = cost)) # Returns a list of [cluster, cost]
}

cluster_old = rep(1, n) # initialize clusters
cost_old = Inf # initialize cost

for (run in 1:n_starts) {
    cluster = rep(1, n) #this vector says which cluster each point is in
    #uniformly choose initial cluster centers
    centers = data[sample(
        x = 1:n,
        size = k,
        replace = FALSE)
    , ] # Sampling datapoints to be cluster centers
    
    while (!done) {
        # Do Step 2.1
        d = matrix(nrow = n, ncol = k) #initialize a matrix of size nxk
        for (j in 1:k) {
            d[, j] = apply(
            X = data,
            MARGIN = 1, #MARGIN = 1 =&gt; Rowwise
            FUN = function(d) sum((d - centers[j, ]) ^ 2)
        ) # Computes the cost function for each point for each cluster center
            
        }
        cluster_new = apply(
            X = d,
            MARGIN = 1,
            FUN = which.min
            ) # Take the minimum of the costs
        
# Throw an error if there is a cluster with no points in it
if (length(unique(cluster_new)) &lt; k) stop(&quot;Empty cluster!&quot;) 

        
# Do Step 2.2
        for (i in 1:k) {
            centers[i, ] = apply(
                X = data[cluster_new == i, ],
                MARGIN = 2, #MARGIN = 2 =&gt; Columnwise
                FUN = mean) # Computes mean of the cluster for each cluster
        }
        
# Check if the cluster assignements changed. If they have, set done=TRUE
        if (all(cluster == cluster_new)) {
            done = TRUE
        }
        
        # Update step 
        cluster = cluster_new
    } #end of while not done
    
cost = sum((data - centers[cluster, ]) ^ 2) # Compute the cost

if (cost_old &lt; cost) {
    cluster = cluster_old
    cost = cost_old
    }
    
    cost_old = cost
    cluster_old = cluster
} # if the cost increased, undo

return(list(cluster = cluster, cost = cost))
}</code></pre>
<div id="task-use-this-algorithm-to-make-a-4-clustering-of-the-data-set-in-question2.rdata.-comment-on-the-clustering." class="section level3">
<h3>Task : Use this algorithm to make a 4 clustering of the data set in question2.RData. Comment on the clustering.</h3>
<pre class="r"><code># Load the data from a data file 
load(&quot;~/Desktop/University/Statistics/TA-ing/STA314/T2/question2.RData&quot;)

# Load the data from a csv file
#data_q2 &lt;- read_csv(&quot;Question2_data.csv&quot;)


out = my_kmeans(dat_q2 #data 
                , 4 # number of clusters
                , 2 # number of runs
                )

dat_q2$cluster = out$cluster # Assign to the column &quot;cluster&quot; in dat_q2 the column &quot;cluster&quot; in out

dat_q2 %&gt;% ggplot(aes(x = x,y = y)) +
    geom_point(aes(colour = factor(cluster))) #plot</code></pre>
<p><img src="STA314_files/figure-html/Q2%20continued-1.png" width="672" /></p>
<p>Depending on how many times the kmeans algorithm is run, it sometimes doesn’t find all four distinct clusters. This is due to the uniform intial sampling and the fact that the bottom left and top right clusters are much smaller than the other two. Try increasing the number of runs!</p>
</div>
</div>
<div id="question-3" class="section level2">
<h2>Question 3</h2>
<pre class="r"><code># Input the data
d = matrix(c(0, 0.3, 0.4, 0.7,
0.3, 0, 0.5, 0.8,
0.4, 0.5, 0.0, 0.45,
0.7, 0.8, 0.45, 0.0), nrow = 4)


# Set it as distance
d = as.dist(d)</code></pre>
<pre class="r"><code># Plot the clusters with complete linkage:
plot(hclust(d,method = &quot;complete&quot;))</code></pre>
<p><img src="STA314_files/figure-html/Q3%20Complete%20Linkage-1.png" width="672" /></p>
<pre class="r"><code># Plot the clusters with complete linkage:
plot(hclust(d,method = &quot;single&quot;))</code></pre>
<p><img src="STA314_files/figure-html/Q3%20Single%20Linkage-1.png" width="672" /></p>
<pre class="r"><code>plot(hclust(d,method = &quot;average&quot;))</code></pre>
<p><img src="STA314_files/figure-html/Q3%20Average%20Linkage-1.png" width="672" /></p>
<p>Comparing the two dendrograms, we see that the two clustering from the complete linkage is {1,2}, {3,4}, while the two clustering from the single linkage is {1,2,3}, {4}.</p>
</div>
<div id="question-4" class="section level2">
<h2>Question 4</h2>
<p>For part a) there is not enough information. If the two linkages are equal, then they will fuse at the same hight. Otherwise, the single linkage dendrogram will merge at a lower hight as it only requires one nearby point and not all of the points to be close.</p>
<p>For part b) They’ll merge at the same hight because when you’re just merging single leaves, the linkages all reduce to the distance and are therefore equal.</p>
</div>
</div>
<div id="week-3" class="section level1">
<h1>Week 3</h1>
<div id="lab-2" class="section level2">
<h2>Lab 2</h2>
<p>Medium house value (medv) for 506 neighborhoods in Boston. Includes 13 predictors such as: Avg number of rooms in the house, Avg age of houses, socioeconomic status.</p>
<pre class="r"><code># Load the data: (from the MASS package)
data(Boston)

# Check names of variables:
names(Boston)</code></pre>
<pre><code>##  [1] &quot;crim&quot;    &quot;zn&quot;      &quot;indus&quot;   &quot;chas&quot;    &quot;nox&quot;     &quot;rm&quot;      &quot;age&quot;    
##  [8] &quot;dis&quot;     &quot;rad&quot;     &quot;tax&quot;     &quot;ptratio&quot; &quot;black&quot;   &quot;lstat&quot;   &quot;medv&quot;</code></pre>
<p>Let’s try fitting a linear model of medv ~ lstat (socioeconomic status)</p>
<pre class="r"><code># Results in an error - doesn&#39;t know where to get the data from.
# lm_fit &lt;- lm(medv ~ lstat)

# Need to specify data = , this is good practice as opposed to following the 
# order set by R inside of the functions most of the time.
lm_fit &lt;- lm(data = Boston, formula = medv ~ lstat)</code></pre>
<p>Now let’s see what the result of the lm function looks like:</p>
<pre class="r"><code># Basic information:
lm_fit</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat, data = Boston)
## 
## Coefficients:
## (Intercept)        lstat  
##       34.55        -0.95</code></pre>
<pre class="r"><code># More comprehensive:
summary(lm_fit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.168  -3.990  -1.318   2.034  24.500 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***
## lstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.216 on 504 degrees of freedom
## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 
## F-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># What are the contents of lm?
names(lm_fit)</code></pre>
<pre><code>##  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;         
##  [5] &quot;fitted.values&quot; &quot;assign&quot;        &quot;qr&quot;            &quot;df.residual&quot;  
##  [9] &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;</code></pre>
<pre class="r"><code># Extracting p-values:

## Save the summary(lm) as an object! 
sum_lm &lt;- summary(lm_fit)

## P-values are stored with coefficients in the fourth column:
### Intercept P-value:
sum_lm$coefficients[,4][1]</code></pre>
<pre><code>##   (Intercept) 
## 3.743081e-236</code></pre>
<pre class="r"><code>### lstat P-value:
sum_lm$coefficients[,4][2]</code></pre>
<pre><code>##        lstat 
## 5.081103e-88</code></pre>
<pre class="r"><code># Or you can just call it directly:
summary(lm_fit)$coefficients[,4][1]</code></pre>
<pre><code>##   (Intercept) 
## 3.743081e-236</code></pre>
<p>Now how about predicting and plotting the data:</p>
<pre class="r"><code># Find the intervals for new data

# Confidence intervals:
predict(lm_fit, data.frame(lstat = c(5, 10, 15)),
        interval = &#39;confidence&#39;)</code></pre>
<pre><code>##        fit      lwr      upr
## 1 29.80359 29.00741 30.59978
## 2 25.05335 24.47413 25.63256
## 3 20.30310 19.73159 20.87461</code></pre>
<pre class="r"><code># Prediction interals: 
predict(lm_fit, data.frame(lstat = c(5, 10, 15)),
        interval = &#39;prediction&#39;)</code></pre>
<pre><code>##        fit       lwr      upr
## 1 29.80359 17.565675 42.04151
## 2 25.05335 12.827626 37.27907
## 3 20.30310  8.077742 32.52846</code></pre>
<pre class="r"><code># Plotting:
plot(Boston$lstat, Boston$medv)
abline(lm_fit)</code></pre>
<p><img src="STA314_files/figure-html/Question%202.4%20-%20predicting%20and%20plotting%20(base)-1.png" width="672" /></p>
<pre class="r"><code># Playing around with base graphics
plot(Boston$lstat, Boston$medv)
abline(lm_fit ,lwd = 3)
abline(lm_fit ,lwd = 3,col = &quot;red&quot;)</code></pre>
<p><img src="STA314_files/figure-html/Question%202.4%20-%20predicting%20and%20plotting%20(base)-2.png" width="672" /></p>
<pre class="r"><code>plot(Boston$lstat ,Boston$medv ,col = &quot;red&quot;)</code></pre>
<p><img src="STA314_files/figure-html/Question%202.4%20-%20predicting%20and%20plotting%20(base)-3.png" width="672" /></p>
<pre class="r"><code>plot(Boston$lstat ,Boston$medv ,pch = 20)</code></pre>
<p><img src="STA314_files/figure-html/Question%202.4%20-%20predicting%20and%20plotting%20(base)-4.png" width="672" /></p>
<pre class="r"><code>plot(Boston$lstat ,Boston$medv ,pch = &quot;+&quot;)</code></pre>
<p><img src="STA314_files/figure-html/Question%202.4%20-%20predicting%20and%20plotting%20(base)-5.png" width="672" /></p>
<pre class="r"><code># Some available symbols:
plot(1:20, 1:20, pch = 1:20)</code></pre>
<p><img src="STA314_files/figure-html/Question%202.4%20-%20predicting%20and%20plotting%20(base)-6.png" width="672" /></p>
<pre class="r"><code># Plotting multiple plots on the same line
par(mfrow = c(2,2))

# plot diagnostics
plot(lm_fit)</code></pre>
<p><img src="STA314_files/figure-html/Question%202.4%20-%20predicting%20and%20plotting%20(base)-7.png" width="672" /></p>
<pre class="r"><code># revert back to 1 plot per plot
par(mfrow = c(1,1))

plot(predict(lm_fit), residuals(lm_fit))</code></pre>
<p><img src="STA314_files/figure-html/Question%202.4%20-%20predicting%20and%20plotting%20(base)-8.png" width="672" /></p>
<pre class="r"><code>plot(predict(lm_fit), rstudent(lm_fit)) # standardized residuals</code></pre>
<p><img src="STA314_files/figure-html/Question%202.4%20-%20predicting%20and%20plotting%20(base)-9.png" width="672" /></p>
<pre class="r"><code># We observe non-linearity - compute the leverage stats and see which one has the largest
plot(hatvalues(lm_fit))</code></pre>
<p><img src="STA314_files/figure-html/Question%202.4%20-%20predicting%20and%20plotting%20(base)-10.png" width="672" /></p>
<pre class="r"><code># Which observation has the highest leverage:
which.max(hatvalues(lm_fit))</code></pre>
<pre><code>## 375 
## 375</code></pre>
<pre class="r"><code># Or plotting using ggplot:
p &lt;- ggplot(data = Boston, aes(x = lstat, y = medv))
p &lt;- p + geom_point()
p &lt;- p + geom_smooth(method = &quot;lm&quot;, colour = &quot;red&quot;)
p &lt;- p + theme_bw()
p</code></pre>
<p><img src="STA314_files/figure-html/Question%202.5%20-%20plotting%20using%20ggplot-1.png" width="672" /></p>
<pre class="r"><code># We can add age to our model (without the interaction)
lm_fit2 &lt;- lm(medv ~ lstat + age, data = Boston)
summary(lm_fit2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat + age, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.981  -3.978  -1.283   1.968  23.158 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***
## lstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***
## age          0.03454    0.01223   2.826  0.00491 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.173 on 503 degrees of freedom
## Multiple R-squared:  0.5513, Adjusted R-squared:  0.5495 
## F-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># We can use all the variables available:
lm_fit3 &lt;- lm(medv ~ ., data = Boston)
summary(lm_fit3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ ., data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.595  -2.730  -0.518   1.777  26.199 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
## crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
## zn           4.642e-02  1.373e-02   3.382 0.000778 ***
## indus        2.056e-02  6.150e-02   0.334 0.738288    
## chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
## nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
## rm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***
## age          6.922e-04  1.321e-02   0.052 0.958229    
## dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
## rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
## tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
## ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
## black        9.312e-03  2.686e-03   3.467 0.000573 ***
## lstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.745 on 492 degrees of freedom
## Multiple R-squared:  0.7406, Adjusted R-squared:  0.7338 
## F-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># We can use all the variables but one:
lm_fit4 &lt;- lm(medv ~ . -age, data = Boston)
summary(lm_fit4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ . - age, data = Boston)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.6054  -2.7313  -0.5188   1.7601  26.2243 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  36.436927   5.080119   7.172 2.72e-12 ***
## crim         -0.108006   0.032832  -3.290 0.001075 ** 
## zn            0.046334   0.013613   3.404 0.000719 ***
## indus         0.020562   0.061433   0.335 0.737989    
## chas          2.689026   0.859598   3.128 0.001863 ** 
## nox         -17.713540   3.679308  -4.814 1.97e-06 ***
## rm            3.814394   0.408480   9.338  &lt; 2e-16 ***
## dis          -1.478612   0.190611  -7.757 5.03e-14 ***
## rad           0.305786   0.066089   4.627 4.75e-06 ***
## tax          -0.012329   0.003755  -3.283 0.001099 ** 
## ptratio      -0.952211   0.130294  -7.308 1.10e-12 ***
## black         0.009321   0.002678   3.481 0.000544 ***
## lstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.74 on 493 degrees of freedom
## Multiple R-squared:  0.7406, Adjusted R-squared:  0.7343 
## F-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># Can also update the previous model variables to exclude age:
update(lm_fit3, ~ . -age)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ crim + zn + indus + chas + nox + rm + dis + 
##     rad + tax + ptratio + black + lstat, data = Boston)
## 
## Coefficients:
## (Intercept)         crim           zn        indus         chas  
##   36.436927    -0.108006     0.046334     0.020562     2.689026  
##         nox           rm          dis          rad          tax  
##  -17.713540     3.814394    -1.478612     0.305786    -0.012329  
##     ptratio        black        lstat  
##   -0.952211     0.009321    -0.523852</code></pre>
<pre class="r"><code>summary(lm_fit3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ ., data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.595  -2.730  -0.518   1.777  26.199 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
## crim        -1.080e-01  3.286e-02  -3.287 0.001087 ** 
## zn           4.642e-02  1.373e-02   3.382 0.000778 ***
## indus        2.056e-02  6.150e-02   0.334 0.738288    
## chas         2.687e+00  8.616e-01   3.118 0.001925 ** 
## nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
## rm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***
## age          6.922e-04  1.321e-02   0.052 0.958229    
## dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
## rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
## tax         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
## ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
## black        9.312e-03  2.686e-03   3.467 0.000573 ***
## lstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.745 on 492 degrees of freedom
## Multiple R-squared:  0.7406, Adjusted R-squared:  0.7338 
## F-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>One big problem with multiple regression is Multicolinearity, to investigate if this is an issue with our models we will be using the car package.</p>
<pre class="r"><code># print Variance Inflation Factors: Common cutoffs are 10 or 5
vif(lm_fit3)</code></pre>
<pre><code>##     crim       zn    indus     chas      nox       rm      age      dis 
## 1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 
##      rad      tax  ptratio    black    lstat 
## 7.484496 9.008554 1.799084 1.348521 2.941491</code></pre>
<p>The interpretation of VIF is that if say VIF(tax) = ~9, then the standard error for the coefficient associated with tax is $  = 3 $ times as large as it would be if the variables were uncorrelated.</p>
<pre class="r"><code># If we want to include the interactions between variables we use the * symbol 
summary(lm(medv ~ lstat * age, data = Boston))</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat * age, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.806  -4.045  -1.333   2.085  27.552 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***
## lstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***
## age         -0.0007209  0.0198792  -0.036   0.9711    
## lstat:age    0.0041560  0.0018518   2.244   0.0252 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.149 on 502 degrees of freedom
## Multiple R-squared:  0.5557, Adjusted R-squared:  0.5531 
## F-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># it automatically includes the variables themselves in the call!

# We can also add non-linear transformations of predictors:
lm_fit_square &lt;- lm(medv ~ lstat + I(lstat^2), data = Boston)
summary(lm_fit_square)</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat + I(lstat^2), data = Boston)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.2834  -3.8313  -0.5295   2.3095  25.4148 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***
## lstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***
## I(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.524 on 503 degrees of freedom
## Multiple R-squared:  0.6407, Adjusted R-squared:  0.6393 
## F-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>plot(lm_fit_square)</code></pre>
<p><img src="STA314_files/figure-html/Question%202.8%20-%20Interactions-1.png" width="672" /><img src="STA314_files/figure-html/Question%202.8%20-%20Interactions-2.png" width="672" /><img src="STA314_files/figure-html/Question%202.8%20-%20Interactions-3.png" width="672" /><img src="STA314_files/figure-html/Question%202.8%20-%20Interactions-4.png" width="672" /></p>
<pre class="r"><code># Test if the model with a square is better than the simpler one:
anova(lm_fit, lm_fit_square)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: medv ~ lstat
## Model 2: medv ~ lstat + I(lstat^2)
##   Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
## 1    504 19472                                 
## 2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code># We can include higher polynomials:
summary(lm(medv ~ poly(lstat, 10), data = Boston))</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ poly(lstat, 10), data = Boston)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.5340  -3.0286  -0.7507   2.0437  26.4738 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         22.5328     0.2311  97.488  &lt; 2e-16 ***
## poly(lstat, 10)1  -152.4595     5.1993 -29.323  &lt; 2e-16 ***
## poly(lstat, 10)2    64.2272     5.1993  12.353  &lt; 2e-16 ***
## poly(lstat, 10)3   -27.0511     5.1993  -5.203 2.88e-07 ***
## poly(lstat, 10)4    25.4517     5.1993   4.895 1.33e-06 ***
## poly(lstat, 10)5   -19.2524     5.1993  -3.703 0.000237 ***
## poly(lstat, 10)6     6.5088     5.1993   1.252 0.211211    
## poly(lstat, 10)7     1.9416     5.1993   0.373 0.708977    
## poly(lstat, 10)8    -6.7299     5.1993  -1.294 0.196133    
## poly(lstat, 10)9     8.4168     5.1993   1.619 0.106116    
## poly(lstat, 10)10   -7.3351     5.1993  -1.411 0.158930    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.199 on 495 degrees of freedom
## Multiple R-squared:  0.6867, Adjusted R-squared:  0.6804 
## F-statistic: 108.5 on 10 and 495 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># As we can see up to the 5th power all are statistically significant!

# We can also include different functions:
summary(lm(medv ~ log(lstat), data = Boston))</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ log(lstat), data = Boston)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.4599  -3.5006  -0.6686   2.1688  26.0129 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  52.1248     0.9652   54.00   &lt;2e-16 ***
## log(lstat)  -12.4810     0.3946  -31.63   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.329 on 504 degrees of freedom
## Multiple R-squared:  0.6649, Adjusted R-squared:  0.6643 
## F-statistic:  1000 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="r-for-data-science-chapter-25-many-models" class="section level2">
<h2>R for data science chapter 25: Many Models</h2>
<p>This is beyond the scope of what we teach, but you might find it very useful in practice:</p>
<pre class="r"><code># We will be using the modelr and gapminder libraries for this part
gapminder</code></pre>
<pre><code>## # A tibble: 1,704 x 6
##    country     continent  year lifeExp      pop gdpPercap
##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;
##  1 Afghanistan Asia       1952    28.8  8425333      779.
##  2 Afghanistan Asia       1957    30.3  9240934      821.
##  3 Afghanistan Asia       1962    32.0 10267083      853.
##  4 Afghanistan Asia       1967    34.0 11537966      836.
##  5 Afghanistan Asia       1972    36.1 13079460      740.
##  6 Afghanistan Asia       1977    38.4 14880372      786.
##  7 Afghanistan Asia       1982    39.9 12881816      978.
##  8 Afghanistan Asia       1987    40.8 13867957      852.
##  9 Afghanistan Asia       1992    41.7 16317921      649.
## 10 Afghanistan Asia       1997    41.8 22227415      635.
## # ... with 1,694 more rows</code></pre>
<pre class="r"><code># plot life expectancy over time by country
gapminder %&gt;% 
  ggplot(aes(x = year, y = lifeExp, group = country)) +
    geom_line(alpha = 1/3)</code></pre>
<p><img src="STA314_files/figure-html/CH25:%20First%20Plot-1.png" width="672" /></p>
<p>It’s really hard to see what is going on!</p>
<p>Solution: Do it by country and nest all of the results in a table</p>
<pre class="r"><code># Create a tibble that separates each country&#39;s data into a separate tibble:
by_country &lt;- gapminder %&gt;% 
  group_by(country, continent) %&gt;% 
  nest()</code></pre>
<pre><code>## Warning: package &#39;bindrcpp&#39; was built under R version 3.4.4</code></pre>
<pre class="r"><code># We group by country and continent since for a country continent is fixed and we 
# want to carry on another variable

# View the data:
by_country</code></pre>
<pre><code>## # A tibble: 142 x 3
##    country     continent data             
##    &lt;fct&gt;       &lt;fct&gt;     &lt;list&gt;           
##  1 Afghanistan Asia      &lt;tibble [12 × 4]&gt;
##  2 Albania     Europe    &lt;tibble [12 × 4]&gt;
##  3 Algeria     Africa    &lt;tibble [12 × 4]&gt;
##  4 Angola      Africa    &lt;tibble [12 × 4]&gt;
##  5 Argentina   Americas  &lt;tibble [12 × 4]&gt;
##  6 Australia   Oceania   &lt;tibble [12 × 4]&gt;
##  7 Austria     Europe    &lt;tibble [12 × 4]&gt;
##  8 Bahrain     Asia      &lt;tibble [12 × 4]&gt;
##  9 Bangladesh  Asia      &lt;tibble [12 × 4]&gt;
## 10 Belgium     Europe    &lt;tibble [12 × 4]&gt;
## # ... with 132 more rows</code></pre>
<p>In this dataset each row is the complete dataset for a country, instead of being just one of the observations.</p>
<p>Now we want to fit a separate model for each of those rows:</p>
<pre class="r"><code># First we define the function that creates a linear model:
country_model &lt;- function(df) {
  lm(lifeExp ~ year, data = df)
}

# Then we can abuse the purrr package to apply that function to each of 
# the elements of a list:
by_country &lt;- by_country %&gt;%
    mutate(model = map(data, country_model))

# Now the column &quot;model&quot; in the by_country tibble contains all of the linear models we just fit!
by_country</code></pre>
<pre><code>## # A tibble: 142 x 4
##    country     continent data              model   
##    &lt;fct&gt;       &lt;fct&gt;     &lt;list&gt;            &lt;list&gt;  
##  1 Afghanistan Asia      &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt;
##  2 Albania     Europe    &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt;
##  3 Algeria     Africa    &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt;
##  4 Angola      Africa    &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt;
##  5 Argentina   Americas  &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt;
##  6 Australia   Oceania   &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt;
##  7 Austria     Europe    &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt;
##  8 Bahrain     Asia      &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt;
##  9 Bangladesh  Asia      &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt;
## 10 Belgium     Europe    &lt;tibble [12 × 4]&gt; &lt;S3: lm&gt;
## # ... with 132 more rows</code></pre>
<p>Now we want to look at the residuals, acessing models stored within tibbles is a hassle, so we can unnest the models:</p>
<pre class="r"><code>by_country &lt;- by_country %&gt;% 
  mutate(
    resids = map2(data, model, add_residuals)
  )

# Let&#39;s unnest the residuals:
resids &lt;- unnest(by_country, resids)

# Plot the residuals:
resids %&gt;% 
  ggplot(aes(year, resid)) +
    geom_line(aes(group = country), alpha = 1 / 3) + 
    geom_smooth(se = FALSE)</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39;</code></pre>
<p><img src="STA314_files/figure-html/CH25:%20Unnesting-1.png" width="672" /></p>
<pre class="r"><code># Let&#39;s see the residuals by continent:
resids %&gt;% 
  ggplot(aes(year, resid, group = country)) +
    geom_line(alpha = 1 / 3) + 
    facet_wrap(~continent)</code></pre>
<p><img src="STA314_files/figure-html/CH25:%20Unnesting-2.png" width="672" /></p>
</div>
</div>
<div id="week-4" class="section level1">
<h1>Week 4</h1>
<div id="principal-component-analysis" class="section level2">
<h2>Principal Component Analysis</h2>
<p><strong>With thanks to Alex Stringer for teaching me this in STA414</strong></p>
<div id="motivation" class="section level3">
<h3>Motivation:</h3>
<p>Sometimes we are given a dataset containing a much larger number of features than what we like for analysis. (We will see this specifically in lab 3). <em>Principal Component Analysis</em> is one way of reducing the number of features while maintaining as much information about the original data as possible.</p>
</div>
<div id="procedure" class="section level3">
<h3>Procedure:</h3>
<p>Given a <span class="math inline">\(n\)</span> datapoints with <span class="math inline">\(p\)</span> features each, PCA tries to find a low-dimensional <span class="math inline">\(d &lt; p\)</span> factorization of the data matrix <span class="math inline">\(X\)</span> that preserves the maximum possible variance.</p>
<p><span class="math display">\[ X = UZ \]</span> <span class="math display">\[ X \in \mathbb{R}^{n \times p} \]</span> <span class="math display">\[ Z \in \mathbb{R}^{d \times p} \]</span> <span class="math display">\[ U \in \mathbb{R}^{n \times d} \]</span></p>
<p>We estimate <span class="math inline">\(U\)</span> from the data, and call the associated <span class="math inline">\(Z\)</span> the principal components of <span class="math inline">\(X\)</span>.</p>
<p>PCA is thus states as follows:</p>
<center>
<p>for <span class="math inline">\(j = 1, ..., d\)</span></p>
</center>
<p><span class="math display">\[ \mathbf{u}_j = argmax(Var(\mathbf{u}^T\mathbf{x})) = argmax(\mathbf{u}^T\mathbf{Su}) \]</span></p>
<center>
<p>subject to:</p>
</center>
<p><span class="math display">\[ \mathbf{u}^T\mathbf{u} = 1 ,~~ \text{and } ~ \mathbf{u} \perp \mathbf{u}_k ~~ \text{for} ~~ k &lt; j \]</span></p>
<p>Where S is the sample covariance matrix: <span class="math display">\[ \mathbf{S} = \frac{1}{n}\sum_{i = 1}^{n}{(\mathbf{x_i} - \mathbf{\bar{x}})(\mathbf{x_i} - \mathbf{\bar{x}}) ^T} \]</span></p>
<p>Using lagrange multipliers we see the solution to the above problem must satisfy:</p>
<p><span class="math display">\[ \mathbf{S}\mathbf{u}_1 = \lambda \mathbf{u}_1 \]</span> Which means that <span class="math inline">\(\mathbf{u}_1\)</span> is an eigenvector of S with the eigenvalue <span class="math inline">\(\lambda\)</span>.</p>
<p>By definition of the problem <span class="math inline">\(\lambda\)</span> must be the largest eigenvalue. This is since it doesn’t the second constraint (as there is no previously selected vectors)</p>
<p>Solving this constrained optimization problem gives us an orthonormal basis where the basis vectors point in the directions of the principal axes of the sample covariance matrix, in decreasing order of length.</p>
<p>It’s equivalent to the rotation in the original input space!</p>
<p>We then proceed to “chop off” the <span class="math inline">\(d-p\)</span> dimensions with least variance. And call this the basis for our <span class="math inline">\(d\)</span> dimensional space.</p>
<p>So, the solution to the PCA problem is:</p>
<ol style="list-style-type: decimal">
<li><p>Choose <span class="math inline">\(\mathbf{u}_j\)</span> to be normalized eigenvector of <span class="math inline">\(\mathbf{S}\)</span> corresponding to the <span class="math inline">\(j\)</span>-th highest eigenvalue.</p></li>
<li><p>Choose <span class="math inline">\(\mathbf{U}\)</span> to be the matrix of orthonormal eigenvectors of S, so that <span class="math inline">\(\mathbf{U}^T\mathbf{U} = \mathbf{I}\)</span></p></li>
<li><p>Then <span class="math inline">\(\mathbf{Z} = \mathbf{XU}^T\)</span>.</p></li>
<li><p>Keep only the first d columns of <span class="math inline">\(\mathbf{Z}\)</span> and the corresponding <span class="math inline">\(d \times d\)</span> submatrix of <span class="math inline">\(\mathbf{U}\)</span></p></li>
<li><p>Reconstruct the data as <span class="math inline">\(\mathbf{X}^* = \mathbf{Z}^*\mathbf{U}^*\)</span></p></li>
</ol>
</div>
</div>
<div id="lab-1-pca" class="section level2">
<h2>Lab 1: PCA</h2>
<pre class="r"><code># See variable names
colnames(USArrests)</code></pre>
<pre><code>## [1] &quot;Murder&quot;   &quot;Assault&quot;  &quot;UrbanPop&quot; &quot;Rape&quot;</code></pre>
<pre class="r"><code># Display means of each row
USArrests %&gt;% summarize_all(funs(mean))</code></pre>
<pre><code>##   Murder Assault UrbanPop   Rape
## 1  7.788  170.76    65.54 21.232</code></pre>
<pre class="r"><code># Display means and variances of each row
USArrests %&gt;% summarize_all(.funs = c(Mean = mean, Variance = var))</code></pre>
<pre><code>##   Murder_Mean Assault_Mean UrbanPop_Mean Rape_Mean Murder_Variance
## 1       7.788       170.76         65.54    21.232        18.97047
##   Assault_Variance UrbanPop_Variance Rape_Variance
## 1         6945.166          209.5188      87.72916</code></pre>
<p>There is a built-in function in R to do the principal component analysis:</p>
<p>It automatically scales the data to have the mean of 0</p>
<p>There is an added parameter <em>scale</em> which will also scale the standard deviation to 1.</p>
<p>In general there are at most <span class="math inline">\(min(n-1,p)\)</span> informative principal components.</p>
<pre class="r"><code># compute the PCA
pca_out = prcomp(USArrests , scale = TRUE)
# output the rotation matrix
pca_out$rotation</code></pre>
<pre><code>##                 PC1        PC2        PC3         PC4
## Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
## Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
## UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
## Rape     -0.5434321 -0.1673186  0.8177779  0.08902432</code></pre>
<pre class="r"><code># output the reconstructed data in the new coordinates:
pca_out$x</code></pre>
<pre><code>##                        PC1         PC2         PC3          PC4
## Alabama        -0.97566045  1.12200121 -0.43980366  0.154696581
## Alaska         -1.93053788  1.06242692  2.01950027 -0.434175454
## Arizona        -1.74544285 -0.73845954  0.05423025 -0.826264240
## Arkansas        0.13999894  1.10854226  0.11342217 -0.180973554
## California     -2.49861285 -1.52742672  0.59254100 -0.338559240
## Colorado       -1.49934074 -0.97762966  1.08400162  0.001450164
## Connecticut     1.34499236 -1.07798362 -0.63679250 -0.117278736
## Delaware       -0.04722981 -0.32208890 -0.71141032 -0.873113315
## Florida        -2.98275967  0.03883425 -0.57103206 -0.095317042
## Georgia        -1.62280742  1.26608838 -0.33901818  1.065974459
## Hawaii          0.90348448 -1.55467609  0.05027151  0.893733198
## Idaho           1.62331903  0.20885253  0.25719021 -0.494087852
## Illinois       -1.36505197 -0.67498834 -0.67068647 -0.120794916
## Indiana         0.50038122 -0.15003926  0.22576277  0.420397595
## Iowa            2.23099579 -0.10300828  0.16291036  0.017379470
## Kansas          0.78887206 -0.26744941  0.02529648  0.204421034
## Kentucky        0.74331256  0.94880748 -0.02808429  0.663817237
## Louisiana      -1.54909076  0.86230011 -0.77560598  0.450157791
## Maine           2.37274014  0.37260865 -0.06502225 -0.327138529
## Maryland       -1.74564663  0.42335704 -0.15566968 -0.553450589
## Massachusetts   0.48128007 -1.45967706 -0.60337172 -0.177793902
## Michigan       -2.08725025 -0.15383500  0.38100046  0.101343128
## Minnesota       1.67566951 -0.62590670  0.15153200  0.066640316
## Mississippi    -0.98647919  2.36973712 -0.73336290  0.213342049
## Missouri       -0.68978426 -0.26070794  0.37365033  0.223554811
## Montana         1.17353751  0.53147851  0.24440796  0.122498555
## Nebraska        1.25291625 -0.19200440  0.17380930  0.015733156
## Nevada         -2.84550542 -0.76780502  1.15168793  0.311354436
## New Hampshire   2.35995585 -0.01790055  0.03648498 -0.032804291
## New Jersey     -0.17974128 -1.43493745 -0.75677041  0.240936580
## New Mexico     -1.96012351  0.14141308  0.18184598 -0.336121113
## New York       -1.66566662 -0.81491072 -0.63661186 -0.013348844
## North Carolina -1.11208808  2.20561081 -0.85489245 -0.944789648
## North Dakota    2.96215223  0.59309738  0.29824930 -0.251434626
## Ohio            0.22369436 -0.73477837 -0.03082616  0.469152817
## Oklahoma        0.30864928 -0.28496113 -0.01515592  0.010228476
## Oregon         -0.05852787 -0.53596999  0.93038718 -0.235390872
## Pennsylvania    0.87948680 -0.56536050 -0.39660218  0.355452378
## Rhode Island    0.85509072 -1.47698328 -1.35617705 -0.607402746
## South Carolina -1.30744986  1.91397297 -0.29751723 -0.130145378
## South Dakota    1.96779669  0.81506822  0.38538073 -0.108470512
## Tennessee      -0.98969377  0.85160534  0.18619262  0.646302674
## Texas          -1.34151838 -0.40833518 -0.48712332  0.636731051
## Utah            0.54503180 -1.45671524  0.29077592 -0.081486749
## Vermont         2.77325613  1.38819435  0.83280797 -0.143433697
## Virginia        0.09536670  0.19772785  0.01159482  0.209246429
## Washington      0.21472339 -0.96037394  0.61859067 -0.218628161
## West Virginia   2.08739306  1.41052627  0.10372163  0.130583080
## Wisconsin       2.05881199 -0.60512507 -0.13746933  0.182253407
## Wyoming         0.62310061  0.31778662 -0.23824049 -0.164976866</code></pre>
<pre class="r"><code># To get the variance explained by each of the prinicpal components we take the 
# square of the std deviation:
pca_var &lt;- pca_out$sdev^2
pca_var</code></pre>
<pre><code>## [1] 2.4802416 0.9897652 0.3565632 0.1734301</code></pre>
<pre class="r"><code># To get the proportion of variance explained we just need to divide by the sum:
pca_varprop &lt;- pca_var / sum(pca_var)
pca_varprop</code></pre>
<pre><code>## [1] 0.62006039 0.24744129 0.08914080 0.04335752</code></pre>
<pre class="r"><code># Plots using base R:
plot(pca_varprop , xlab = &quot; Principal Component &quot;,
     ylab = &quot;Proportion of Variance Explained &quot;,
     ylim = c(0,1), type = &quot;b&quot;)</code></pre>
<p><img src="STA314_files/figure-html/plots-1.png" width="672" /></p>
<pre class="r"><code>plot(cumsum(pca_varprop), xlab = &quot;Principal Component &quot;,
     ylab = &quot;Cumulative Proportion of Variance Explained&quot;,
     ylim = c(0,1), type = &quot;b&quot;)</code></pre>
<p><img src="STA314_files/figure-html/plots-2.png" width="672" /></p>
<pre class="r"><code># Plots using ggplot:
## Create the combined dataset:
df &lt;- tibble(PCA = 1:4, VarianceProportion = pca_varprop)

ggplot(data = df, aes(x = PCA, y = VarianceProportion)) +
    geom_line() +
    geom_point() +
    labs(x = &quot;Principal Component&quot;, y = &quot;Proportion of Variance Explained&quot;) +
    geom_text(aes(label = round(VarianceProportion,2)), vjust = -1) +
    scale_y_continuous(limits = c(0, 1))</code></pre>
<p><img src="STA314_files/figure-html/plots-3.png" width="672" /></p>
<pre class="r"><code>ggplot(data = df, aes(x = PCA, y = cumsum(VarianceProportion))) +
    geom_line() +
    geom_point() +
    labs(x = &quot;Principal Component&quot;, y = &quot;Cumulative Proportion of Variance Explained&quot;) +
    geom_text(aes(label = round(cumsum(VarianceProportion),2)), vjust = 2) +
    scale_y_continuous(limits = c(0, 1))</code></pre>
<p><img src="STA314_files/figure-html/plots-4.png" width="672" /></p>
<pre class="r"><code># standardize all the variables
USArrests_std &lt;- USArrests %&gt;% mutate_all(.funs = (scale))

varcov_matrix &lt;- cor(USArrests_std)
varcov_matrix</code></pre>
<pre><code>##              Murder   Assault   UrbanPop      Rape
## Murder   1.00000000 0.8018733 0.06957262 0.5635788
## Assault  0.80187331 1.0000000 0.25887170 0.6652412
## UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412
## Rape     0.56357883 0.6652412 0.41134124 1.0000000</code></pre>
<pre class="r"><code># look at the eigenvectors and eigenvalues of the var cov matrix
e &lt;- eigen(varcov_matrix)
e</code></pre>
<pre><code>## eigen() decomposition
## $values
## [1] 2.4802416 0.9897652 0.3565632 0.1734301
## 
## $vectors
##            [,1]       [,2]       [,3]        [,4]
## [1,] -0.5358995  0.4181809 -0.3412327  0.64922780
## [2,] -0.5831836  0.1879856 -0.2681484 -0.74340748
## [3,] -0.2781909 -0.8728062 -0.3780158  0.13387773
## [4,] -0.5434321 -0.1673186  0.8177779  0.08902432</code></pre>
<pre class="r"><code># Compute the eigenvector transformation

for (i in 1:length(names(USArrests_std))) {
    assign(paste0(&quot;PC&quot;, i), 
    as.vector(USArrests_std$Murder * e$vectors[1,i] +
    USArrests_std$Assault * e$vectors[2,i] + 
    USArrests_std$UrbanPop * e$vectors[3,i] +
    USArrests_std$Rape * e$vectors[4,i]))
}

manual_PCA &lt;- tibble(PC1 = PC1, PC2 = PC2, PC3 = PC3, PC4 = PC4)
auto_PCA &lt;- as.tibble(pca_out$x)

difference = manual_PCA - auto_PCA
difference</code></pre>
<pre><code>##              PC1           PC2           PC3           PC4
## 1  -5.551115e-16 -8.881784e-16 -3.330669e-16 -9.992007e-16
## 2   6.661338e-16 -8.881784e-16  4.440892e-16  1.942890e-15
## 3   1.110223e-15 -1.998401e-15  1.554312e-15  6.661338e-16
## 4  -8.326673e-16  2.220446e-16 -1.110223e-16 -3.885781e-16
## 5   2.220446e-15 -2.442491e-15  1.332268e-15  1.665335e-15
## 6   1.776357e-15 -1.110223e-15  2.220446e-16  1.859624e-15
## 7   0.000000e+00  8.881784e-16  2.220446e-16 -1.526557e-16
## 8   1.665335e-16 -4.996004e-16  1.110223e-15 -5.551115e-16
## 9   1.332268e-15 -3.080869e-15  8.881784e-16 -3.469447e-16
## 10 -2.220446e-16 -1.332268e-15 -1.110223e-15 -6.661338e-16
## 11  6.661338e-16  6.661338e-16 -8.326673e-16  6.661338e-16
## 12 -8.881784e-16  1.637579e-15  2.220446e-16  5.551115e-17
## 13  6.661338e-16 -1.665335e-15  6.661338e-16 -3.608225e-16
## 14 -2.220446e-16  6.106227e-16 -5.551115e-16  3.330669e-16
## 15 -4.440892e-16  2.137179e-15 -6.661338e-16  9.714451e-17
## 16 -2.220446e-16  7.771561e-16 -3.885781e-16  8.326673e-17
## 17 -1.110223e-15  9.992007e-16 -1.110223e-15 -5.551115e-16
## 18  0.000000e+00 -1.554312e-15 -4.440892e-16 -1.110223e-15
## 19 -8.881784e-16  2.442491e-15 -2.220446e-16 -4.440892e-16
## 20  4.440892e-16 -1.776357e-15  8.881784e-16 -4.440892e-16
## 21  5.551115e-16  2.220446e-16  4.440892e-16  0.000000e+00
## 22  4.440892e-16 -1.915135e-15  3.330669e-16  6.938894e-16
## 23  0.000000e+00  1.665335e-15 -3.330669e-16  2.914335e-16
## 24 -1.554312e-15 -8.881784e-16 -6.661338e-16 -1.887379e-15
## 25  5.551115e-16 -4.996004e-16 -5.551115e-17  6.106227e-16
## 26 -8.881784e-16  1.332268e-15 -4.440892e-16 -9.714451e-17
## 27 -2.220446e-16  1.304512e-15 -1.110223e-16  2.428613e-16
## 28  1.776357e-15 -2.442491e-15  2.220446e-16  1.776357e-15
## 29 -8.881784e-16  2.525757e-15 -4.440892e-16 -1.110223e-16
## 30  8.881784e-16 -6.661338e-16  2.220446e-16 -1.665335e-16
## 31  8.881784e-16 -1.915135e-15  7.771561e-16  3.885781e-16
## 32  1.554312e-15 -1.998401e-15  6.661338e-16 -2.289835e-16
## 33 -1.554312e-15 -1.332268e-15  9.992007e-16 -1.887379e-15
## 34 -1.776357e-15  2.997602e-15 -4.440892e-16 -2.220446e-16
## 35  3.608225e-16  0.000000e+00 -4.163336e-16  2.220446e-16
## 36  1.665335e-16  2.775558e-16 -2.775558e-17  1.110223e-16
## 37  6.106227e-16  1.110223e-16  3.330669e-16  1.387779e-15
## 38 -2.220446e-16  7.771561e-16 -4.440892e-16 -3.330669e-16
## 39  4.440892e-16  2.220446e-16  1.110223e-15 -8.881784e-16
## 40 -8.881784e-16 -8.881784e-16  5.551115e-17 -1.082467e-15
## 41 -1.110223e-15  2.220446e-15 -4.440892e-16 -5.551115e-17
## 42 -2.220446e-16 -5.551115e-16 -7.771561e-16 -1.110223e-16
## 43  6.661338e-16 -1.443290e-15 -3.330669e-16 -3.330669e-16
## 44  8.881784e-16  4.440892e-16  2.220446e-16  9.020562e-16
## 45 -2.220446e-15  3.108624e-15 -8.881784e-16  3.053113e-16
## 46 -2.081668e-16  1.665335e-16 -2.983724e-16 -8.326673e-17
## 47  8.326673e-16  2.220446e-16  3.330669e-16  1.110223e-15
## 48 -1.776357e-15  1.998401e-15 -8.881784e-16 -7.216450e-16
## 49 -4.440892e-16  1.776357e-15 -5.551115e-16 -5.551115e-17
## 50 -5.551115e-16  4.996004e-16  0.000000e+00 -4.718448e-16</code></pre>
</div>
<div id="lab-3-pca-and-clustering" class="section level2">
<h2>Lab 3: PCA and Clustering</h2>
<pre class="r"><code># Data Load
nci_labs &lt;- NCI60$labs
nci_data &lt;- NCI60$data

# Combine
nci &lt;- as.tibble(nci_data)
nci$labels &lt;- nci_labs

# It&#39;s a large dataset! 
dim(nci)</code></pre>
<pre><code>## [1]   64 6831</code></pre>
<pre class="r"><code># Let&#39;s see what are the possible labels
unique(nci$labels)</code></pre>
<pre><code>##  [1] &quot;CNS&quot;         &quot;RENAL&quot;       &quot;BREAST&quot;      &quot;NSCLC&quot;       &quot;UNKNOWN&quot;    
##  [6] &quot;OVARIAN&quot;     &quot;MELANOMA&quot;    &quot;PROSTATE&quot;    &quot;LEUKEMIA&quot;    &quot;K562B-repro&quot;
## [11] &quot;K562A-repro&quot; &quot;COLON&quot;       &quot;MCF7A-repro&quot; &quot;MCF7D-repro&quot;</code></pre>
<pre class="r"><code># Let&#39;s see how many of each label:
nci %&gt;% group_by(labels) %&gt;% summarize(n())</code></pre>
<pre><code>## # A tibble: 14 x 2
##    labels      `n()`
##    &lt;chr&gt;       &lt;int&gt;
##  1 BREAST          7
##  2 CNS             5
##  3 COLON           7
##  4 K562A-repro     1
##  5 K562B-repro     1
##  6 LEUKEMIA        6
##  7 MCF7A-repro     1
##  8 MCF7D-repro     1
##  9 MELANOMA        8
## 10 NSCLC           9
## 11 OVARIAN         6
## 12 PROSTATE        2
## 13 RENAL           9
## 14 UNKNOWN         1</code></pre>
<pre class="r"><code># Not very many data points, with a lot of features!</code></pre>
<p>Are all of those features really necessary? Let’s try doing PCA and seeing what proportion of variance can be explained by taking just a few:</p>
<pre class="r"><code># I don&#39;t want to rescale the labels (which are not numerical in the first place!)
nci_pca &lt;- nci_data %&gt;% prcomp(scale = TRUE)

# Since we will be plotting this data, we would like a function that assigns 
# a color based on the value of the label to each of the datapoints:

Colors = function(vec) {
    colors = rainbow(length(unique(vec)))
    return(colors[as.numeric(as.factor(vec))])
}

# Now let&#39;s plot the PCs 
par(mfrow = c(1,2))

# plot the first and second PC
plot(nci_pca$x[,1:2], col = Colors(nci_labs), pch = 19, xlab = &quot;PC1&quot;,ylab = &quot;PC2&quot;)
# plot the first and third PC
plot(nci_pca$x[,c(1,3)], col = Colors(nci_labs), pch = 19, xlab = &quot;PC1&quot;, ylab = &quot;PC3&quot;)</code></pre>
<p><img src="STA314_files/figure-html/NCI60%20PCA-1.png" width="672" /></p>
<pre class="r"><code># Get back to regular plotting:
par(mfrow = c(1,1))</code></pre>
<p>Now if we want to plot without having to write a function to assign colors each time, we can use ggplot:</p>
<pre class="r"><code># Save as tibble for convenience (you could probably get away with not doing this)
nci_pca_tb &lt;- as.tibble(nci_pca$x)
# Add back the labels 
nci_pca_tb$labels &lt;- nci$labels

# Plot (I&#39;m dropping the argument names beyond this point)
ggplot(nci_pca_tb, aes(x = PC1, y = PC2, color = labels)) +
    geom_point()</code></pre>
<p><img src="STA314_files/figure-html/Plotting%20PCA%20with%20ggplot-1.png" width="672" /></p>
<pre class="r"><code># Doesn&#39;t this look 100 times simpler to do ?
ggplot(nci_pca_tb, aes(x = PC1, y = PC3, color = labels)) +
    geom_point()</code></pre>
<p><img src="STA314_files/figure-html/Plotting%20PCA%20with%20ggplot-2.png" width="672" /></p>
<p>For this reason, beyond this point, I will be rewriting all of visualizations from the labs into tidyverse code and omit the base R code.</p>
<pre class="r"><code># Extract the variances
df &lt;- tibble(PCA = 1:length(nci_pca$sdev), VarianceProportion = nci_pca$sdev^2 / sum(nci_pca$sdev^2))

# Plot just the variance explanation proportions
ggplot(data = df, aes(x = PCA, y = VarianceProportion)) +
    geom_line() +
    geom_point() +
    labs(x = &quot;Principal Component&quot;, y = &quot;Proportion of Variance Explained&quot;) +
    scale_y_continuous(limits = c(0, 1))</code></pre>
<p><img src="STA314_files/figure-html/NCI60%20PCA%20proportion%20of%20variance-1.png" width="672" /></p>
<pre class="r"><code># Plot the cumulative variance explanation proportions 
ggplot(data = df, aes(x = PCA, y = cumsum(VarianceProportion))) +
    geom_line(color = if_else(cumsum(df$VarianceProportion) &gt; 0.9, &quot;red&quot;, &quot;green&quot;)) +
    geom_point(color = if_else(cumsum(df$VarianceProportion) &gt; 0.9, &quot;red&quot;, &quot;green&quot;)) +
    labs(x = &quot;Principal Component&quot;, y = &quot;Cumulative Proportion of Variance Explained&quot;) +
    scale_y_continuous(limits = c(0, 1)) +
    geom_line(y = 0.9)</code></pre>
<p><img src="STA314_files/figure-html/NCI60%20PCA%20proportion%20of%20variance-2.png" width="672" /></p>
<p>The line on the second plot shows at which point you can cut out to keep <span class="math inline">\(90\)</span>% of variance (<span class="math inline">\(90\)</span>% of information about data), which looks to be about 20 features.</p>
<p>Note that this is an improvent of an improvements since by just doing the PCA we have decreased the number of features from 6830 to 64 which is a 99.06% decrease, without losing much information!</p>
<p>Now let’s do some clustering to review what we were doing 2 weeks ago:</p>
<p>Again for simplicity of visualization I will be using the ggdendro package.</p>
<pre class="r"><code># Create a vector of types of Hierarchical clustering we covered
clustering_types &lt;- c(&quot;complete&quot;, &quot;average&quot;, &quot;single&quot;)

# Perform HC in a loop and plotting
for (method in clustering_types) {
    # Good example of why %&gt;% is a great operator and method chaining is important
    nci_data %&gt;% # Take the data
    scale %&gt;% # Rescale it
    dist %&gt;%  # Create a distance matrix 
    hclust(method = method) %&gt;% # Perform HC
    as.dendrogram %&gt;% # Change type to dendrogram
    set(&quot;labels&quot;, nci_labs) %&gt;% # Add the labels from the original data
    set(&quot;branches_k_color&quot;, k = 4) %&gt;% # Split the tree into 4 classes and color
    set(&quot;labels_cex&quot;, 0.5) %&gt;%
    plot(main = paste0(method,&quot; linkage&quot;))
}</code></pre>
<p><img src="STA314_files/figure-html/NCI60%20Hierarichical%20Clustering-1.png" width="672" /><img src="STA314_files/figure-html/NCI60%20Hierarichical%20Clustering-2.png" width="672" /><img src="STA314_files/figure-html/NCI60%20Hierarichical%20Clustering-3.png" width="672" /></p>
<p>This illustrates just how different results can be for the types of HC we learned. Complete and average linkages usually result in similarily sized clusters, while Single linkage usually results in a single large cluster that has single leaves added to it at each step. Now let’s compare Complete Linkage with K-Means clustering</p>
<pre class="r"><code># Create the complete linkage clustering
cl_clusters &lt;- nci_data %&gt;% # Take the data
    scale %&gt;% # Rescale it
    dist %&gt;%  # Create a distance matrix 
    hclust(method = &quot;complete&quot;) %&gt;% # Perform HC
    cutree(k = 4) # cut 4 clusters

# Create the kmeans clustering
kmeans_clusters &lt;- nci_data %&gt;% # Take the data
    scale %&gt;%
    kmeans(centers = 4, nstart = 20) # Perform K-Means for 4 clusters 20 times
    
# Create a comparison Table:
table(kmeans_clusters$cluster, cl_clusters)</code></pre>
<pre><code>##    cl_clusters
##      1  2  3  4
##   1 20  7  0  0
##   2  9  0  0  0
##   3 11  0  0  9
##   4  0  0  8  0</code></pre>
<p>From this table we can see that cluster 4 in k-means is the same as cluster 3 in complete linkage, but the other clusters are a mixture. For example cluster 3 in k-means is a combination of 11 elements from cluster 1 in CL and 9 elements of cluster 4 in CL.</p>
</div>
<div id="question-3-1" class="section level2">
<h2>Question 3:</h2>
<div id="question" class="section level3">
<h3>Question:</h3>
<p>By modifying the argument used in the lecture, argue that the second principal component of a (centred) feature matrix <span class="math inline">\(X\)</span> should maximise the Rayleigh quotent of <span class="math inline">\(X^TX\)</span> over all vectors that are orthogonal to the first principal component and show that this implies that the second prinicipal component is the eigenvector of <span class="math inline">\(X^TX\)</span> that corresponds to the second largest eigenvalue.</p>
</div>
<div id="solution" class="section level3">
<h3>Solution:</h3>
<p><em>Solution adapted from <a href="https://a-morariu.github.io/">Alin Morariu</a></em></p>
<p> The second principle component of a centred feature matrix <span class="math inline">\(X\)</span> should maximize Rayleigh’s quotient up to being orthogonal to the fist principle component.</p>
<p> Note, this argument is almost trivial once we understand that we can order our set of eigenvalues and combining this with the following notation:</p>
<p><span class="math display">\[ v_2 = \text{arg} \max_{v \in \mathbb{R}^p, v \perp v_1 } \frac{v^T X^T X v}{v^T v} \]</span></p>
<p>All that this is asking, is that we find the eigenvector that maximizes the quotient with respect to all vectors in <span class="math inline">\(\mathbb{R}^p\)</span> which are orthogonal to <span class="math inline">\(v_1\)</span>. Furthermore, this corresponds to taking the second largest eignevalue from our ordered set. However, we will show this more rigorously using the argument made in class. We begin with the spectral decomposition of our feature matrix.</p>
<p><span class="math display">\[X^T X = UDU^T \text{ and define } Z = U^Tv \text{ where } v \in \mathbb{R}^p \]</span></p>
<p><span class="math display">\[ \Rightarrow v^T X^T X v =
z^T D z =
\sum_{i=1}^{p} \lambda_i z_i z_i^T \leq \sum_{i=1}^{p} \lambda_2 z_i z_i^T =
\lambda_2 \sum_{i=1}^{p} z_i z_I^T =
\lambda_2 v^T U^T U v\]</span></p>
<p>The inequality above turns into an equality in the case where <span class="math inline">\(z_3 = z_4 = \ldots = z_p = 0\)</span>; otherwise we can say that <span class="math inline">\(\lambda_2 \geq \lambda_i\)</span> for <span class="math inline">\(i \in \{3, 4, \ldots, p\}\)</span> which in turn maximizes the quotient with respect to the constraints. This concludes the proof.</p>
</div>
</div>
<div id="question-4-1" class="section level2">
<h2>Question 4</h2>
<div id="question-1" class="section level3">
<h3>Question:</h3>
<p>An experiment was undertaken to exmaine differences in burritos made across Toronto. 400 burritos were purchased at commercial venues and four measurements were taken of each burrito: mass (in grams), length (in centimetres), sturdiness (scored from 1-10), and taste (scored from 1-10). The scaled sample covariance matrix was calculated and its four eigenvalues were found to be 14.1, 4.3, 1.2 and 0.4. The corresponding first and second eigenvectors were:</p>
<p><span class="math display">\[v_1^T = [0.39, 0.42, 0.44, 0.69]\]</span> <span class="math display">\[v_2^T = [0.40, 0.39, 0.42,−0.72]\]</span></p>
<ol style="list-style-type: lower-roman">
<li><p>What is the proportion of variance explained by the first two prinicple components?</p></li>
<li><p>Give an interpretation of the first two principal components</p></li>
<li><p>Suppose that the data were stored by recording the eigenvalues and eigenvectors together with the 400 values of the first and second principal components and the mean values for the original variables. How would you use this information reconstruct approximations to the original covariance matrix and the original data?</p></li>
</ol>
</div>
<div id="solution-1" class="section level3">
<h3>Solution:</h3>
<p><em>Slightly modfied solution from <a href="https://a-morariu.github.io/">Alin Morariu</a></em></p>
<p>Given: <span class="math inline">\(\lambda^* = \{14.1, 4.3, 1.2, 0.4 \}\)</span> , <span class="math inline">\(v_1^T = [0.39, 0.42, 0.44, 0.69]\)</span> , and <span class="math inline">\(v_2^T = [0.40, 0.39, 0.42, -0.72]\)</span></p>
<div id="i" class="section level4">
<h4>i)</h4>
<p>We are asked to calculate the proportion of variance explained by the fist 2 principle components.</p>
<p><span class="math display">\[\text{proportion explained} =
\frac{\sum_{i=1}^{2} \lambda_i }{\sum_{i=1}^{4} \lambda_i } =
\frac{14.1 + 4.3}{14.1 + 4.3 + 1.2 + 0.4} =
0.92 \]</span></p>
</div>
<div id="ii" class="section level4">
<h4>ii)</h4>
<p>Interpreting the principle components:</p>
<p>First PC: Notice that each of the components have a positive coefficient we can say that heavier and longer burritos have a tendency to be tastier.</p>
<p>Second PC: Here we see that the component corresponding to taste has a negative coefficient which indicates that we have some burritos that are small, light, unsturdy and tasty OR large, heavy, sturdy, and bland (their scores would be equivalent).</p>
</div>
<div id="iii" class="section level4">
<h4>iii)</h4>
<p>Here we are tasked with reconstructing the variance-covariance matrix of the centred data based on our 2 eigenvectors, and 4 eigenvalues. We back out the desired result from the spectral decomposition of the variance-covariance matrix.</p>
<p>Since we only have the first two principal components we can only use the first two eigenvalues:</p>
<p><span class="math display">\[\hat{\Sigma} = \hat{U}\hat{D}\hat{U}^T = \sum_{i=1}^{2} \lambda_iv_i v_i^T = \lambda_1 v_1 v_1^T + \lambda_2 v_2 v_2^T  \]</span></p>
<p>Note that we have all of the quantities specified above so now its a matter of plugging in the values and multiplying them together.</p>
<p><span class="math display">\[\hat{\Sigma} = \left( \begin{array}{cccc} 
2.833 \\ 
2.980 &amp; 3.141 \\
3.142 &amp; 3.310 &amp; 3.488 \\
2.556 &amp; 2.879 &amp; 2.980 &amp; 8.942
\end{array} \right) \]</span></p>
<p>Noting that <span class="math inline">\(\hat{\Sigma}\)</span> is symmetric we can fill in the rest.</p>
<p>Remember that we framed the original data as: <span class="math display">\[ X = LU \]</span> Where <span class="math inline">\(U\)</span> are the principal components and <span class="math inline">\(L\)</span> is the loading matrix. Thus to retrieve the original data we need to multiply our eigenvectors by our loading factors matrix to get estimates of mean corrected data.</p>
<p><span class="math display">\[ x_i^T =
\begin{pmatrix}
    l_1 &amp; l_2
\end{pmatrix} 
\begin{pmatrix}
    v_1^T\\
    v_2^T
\end{pmatrix} = 
\begin{pmatrix}
    l_1 &amp; l_2
\end{pmatrix} 
\begin{pmatrix}
    0.39 &amp; 0.42 &amp; 0.44 &amp; 0.69\\
    0.40 &amp; 0.39 &amp; 0.42 &amp; -0.72
\end{pmatrix}
\]</span></p>
<p>So:</p>
<p><span class="math display">\[ x_1^T =  \begin{pmatrix}
    0.39 \cdot l_1 + 0.4 \cdot l_2
\end{pmatrix}\]</span> <span class="math display">\[ x_2^T =  \begin{pmatrix}
    0.42 \cdot l_1 + 0.39 \cdot l_2
\end{pmatrix}\]</span></p>
<p>etc.</p>
<p>And then to retrive the original we need to add back the mean.</p>
</div>
</div>
</div>
<div id="questions-from-the-tutorial" class="section level2">
<h2>Questions from the tutorial:</h2>
<div id="spectral-decomposition-explanation" class="section level3">
<h3>Spectral Decomposition explanation</h3>
<p>Any symmetric matrix A has a spectral decomposition of the form:</p>
<p><span class="math display">\[ A = Q \Lambda Q^{-1} \]</span></p>
<p>Where <span class="math inline">\(Q\)</span> is Orthogonal (its column vectors are orthogonal), and <span class="math inline">\(\Lambda\)</span> is diagonal.</p>
<p>Let <span class="math inline">\(v_i\)</span> denote the <span class="math inline">\(i\)</span>-th column of <span class="math inline">\(Q\)</span>. Then <span class="math inline">\(v_i^Tv_j\)</span> is the <span class="math inline">\(i,j\)</span>-th element of <span class="math inline">\(Q^TQ = Q^{-1}Q = I\)</span>. This means that each column of <span class="math inline">\(Q\)</span> has length <span class="math inline">\(1\)</span> and is perpendicular to every other column.</p>
<p>Multiplying the equation by <span class="math inline">\(Q\)</span> from the right yields:</p>
<p><span class="math display">\[ AQ = Q \Lambda Q^{-1} Q = Q \Lambda \]</span></p>
<p>If we interpret this by looking at columns of <span class="math inline">\(Q\)</span>, <span class="math inline">\(v_i\)</span>:</p>
<p><span class="math display">\[ Av_i = \lambda_i v_i \]</span></p>
<p>Where <span class="math inline">\(\lambda_i\)</span> is the <span class="math inline">\(i\)</span>-th diagonal entry of <span class="math inline">\(\Lambda\)</span>.</p>
<p>Which means that the <span class="math inline">\(v_i\)</span>’s are eigenvectors of <span class="math inline">\(A\)</span> with eigenvalues <span class="math inline">\(\lambda_i\)</span>!</p>
</div>
<div id="why-is-variance-covariance-positive-semi-definite-i.e.why-is-s-q-lambda-qt" class="section level3">
<h3>Why is variance-covariance positive semi-definite? i.e. why is <span class="math inline">\(S = Q \Lambda Q^T\)</span></h3>
<p>A matrix <span class="math inline">\(V\)</span> is called positive semi-definite if for any vector <span class="math inline">\(a\)</span>:</p>
<p><span class="math display">\[ a^T V a \geq 0  \]</span></p>
<p><a href="https://stats.stackexchange.com/a/53105">link to stack exchange for this answer</a></p>
<p>And for the variance-covariance matrix <span class="math inline">\(S\)</span>:</p>
<p><span class="math display">\[ S = \frac{1}{n}\sum_{i=1}^{n}{(x_i - \bar{x})(x_i - \bar{x})^T} \]</span></p>
<p>Then for every non-zero vector <span class="math inline">\(a\)</span> we have:</p>
<p><span class="math display">\[ a^TSa = a^T(\frac{1}{n}\sum_{i=1}^{n}{(x_i - \bar{x})(x_i - \bar{x})^T} )a\]</span> <span class="math display">\[ = \frac{1}{n}(\sum_{i=1}^{n}{(a^T(x_i - \bar{x})(x_i - \bar{x})^Ta)} ) \]</span> <span class="math display">\[ = \frac{1}{n}\sum_{i=1}^{n}{(((x_i - \bar{x})^Ta^T)((x_i - \bar{x})^Ta))} \]</span></p>
<p><span class="math display">\[ = \frac{1}{n}\sum_{i=1}^{n}{((x_i - \bar{x})^Ta)^2} \geq 0  \]</span></p>
<p>So the variance-covariance matrix <span class="math inline">\(S\)</span> is always positive semi-definite and symmetric.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
